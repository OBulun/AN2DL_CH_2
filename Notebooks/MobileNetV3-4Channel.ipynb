{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3293c812",
   "metadata": {
    "id": "3293c812"
   },
   "source": [
    "## **1. Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631b342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4631b342",
    "outputId": "58fbc501-7799-471f-9a5a-e64689609281"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#cur_dir = \"/content/drive/MyDrive/CH2/Notebooks\"\n",
    "#%cd $cur_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfd385",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8bfd385",
    "outputId": "b4006194-5220-4fbf-9b47-0be0b5673790"
   },
   "outputs": [],
   "source": [
    "#%pip install torchview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee15c7",
   "metadata": {
    "id": "edee15c7"
   },
   "source": [
    "## **2. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c7403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "963c7403",
    "outputId": "b6727f0e-41f8-4a6b-dea0-f74d54e6a861"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tfs\n",
    "\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ee8",
   "metadata": {
    "id": "3b5b1ee8"
   },
   "source": [
    "## **3. Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d674a2d",
   "metadata": {
    "id": "7d674a2d"
   },
   "outputs": [],
   "source": [
    "USE_MASKED_PATCHES = False\n",
    "SUB_MODEL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3da911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df3da911",
    "outputId": "8f0365f9-913a-4b45-9039-d4b26ac59c53"
   },
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path\n",
    "\n",
    "if USE_MASKED_PATCHES:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"train_patches_masked\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"submission_patches_masked\")\n",
    "else:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\")\n",
    "\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "print(f\"Test data path: {test_data_path}\")\n",
    "print(f\"Patches output path: {PATCHES_OUT}\")\n",
    "print(f\"Submission patches output path: {SUBMISSION_PATCHES_OUT}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427b09e",
   "metadata": {
    "id": "e427b09e"
   },
   "source": [
    "## **4. Train/Val Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7455a",
   "metadata": {
    "id": "7cc7455a"
   },
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
    "    \"\"\"\n",
    "    # 1. Load the labels CSV\n",
    "    # Assuming CSV structure: [image_id, label] or similar\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "\n",
    "    # Standardize column names for easier merging\n",
    "    # We assume the first column is the ID and the second is the Label\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "\n",
    "    # Ensure IDs in CSV are strings (to match filenames)\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "\n",
    "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
    "    # because our parsed Bag IDs won't have them.\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # 2. List all patch files\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
    "\n",
    "    # 3. Parse filenames to get Bag IDs\n",
    "    data = []\n",
    "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
    "\n",
    "    for filename in patch_files:\n",
    "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
    "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
    "\n",
    "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
    "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
    "        try:\n",
    "            bag_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(f\"Skipping malformed filename: {filename}\")\n",
    "\n",
    "    # Create temporary patches DataFrame\n",
    "    df_patches = pd.DataFrame(data)\n",
    "\n",
    "    # 4. Merge patches with labels\n",
    "    # This assigns the correct Bag Label to every Patch in that Bag\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
    "\n",
    "    # 5. Clean up and Rename\n",
    "    # Keep only required columns\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "\n",
    "    # Rename label column to standard 'label' if it isn't already\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "\n",
    "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b8e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633b8e05",
    "outputId": "7d671b8b-3e81-446b-d478-67847824bf41"
   },
   "outputs": [],
   "source": [
    "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(patches_metadata_df.head().drop(columns=['path']))\n",
    "print(\"\\nPatches per Bag (Distribution):\")\n",
    "print(patches_metadata_df['sample_id'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6c6cd3",
    "outputId": "be243fc3-59b3-4318-ed4b-09eb2ce21d3d"
   },
   "outputs": [],
   "source": [
    "# Add Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "\n",
    "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"  {orig} -> {enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52314ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52314ae9",
    "outputId": "ce81e6f4-f3fe-4311-ebd2-11946f50a08e"
   },
   "outputs": [],
   "source": [
    "# Train/Val Split on Original Images (not patches)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train/Val Split on Original Images\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get unique sample IDs\n",
    "unique_samples = patches_metadata_df['sample_id'].unique()\n",
    "print(f\"\\nTotal unique samples (original images): {len(unique_samples)}\")\n",
    "\n",
    "# Split samples into train (80%) and val (20%)\n",
    "train_samples, val_samples = train_test_split(\n",
    "    unique_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples, 'label_encoded'].values\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples: {len(val_samples)}\")\n",
    "\n",
    "# Create train and val DataFrames by filtering patches\n",
    "df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_samples)].reset_index(drop=True)\n",
    "df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_samples)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain patches: {len(df_train)}\")\n",
    "print(f\"Val patches: {len(df_val)}\")\n",
    "print(f\"\\nTrain label distribution:\\n{df_train['label'].value_counts()}\")\n",
    "print(f\"\\nVal label distribution:\\n{df_val['label'].value_counts()}\")\n",
    "\n",
    "# Print percentage distribution\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Percentage Distribution\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTrain label percentage:\\n{df_train['label'].value_counts(normalize=True) * 100}\")\n",
    "print(f\"\\nVal label percentage:\\n{df_val['label'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b797aff",
   "metadata": {
    "id": "4b797aff"
   },
   "source": [
    "## **5. Transformations & Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94459d26",
   "metadata": {
    "id": "94459d26"
   },
   "outputs": [],
   "source": [
    "# Define augmentation for training with enhanced transformations\n",
    "train_augmentation = transforms.Compose([\n",
    "    # Geometric transformations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Small rotations to handle orientation variations\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),  # Reduced from 0.2 for more conservative shifts\n",
    "        scale=None,  # Add scale variation\n",
    "        shear=10  # Add shear transformation\n",
    "    ),\n",
    "    # Optional: Add Gaussian blur for noise robustness\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cdd92",
   "metadata": {
    "id": "dc8cdd92"
   },
   "source": [
    "## **6. Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization statistics\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b69bba",
   "metadata": {
    "id": "29b69bba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "class TissueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, masks_dir=None, augmentation=None, normalize_imagenet=True, target_size=(224, 224)):\n",
    "        self.df = df\n",
    "        self.masks_dir = masks_dir\n",
    "        self.augmentation = augmentation\n",
    "        self.normalize_imagenet = normalize_imagenet\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        self.paths = df['path'].tolist()\n",
    "        self.labels = df['label_encoded'].tolist()\n",
    "        \n",
    "        # 1. Base Converter: PIL -> Tensor (Keep original resolution here!)\n",
    "        # We DO NOT resize here. We let the augmentation handle it to preserve detail.\n",
    "        self.to_tensor_raw = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True), # Scales [0, 255] -> [0.0, 1.0]\n",
    "        ])\n",
    "        \n",
    "        # 2. Safety Resize: If NO augmentation is provided, we need a fallback resize\n",
    "        # to ensure batches stack correctly.\n",
    "        self.resize_fallback = v2.Resize(self.target_size, antialias=True)\n",
    "\n",
    "        # 3. Normalization\n",
    "        self.norm = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # --- 1. Load Image ---\n",
    "        # Fail loudly on IO errors so you know your data is bad\n",
    "        img_pil = Image.open(img_path).convert('RGB')\n",
    "        image = self.to_tensor_raw(img_pil) # Shape: [3, H_orig, W_orig]\n",
    "\n",
    "        # --- 2. Load Mask ---\n",
    "        # Construct mask path\n",
    "        mask_filename = os.path.basename(img_path).replace('img_', 'mask_')\n",
    "        mask_path = os.path.join(self.masks_dir, mask_filename)\n",
    "        \n",
    "        if os.path.exists(mask_path):\n",
    "            mask_pil = Image.open(mask_path).convert('L')\n",
    "            mask = self.to_tensor_raw(mask_pil) # Shape: [1, H_orig, W_orig]\n",
    "        else:\n",
    "            # Fallback: Attention everywhere (all 1s)\n",
    "            # Use image shape to match resolution\n",
    "            mask = torch.ones((1, image.shape[1], image.shape[2]), dtype=torch.float32)\n",
    "\n",
    "        # --- 3. Wrap for V2 Semantics ---\n",
    "        # This is CRITICAL. We tell PyTorch exactly what these tensors are.\n",
    "        image = tv_tensors.Image(image)\n",
    "        mask = tv_tensors.Mask(mask)\n",
    "\n",
    "        # --- 4. Augmentation (Syncs Image + Mask) ---\n",
    "        if self.augmentation:\n",
    "            # The augmentation pipeline is responsible for the final Resize/Crop\n",
    "            image, mask = self.augmentation(image, mask)\n",
    "        else:\n",
    "            # If no augmentation, we must resize manually to target_size\n",
    "            image = self.resize_fallback(image)\n",
    "            mask = self.resize_fallback(mask)\n",
    "\n",
    "        # --- 5. Final Normalization (Image Only) ---\n",
    "        if self.normalize_imagenet:\n",
    "            image = self.norm(image)\n",
    "\n",
    "        # --- 6. Concatenate ---\n",
    "        # [3, H, W] + [1, H, W] -> [4, H, W]\n",
    "        image_tensor = image.as_subclass(torch.Tensor)\n",
    "        mask_tensor = mask.as_subclass(torch.Tensor)\n",
    "    \n",
    "    # Now both are standard Tensors, so they can be concatenated\n",
    "        combined_input = torch.cat((image_tensor, mask_tensor), dim=0)\n",
    "\n",
    "        return combined_input, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4cb5a",
   "metadata": {
    "id": "ffa4cb5a"
   },
   "source": [
    "## **7. Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19525f4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19525f4f",
    "outputId": "6eb2758d-db78-4ec9-9135-2b5effb6c297"
   },
   "outputs": [],
   "source": [
    "if LOCAL: \n",
    "    num_workers = 0\n",
    "    CACHE_IMAGES = False\n",
    "else:\n",
    "    num_workers = max(2, (os.cpu_count() or 2)//2)\n",
    "    CACHE_IMAGES = True\n",
    "\n",
    "# Instantiate Datasets\n",
    "train_dataset = TissueDataset(\n",
    "    df_train, \n",
    "    augmentation=train_augmentation, \n",
    "    normalize_imagenet=True,\n",
    "    masks_dir=os.path.join(SUBMISSION_PATCHES_OUT, \"masks\"),\n",
    "    #cache_images=CACHE_IMAGES  # Enable image pre-loading\n",
    ")\n",
    "val_dataset = TissueDataset(\n",
    "    df_val, \n",
    "    augmentation=None, \n",
    "    normalize_imagenet=True,\n",
    "    masks_dir=os.path.join(SUBMISSION_PATCHES_OUT, \"masks\"),\n",
    "    #cache_images=CACHE_IMAGES  # Enable image pre-loading\n",
    ")\n",
    "\n",
    "# Batch Size: 32 or 64 is standard for ResNet18/50 on 1MP images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate Loaders with optimizations\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "print(f\"Train Loader: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader: {len(val_loader)} batches\")\n",
    "print(f\"Num workers: {train_loader.num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bfef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "4212bfef",
    "outputId": "89015ce8-f70c-419c-c798-0243efeb0983"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_batch(loader, count=4):\n",
    "    # Fetch a batch\n",
    "    batch, labels = next(iter(loader))\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Define stats for Un-normalization (RGB only)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    for i in range(count):\n",
    "        ax = plt.subplot(1, count, i + 1)\n",
    "\n",
    "        # 1. Get the single sample: Shape [4, H, W]\n",
    "        sample = batch[i] \n",
    "\n",
    "        # 2. Split RGB and Mask\n",
    "        rgb_tensor = sample[:3, :, :]   # First 3 channels\n",
    "        mask_tensor = sample[3, :, :]   # 4th channel\n",
    "\n",
    "        # 3. Denormalize RGB\n",
    "        # Note: We must move mean/std to the same device as sample if using GPU, \n",
    "        # but usually visualization happens on CPU.\n",
    "        rgb_tensor = rgb_tensor * std + mean\n",
    "        rgb_tensor = torch.clamp(rgb_tensor, 0, 1)\n",
    "\n",
    "        # 4. Convert to Numpy for plotting\n",
    "        img_np = rgb_tensor.permute(1, 2, 0).numpy() # [H, W, 3]\n",
    "        mask_np = mask_tensor.numpy()                # [H, W]\n",
    "\n",
    "        # 5. Plot RGB\n",
    "        ax.imshow(img_np)\n",
    "\n",
    "        # 6. Overlay Mask (Yellow/Red with transparency)\n",
    "        # Using 'jet' colormap with alpha=0.4 to see the image underneath\n",
    "        # We mask out values near 0 to avoid obscuring the background if the mask is sparse\n",
    "        #masked_data = np.ma.masked_where(mask_np < 0.1, mask_np)\n",
    "        #ax.imshow(masked_data, cmap='jet', alpha=0.5)\n",
    "\n",
    "        #ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "        #ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing Training Batch (RGB + Mask Overlay):\")\n",
    "show_batch(train_loader, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89d235",
   "metadata": {
    "id": "7c89d235"
   },
   "source": [
    "## **8. Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e381193",
   "metadata": {
    "id": "9e381193"
   },
   "outputs": [],
   "source": [
    "DROPOUT_RATE = 0.4\n",
    "HIDDEN_SIZE = 512\n",
    "L2_REG = 1e-4\n",
    "\n",
    "NUM_EPOCHS = 1000  # Increased since we have early stopping\n",
    "PATIENCE = 20    # Stop if val_loss doesn't improve for 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d978b",
   "metadata": {
    "id": "4f1d978b"
   },
   "source": [
    "## **9. Model Definition (Transfer Learning - MobileNetV3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4ec0",
   "metadata": {
    "id": "5c6c4ec0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class MobileNetV3Small(nn.Module):\n",
    "    \"\"\"MobileNetV3 Small architecture for image classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, dropout_rate=0.2, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "\n",
    "        # Load MobileNetV3 Small with pretrained weights\n",
    "        self.backbone = torchvision.models.mobilenet_v3_small(\n",
    "            weights=torchvision.models.MobileNet_V3_Small_Weights.DEFAULT\n",
    "        )\n",
    "\n",
    "        # 1. FREEZE PARAMETERS (Weights/Biases)\n",
    "        # This prevents the weights (gamma) and bias (beta) from changing.\n",
    "        if self.freeze_backbone:\n",
    "            for param in self.backbone.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # MobileNetV3 classifier structure inputs\n",
    "        # For MobileNetV3-Small, the last feature size is 576\n",
    "        in_features = 576\n",
    "\n",
    "        # Optimized Classifier Head\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            # Layer 1: Expansion\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            # Layer 2: Bottleneck\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            # Layer 3: Final Classification \n",
    "            # If you need probabilities, add nn.Softmax(dim=1) after this.\n",
    "            nn.Linear(128, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Override train mode to ensure frozen backbone BN layers \n",
    "        STAY in eval mode.\n",
    "        \"\"\"\n",
    "        super().train(mode)\n",
    "\n",
    "        # 2. FREEZE STATISTICS (Running Mean/Variance)\n",
    "        # If we are freezing the backbone, we must ensure the BatchNorm layers\n",
    "        # inside the backbone remain in eval mode, even when the rest of the \n",
    "        # model is training.\n",
    "        if self.freeze_backbone and mode:\n",
    "            for m in self.backbone.features.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    # Optional: Disable tracking purely for safety\n",
    "                    m.track_running_stats = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "def get_4channel_mobilenet(num_classes):\n",
    "    # 1. Load standard model\n",
    "    weights = MobileNet_V3_Small_Weights.DEFAULT\n",
    "    model = mobilenet_v3_small(weights=weights)\n",
    "    \n",
    "    # 2. Modify First Convolution Layer\n",
    "    # Original: Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "    original_first_layer = model.features[0][0]\n",
    "    \n",
    "    # Create new layer with in_channels=4\n",
    "    new_first_layer = nn.Conv2d(\n",
    "        in_channels=4, \n",
    "        out_channels=original_first_layer.out_channels, \n",
    "        kernel_size=original_first_layer.kernel_size, \n",
    "        stride=original_first_layer.stride, \n",
    "        padding=original_first_layer.padding, \n",
    "        bias=original_first_layer.bias is not None\n",
    "    )\n",
    "    \n",
    "    # 3. Intelligent Weight Initialization (Crucial for Transfer Learning)\n",
    "    with torch.no_grad():\n",
    "        # Copy weights for the first 3 channels (RGB) from the pre-trained model\n",
    "        new_first_layer.weight[:, :3, :, :] = original_first_layer.weight\n",
    "        \n",
    "        # Initialize the 4th channel (Mask). \n",
    "        # We can average the RGB weights or initialize to zero/small random values.\n",
    "        # Averaging allows the mask to activate features similar to visual intensity initially.\n",
    "        new_first_layer.weight[:, 3:4, :, :] = torch.mean(original_first_layer.weight, dim=1, keepdim=True)\n",
    "\n",
    "    # Replace the layer in the model\n",
    "    model.features[0][0] = new_first_layer\n",
    "    \n",
    "    # 4. Modify Classifier Head\n",
    "    in_features = model.classifier[3].in_features\n",
    "    model.classifier[3] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ba3db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "439ba3db",
    "outputId": "e91a18e2-eb93-4cef-cf29-7c221c88ccf7"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Ensure device is defined (usually from previous cells, but safe to redefine if standalone)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming label_encoder is defined in your notebook scope\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = get_4channel_mobilenet(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model initialized (MobileNetV3Small) with {num_classes} output classes.\")\n",
    "#summary(model, input_size=(4, TARGET_SIZE[0], TARGET_SIZE[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb49a6",
   "metadata": {
    "id": "fcfb49a6"
   },
   "source": [
    "## **10. Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c87c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b270c87c",
    "outputId": "3e5283ef-c4a8-4fef-b0b3-0399a7ee7e65"
   },
   "outputs": [],
   "source": [
    "# 1. Get Counts (from your snippet)\n",
    "class_counts = df_train['label_encoded'].value_counts().sort_index().values\n",
    "total_samples = sum(class_counts)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "# 2. Define Manual Tuning Factors (The \"weight\" knob)\n",
    "# 1.0 = Default (Pure Inverse Frequency)\n",
    "# > 1.0 = Force model to focus MORE on this class (e.g., critical error)\n",
    "# < 1.0 = Force model to focus LESS on this class (e.g., noisy label)\n",
    "# Ensure this list length matches n_classes (4 in your case)\n",
    "tuning_factors = torch.tensor([1.0, 0.8, 0.8, 0.6], dtype=torch.float32)\n",
    "\n",
    "# 3. Calculate Base Weights (Standard Inverse Frequency)\n",
    "# Formula: N / (C * freq)\n",
    "base_weights = torch.tensor(\n",
    "    [total_samples / (n_classes * c) for c in class_counts],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 4. Apply Tuning\n",
    "# Final Weight = Inverse_Freq_Weight * Manual_Tuning_Factor\n",
    "final_weights = base_weights * tuning_factors\n",
    "\n",
    "# 5. Move to device\n",
    "final_weights = final_weights.to(device)\n",
    "\n",
    "print(f\"Base Weights:  {base_weights}\")\n",
    "print(f\"Tuning Factors:{tuning_factors}\")\n",
    "print(f\"Final Weights: {final_weights}\")\n",
    "\n",
    "# Update Loss Function\n",
    "criterion = nn.CrossEntropyLoss(weight=None, label_smoothing=0.2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RAdam(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=1e-3,\n",
    "    # You can also include other Adam parameters like betas, eps, weight_decay\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=L2_REG\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aa519",
   "metadata": {
    "id": "9c7aa519"
   },
   "source": [
    "## **11. Function: Training & Validation Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abb343",
   "metadata": {
    "id": "37abb343"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Lists to store all predictions and labels for F1 calculation\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    loop = tqdm(loader, leave=False)\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics accumulation\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Move to CPU and convert to numpy for sklearn metrics\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    # Calculate F1 Score (Macro for imbalanced data)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440fc05",
   "metadata": {
    "id": "e440fc05"
   },
   "source": [
    "## **12. Training Loop: Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6689e",
   "metadata": {
    "id": "41d6689e"
   },
   "source": [
    "### 12.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897de7e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "897de7e8",
    "outputId": "b75e0d4b-e0b2-46f9-87da-27ef1f064240"
   },
   "outputs": [],
   "source": [
    "best_val_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_tl_epoch = 0\n",
    "model_saved = False\n",
    "\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "print(f\"Starting Training with MobileNetV3 (Patience: {PATIENCE})...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Update Scheduler (based on Loss) EXPERIMENT: No scheduler on TL\n",
    "    #scheduler.step(val_loss)\n",
    "\n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "\n",
    "    # --- Checkpointing (Save Best Model based on F1) ---\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0  # Reset counter\n",
    "        best_tl_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'models/best_model_mobilenetv3_tl.pt')\n",
    "        model_saved = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "    if model_saved:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE} Best:{best_val_f1:.4f} âœ“\")\n",
    "    else:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"   >>> Early Stopping Triggered! Training stopped.\")\n",
    "        break\n",
    "    \n",
    "SUB_MODEL = 'models/best_model_mobilenetv3_tl.pt'\n",
    "print(f\"Submodel saved to {SUB_MODEL} at epoch {best_tl_epoch} with Val F1: {best_val_f1:.4f} for now. Will update if better model found in fine tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dbdd6",
   "metadata": {
    "id": "437dbdd6"
   },
   "source": [
    "### 12.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "a6263355",
    "outputId": "e4b0b334-f800-48f0-ee2d-303d639ad12b"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67506d70",
   "metadata": {
    "id": "67506d70"
   },
   "source": [
    "### 12.3 Function: Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1be552",
   "metadata": {
    "id": "3e1be552"
   },
   "outputs": [],
   "source": [
    "def get_image_predictions(model, loader, device):\n",
    "    \"\"\"\n",
    "    Aggregates patch-level predictions to image-level.\n",
    "    Strategy: Average the Softmax probabilities of all patches in a bag.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Access the dataframe from the dataset\n",
    "    dataset = loader.dataset\n",
    "    df = dataset.df\n",
    "    \n",
    "    # Define the inference transform explicitly to match Validation logic\n",
    "    # (Resize -> ToTensor -> Normalize)\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Get unique sample IDs (original images)\n",
    "    sample_ids = df['sample_id'].unique()\n",
    "\n",
    "    print(f\"\\nAggregating predictions for {len(sample_ids)} unique images...\")\n",
    "\n",
    "    for sample_id in tqdm(sample_ids, leave=False):\n",
    "        # Get all patches belonging to this image\n",
    "        sample_patches = df[df['sample_id'] == sample_id]\n",
    "\n",
    "        # Ground Truth (all patches share the image label)\n",
    "        # We use the column name you stored in dataset.label_col (default 'label_encoded')\n",
    "        true_label = sample_patches.iloc[0]['label_encoded'] \n",
    "        y_true.append(true_label)\n",
    "\n",
    "        # Load and process all patches for this image\n",
    "        patches = []\n",
    "        for img_path in sample_patches['path']:\n",
    "            try:\n",
    "                # Load image (ensure RGB)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                # Apply the manual inference transform\n",
    "                img = inference_transform(img)\n",
    "                \n",
    "                patches.append(img)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if not patches:\n",
    "            y_pred.append(true_label) # Fallback\n",
    "            continue\n",
    "\n",
    "        # Stack patches into a single batch: [Num_Patches, 3, 224, 224]\n",
    "        batch = torch.stack(patches).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits  = model(batch) \n",
    "            \n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # --- Aggregation: Mean Probability ---\n",
    "            avg_probs = torch.mean(probs, dim=0)\n",
    "            pred_label = torch.argmax(avg_probs).item()\n",
    "\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2068748",
   "metadata": {
    "id": "c2068748"
   },
   "source": [
    "### 12.4 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88a0a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "cd88a0a7",
    "outputId": "4e04e6e1-67f1-45aa-e1ae-84c28ea6bf6b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Calculate and Plot Confusion Matrix\n",
    "print(\"Generating Confusion Matrix on Original Images...\")\n",
    "y_true_img, y_pred_img = get_image_predictions(model, val_loader, device)\n",
    "\n",
    "# Compute Matrix\n",
    "cm = confusion_matrix(y_true_img, y_pred_img)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Aggregated per Image)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319bc10",
   "metadata": {
    "id": "8319bc10"
   },
   "outputs": [],
   "source": [
    "def plot_sample_with_predictions(model, loader, device, label_encoder, sample_id=None, aggregation_method='max_confidence'):\n",
    "    \"\"\"Plot all patches of a single sample and the aggregated image prediction.\"\"\"\n",
    "    import math\n",
    "    model.eval()\n",
    "    dataset = loader.dataset\n",
    "    df = dataset.df\n",
    "\n",
    "    # Pick a sample_id\n",
    "    if sample_id is None:\n",
    "        sample_id = np.random.choice(df['sample_id'].unique())\n",
    "    sample_patches = df[df['sample_id'] == sample_id].reset_index(drop=True)\n",
    "\n",
    "    # Load and transform all patches\n",
    "    images_tensors = []\n",
    "    display_imgs = []\n",
    "    for _, row in sample_patches.iterrows():\n",
    "        img = Image.open(row['path']).convert('RGB')\n",
    "        if dataset.transform:\n",
    "            tensor_img = dataset.transform(img)\n",
    "        else:\n",
    "            tensor_img = transforms.ToTensor()(img)\n",
    "        images_tensors.append(tensor_img)\n",
    "        # denormalize for display\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "        display_imgs.append(torch.clamp(tensor_img * std + mean, 0, 1))\n",
    "\n",
    "    batch = torch.stack(images_tensors).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "        probs = torch.softmax(logits, dim=1).cpu()\n",
    "\n",
    "    patch_preds = probs.argmax(dim=1).numpy()\n",
    "    patch_confs = probs.max(dim=1).values.numpy()\n",
    "\n",
    "    if aggregation_method == 'max_confidence':\n",
    "        # Average probabilities (Soft Voting)\n",
    "        image_probs = probs.mean(dim=0).numpy()\n",
    "        image_pred = image_probs.argmax()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    elif aggregation_method == 'majority_voting':\n",
    "        # Hard Voting\n",
    "        counts = np.bincount(patch_preds, minlength=len(label_encoder.classes_))\n",
    "        image_pred = counts.argmax()\n",
    "        # Normalize counts for visualization purposes\n",
    "        image_probs = counts / counts.sum()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n",
    "\n",
    "    image_pred_label = label_encoder.inverse_transform([image_pred])[0]\n",
    "    true_label = label_encoder.inverse_transform([sample_patches.iloc[0]['label_encoded']])[0]\n",
    "\n",
    "    cols = min(6, len(sample_patches))\n",
    "    rows = math.ceil(len(sample_patches) / cols)\n",
    "    fig = plt.figure(figsize=(3*cols + 4, 3*rows))\n",
    "    gs = fig.add_gridspec(rows, cols + 1, width_ratios=[1]*cols + [1.3])\n",
    "\n",
    "    # Patch grid\n",
    "    for idx, (img_disp, pred, conf) in enumerate(zip(display_imgs, patch_preds, patch_confs)):\n",
    "        ax = fig.add_subplot(gs[idx // cols, idx % cols])\n",
    "        ax.imshow(img_disp.permute(1,2,0))\n",
    "        lbl = label_encoder.inverse_transform([pred])[0]\n",
    "        ax.set_title(f\"{lbl}\\n{conf:.2%}\", fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Aggregated distribution\n",
    "    ax_bar = fig.add_subplot(gs[:, -1])\n",
    "    class_names = label_encoder.classes_\n",
    "    colors = ['green' if i == image_pred else 'steelblue' for i in range(len(class_names))]\n",
    "    ax_bar.barh(class_names, image_probs, color=colors)\n",
    "    ax_bar.set_xlabel('Probability' if aggregation_method == 'max_confidence' else 'Vote Share')\n",
    "    ax_bar.set_xlim([0,1])\n",
    "    ax_bar.set_title(f\"Sample: {sample_id}\\nTrue: {true_label} | Pred: {image_pred_label} ({image_conf:.2%})\\nMethod: {aggregation_method}\")\n",
    "    for i, prob in enumerate(image_probs):\n",
    "        ax_bar.text(prob + 0.02, i, f\"{prob:.3f}\", va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b568a48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "9b568a48",
    "outputId": "3f9a323d-c874-4a44-efa4-d2b6d501dbde"
   },
   "outputs": [],
   "source": [
    "# Visualize a random validation sample\n",
    "print(\"Plotting random validation sample with prediction distribution:\")\n",
    "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad76976",
   "metadata": {
    "id": "2ad76976"
   },
   "source": [
    "## **13. Training Loop: Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01cc5a",
   "metadata": {
    "id": "af01cc5a"
   },
   "source": [
    "### 13.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ed2bdb",
    "outputId": "bba7f2ce-d9c2-4249-d9a1-fe1d04603a8f"
   },
   "outputs": [],
   "source": [
    "# 1. Initialize the NEW model instance\n",
    "#    (Make sure to use the same class definition you used for training)\n",
    "ft_model = MobileNetV3Small(num_classes, DROPOUT_RATE, freeze_backbone=True).to(device)\n",
    "\n",
    "# 2. Load the best weights from the first phase\n",
    "ft_model.load_state_dict(torch.load(\"models/best_model_mobilenetv3_tl.pt\"), strict=True)\n",
    "\n",
    "# 3. Unfreeze parameters\n",
    "#    First, ensure everything is frozen\n",
    "for param in ft_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#    Unfreeze the Classifier (Head)\n",
    "for param in ft_model.backbone.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#    Unfreeze the last 2 blocks of the Backbone (Features)\n",
    "#    EfficientNet B0 features are in 'ft_model.backbone.features'\n",
    "for block in ft_model.backbone.features[-2:]:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Optimizer\n",
    "ft_optimizer = torch.optim.RAdam(\n",
    "    [p for p in ft_model.parameters() if p.requires_grad],\n",
    "    lr=1e-4,\n",
    "    # You can also include other Adam parameters like betas, eps, weight_decay\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=L2_REG\n",
    ")\n",
    "\n",
    "# 5. New Scheduler for the new optimizer\n",
    "ft_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    ft_optimizer, mode='min', factor=0.1, patience=3,\n",
    ")\n",
    "\n",
    "print(\"Starting Fine-Tuning\")\n",
    "\n",
    "# Reset history\n",
    "ft_history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1_ft = 0.0\n",
    "patience_counter_ft = 0\n",
    "best_ft_epoch = 0\n",
    "model_saved = False\n",
    "ft_better_than_tl = False\n",
    "# Training Loop using ft_model and ft_optimizer\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(ft_model, train_loader, criterion, ft_optimizer, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(ft_model, val_loader, criterion, device)\n",
    "\n",
    "    # Update Scheduler\n",
    "    ft_scheduler.step(val_loss)\n",
    "\n",
    "    ft_history['train_loss'].append(train_loss)\n",
    "    ft_history['train_f1'].append(train_f1)\n",
    "    ft_history['val_loss'].append(val_loss)\n",
    "    ft_history['val_f1'].append(val_f1)\n",
    "\n",
    "\n",
    "\n",
    "    # --- Checkpointing (Save Best Model based on F1) --- and Early Stopping\n",
    "    if val_f1 > best_val_f1_ft:\n",
    "        best_val_f1_ft = val_f1\n",
    "        best_ft_epoch = epoch + 1\n",
    "        patience_counter_ft = 0  # Reset counter\n",
    "        torch.save(ft_model.state_dict(), 'models/best_model_mobilenetv3_ft.pt')\n",
    "        model_saved = True\n",
    "        if best_val_f1_ft > best_val_f1:\n",
    "            ft_better_than_tl = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter_ft += 1\n",
    "\n",
    "    if model_saved:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE} âœ“\")\n",
    "    else:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE}\")\n",
    "    if patience_counter_ft >= PATIENCE:\n",
    "        print(\"Early Stopping Triggered! Best FT Epoch: {} with Val F1: {:.4f}\".format(best_ft_epoch, best_val_f1_ft))\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ft_better_than_tl:\n",
    "    print(\"Fine-Tuning improved over Transfer Learning. Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1_ft, best_ft_epoch))\n",
    "    SUB_MODEL = 'models/best_model_mobilenetv3_ft.pt'\n",
    "else:\n",
    "    print(\"Fine-Tuning did not improve over Transfer Learning. Best Validation F1 Score remains: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))\n",
    "    SUB_MODEL = 'models/best_model_mobilenetv3_tl.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb7a76",
   "metadata": {
    "id": "b2bb7a76"
   },
   "source": [
    "### 13.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc006f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "f8dc006f",
    "outputId": "3c0c6a86-695a-4a21-b45c-a5b9bd0153f3"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ft_history['train_loss'], label='Train Loss')\n",
    "plt.plot(ft_history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ft_history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(ft_history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Fine-Tuned Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_ft_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c2dac",
   "metadata": {
    "id": "808c2dac"
   },
   "source": [
    "### 13.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230087f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "230087f0",
    "outputId": "246d19d8-ea87-4708-de4a-5931cbc85a5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get predictions for the Transfer Learning model (from original `model`)\n",
    "print(\"Generating Confusion Matrix for Transfer Learning Model...\")\n",
    "y_true_tl, y_pred_tl = get_image_predictions(model, val_loader, device)\n",
    "cm_tl = confusion_matrix(y_true_tl, y_pred_tl)\n",
    "\n",
    "# 2. Get predictions for the Fine-Tuning model (from `ft_model`)\n",
    "print(\"Generating Confusion Matrix for Fine-Tuning Model...\")\n",
    "y_true_ft, y_pred_ft = get_image_predictions(ft_model, val_loader, device)\n",
    "cm_ft = confusion_matrix(y_true_ft, y_pred_ft)\n",
    "\n",
    "# 3. Plotting side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot Transfer Learning Confusion Matrix\n",
    "sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_title('Confusion Matrix (Transfer Learning)')\n",
    "\n",
    "# Plot Fine-Tuning Confusion Matrix\n",
    "sns.heatmap(cm_ft, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_title('Confusion Matrix (Fine-Tuning)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710db55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "a710db55",
    "outputId": "88b89cb5-7868-4e30-e496-1da01ee65c19"
   },
   "outputs": [],
   "source": [
    "# Visualize a random validation sample\n",
    "print(\"Transfer Learning:\")\n",
    "sample_id_plot = np.random.choice(val_loader.dataset.df['sample_id'].unique())\n",
    "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)\n",
    "print(\"Fine Tuning:\")\n",
    "plot_sample_with_predictions(ft_model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923f870",
   "metadata": {},
   "source": [
    "## **14. Class Activation Maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c870f",
   "metadata": {},
   "source": [
    "### **14.1 Cam Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Grad-CAM Helper Class ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Hooks\n",
    "        self.handle_fwd = self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.handle_bwd = self.target_layer.register_full_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output)\n",
    "            \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        score = output[0, class_idx]\n",
    "        score.backward()\n",
    "        \n",
    "        # Generate CAM\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        b, k, u, v = gradients.size()\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        alpha = gradients.view(b, k, -1).mean(2)\n",
    "        weights = alpha.view(b, k, 1, 1)\n",
    "        \n",
    "        # Linear combination\n",
    "        cam = (weights * activations).sum(1, keepdim=True)\n",
    "        cam = F.relu(cam) \n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam.view(1, -1)\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-7)\n",
    "        cam = cam.view(1, 1, u, v)\n",
    "        \n",
    "        return cam.detach().cpu().numpy()[0, 0], output\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.handle_fwd.remove()\n",
    "        self.handle_bwd.remove()\n",
    "\n",
    "# --- 2. Mask Overlay Function ---\n",
    "def get_mask_overlay(cam_mask, patch_path, masks_dir, img_fallback=None, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Overlays a Class Activation Map (CAM) onto the ground truth mask.\n",
    "    Returns: (overlay_image, mask_found_boolean)\n",
    "    \"\"\"\n",
    "    bg_img = None\n",
    "    mask_found = False\n",
    "\n",
    "    if masks_dir:\n",
    "        # Derive mask filename: img_xxxx.png -> mask_xxxx.png\n",
    "        filename = os.path.basename(patch_path)\n",
    "        mask_filename = filename.replace('img_', 'mask_')\n",
    "        mask_path = os.path.join(masks_dir, mask_filename)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            try:\n",
    "                mask_pil = Image.open(mask_path).convert('L') \n",
    "                mask_pil = mask_pil.resize((224, 224))\n",
    "                mask_np = np.array(mask_pil)\n",
    "                # Normalize and convert to RGB\n",
    "                mask_display = mask_np.astype(np.float32) / 255.0 \n",
    "                bg_img = np.stack([mask_display]*3, axis=-1)\n",
    "                mask_found = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading mask {mask_filename}: {e}\")\n",
    "\n",
    "    # Fallback Logic\n",
    "    if bg_img is None:\n",
    "        if img_fallback is not None:\n",
    "            bg_img = img_fallback\n",
    "        else:\n",
    "            bg_img = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "\n",
    "    # Process Heatmap\n",
    "    heatmap = cv2.resize(cam_mask, (224, 224))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_colored = np.float32(heatmap_colored) / 255\n",
    "\n",
    "    # Overlay\n",
    "    overlay = (alpha * heatmap_colored) + ((1 - alpha) * bg_img)\n",
    "    overlay = overlay / np.max(overlay) \n",
    "    \n",
    "    return overlay, mask_found\n",
    "\n",
    "# --- 3. Main Visualization Function ---\n",
    "def visualize_sample_analysis(model, df_metadata, sample_id, label_encoder, device, masks_dir=None):\n",
    "    \"\"\"\n",
    "    Visualizes analysis for a sample. \n",
    "    Row 1: [Original], [CAM on Img], [CAM on Mask (HIDDEN if missing)], [Stats]\n",
    "    Row 2: Other Classes CAMs (Fallback to Image if Mask missing)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Setup\n",
    "    sample_rows = df_metadata[df_metadata['sample_id'] == sample_id]\n",
    "    if len(sample_rows) == 0:\n",
    "        print(f\"Sample {sample_id} not found.\")\n",
    "        return\n",
    "\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                             std=[1/0.229, 1/0.224, 1/0.225])\n",
    "    ])\n",
    "\n",
    "    # Inference\n",
    "    patch_probs = []\n",
    "    patch_images = []\n",
    "    \n",
    "    target_layer = model.backbone.features[-1]\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    print(f\"Processing sample {sample_id}...\")\n",
    "    \n",
    "    for _, row in sample_rows.iterrows():\n",
    "        img_pil = Image.open(row['path']).convert('RGB')\n",
    "        img_tensor = transform_pipeline(img_pil).unsqueeze(0).to(device)\n",
    "        patch_images.append({'tensor': img_tensor, 'path': row['path']})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            patch_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    patch_probs = np.vstack(patch_probs)\n",
    "    avg_probs = np.mean(patch_probs, axis=0) \n",
    "    \n",
    "    classes = label_encoder.classes_\n",
    "    pred_class_idx = np.argmax(avg_probs)\n",
    "    pred_label = classes[pred_class_idx]\n",
    "    true_label = sample_rows.iloc[0]['label']\n",
    "    \n",
    "    # Best patch\n",
    "    best_patch_idx = np.argmax(patch_probs[:, pred_class_idx])\n",
    "    best_patch_data = patch_images[best_patch_idx]\n",
    "    img_tensor_active = best_patch_data['tensor'].clone().detach().requires_grad_(True)\n",
    "    patch_path = best_patch_data['path']\n",
    "\n",
    "    # 1. Background Image\n",
    "    img_display = inv_normalize(best_patch_data['tensor'][0]).cpu().detach().numpy()\n",
    "    img_display = np.transpose(img_display, (1, 2, 0))\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "    # 2. Predicted Class CAMs\n",
    "    pred_cam_mask, _ = grad_cam(img_tensor_active, pred_class_idx)\n",
    "    \n",
    "    # A: Overlay on Image\n",
    "    heatmap = cv2.resize(pred_cam_mask, (224, 224))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_colored = np.float32(heatmap_colored) / 255\n",
    "    cam_on_image = (0.5 * heatmap_colored) + (0.5 * img_display)\n",
    "    cam_on_image = cam_on_image / np.max(cam_on_image)\n",
    "    \n",
    "    # B: Overlay on Mask (Row 1, Col 3)\n",
    "    cam_on_mask_main, mask_found_main = get_mask_overlay(\n",
    "        pred_cam_mask, patch_path, masks_dir, img_fallback=img_display\n",
    "    )\n",
    "\n",
    "    # 3. Other Classes CAMs (Row 2)\n",
    "    cam_others_data = {}\n",
    "    for i in range(len(classes)):\n",
    "        if i == pred_class_idx: continue\n",
    "        mask, _ = grad_cam(img_tensor_active, i)\n",
    "        \n",
    "        # We allow fallback here so we can still see activations for other classes\n",
    "        overlay, is_mask = get_mask_overlay(\n",
    "            mask, patch_path, masks_dir, img_fallback=img_display\n",
    "        )\n",
    "        cam_others_data[i] = (overlay, is_mask)\n",
    "        \n",
    "    grad_cam.remove_hooks() \n",
    "\n",
    "    # --- Plotting ---\n",
    "    other_indices = [i for i in range(len(classes)) if i != pred_class_idx]\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 10))\n",
    "    gs = gridspec.GridSpec(2, max(4, len(other_indices)), height_ratios=[1.2, 0.8])\n",
    "\n",
    "    # Row 1\n",
    "    # 1. Original\n",
    "    ax0 = plt.subplot(gs[0, 0])\n",
    "    ax0.imshow(img_display)\n",
    "    ax0.set_title(f\"Most Representative Patch\\nTrue: {true_label}\", fontsize=12, fontweight='bold')\n",
    "    ax0.axis('off')\n",
    "\n",
    "    # 2. Predicted CAM on Image\n",
    "    ax1 = plt.subplot(gs[0, 1])\n",
    "    ax1.imshow(cam_on_image)\n",
    "    ax1.set_title(f\"Focus on Image: {pred_label}\\n(Score: {avg_probs[pred_class_idx]:.2f})\", fontsize=12, fontweight='bold', color='darkblue')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 3. Predicted CAM on Mask (Conditional)\n",
    "    ax2 = plt.subplot(gs[0, 2])\n",
    "    if mask_found_main:\n",
    "        ax2.imshow(cam_on_mask_main)\n",
    "        ax2.set_title(f\"Focus on Mask: {pred_label}\\n(Ground Truth Overlay)\", fontsize=12, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "    else:\n",
    "        # Deactivate subplot if mask is missing (avoid redundancy with ax1)\n",
    "        ax2.set_visible(False)\n",
    "\n",
    "    # 4. Stats\n",
    "    ax3 = plt.subplot(gs[0, 3])\n",
    "    colors = ['#d3d3d3'] * len(classes)\n",
    "    colors[pred_class_idx] = '#4CAF50' if pred_label == true_label else '#F44336'\n",
    "    \n",
    "    bars = ax3.bar(classes, avg_probs, color=colors, alpha=0.85, edgecolor='black')\n",
    "    ax3.set_title(f\"Bag Prediction Probabilities\\nSample ID: {sample_id}\", fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    ax3.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                 f'{height:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Row 2: Other Classes on MASK (with fallback)\n",
    "    for idx, class_idx in enumerate(other_indices):\n",
    "        ax = plt.subplot(gs[1, idx])\n",
    "        overlay, is_mask = cam_others_data[class_idx]\n",
    "        \n",
    "        ax.imshow(overlay)\n",
    "        class_name = classes[class_idx]\n",
    "        bg_type = \"Mask\" if is_mask else \"Img\"\n",
    "        ax.set_title(f\"Activ. on {bg_type}: {class_name}\\n(Score: {avg_probs[class_idx]:.2f})\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1efec",
   "metadata": {},
   "source": [
    "### **14.2 CAM Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample ID from your validation set\n",
    "if SUB_MODEL is None:\n",
    "    SUB_MODEL = 'models/best_model_mobilenetv3_tl.pt'  # Default to TL model if none specified\n",
    "PATCH_MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\",\"masks\")\n",
    "cam_model = MobileNetV3Small(num_classes, DROPOUT_RATE, freeze_backbone=True).to(device)\n",
    "cam_model.load_state_dict(torch.load(SUB_MODEL), strict=True)\n",
    "cam_idx = random.randint(0, len(df_val['sample_id'].unique()) - 1)\n",
    "sample_id_to_test = df_val['sample_id'].iloc[cam_idx] \n",
    "visualize_sample_analysis(cam_model, patches_metadata_df, sample_id_to_test, label_encoder, device)\n",
    "visualize_sample_analysis(cam_model, patches_metadata_df, sample_id_to_test, label_encoder, device, masks_dir=PATCH_MASKS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06322077",
   "metadata": {
    "id": "06322077"
   },
   "source": [
    "## **14. Submission Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9edaa",
   "metadata": {
    "id": "05e9edaa"
   },
   "source": [
    "### 14.1 Create Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942dad1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0942dad1",
    "outputId": "21f19774-17f8-4bb4-b5cd-d96f459d47ef"
   },
   "outputs": [],
   "source": [
    "sub_model = MobileNetV3Small(num_classes, DROPOUT_RATE, freeze_backbone=True).to(device)\n",
    "\n",
    "# 2. Load the best weights from the training\n",
    "sub_model.load_state_dict(torch.load(SUB_MODEL), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f4093",
   "metadata": {
    "id": "8a4f4093"
   },
   "source": [
    "### 14.2 Define Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd99e8",
   "metadata": {
    "id": "80cd99e8"
   },
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    # No augmentation for validation, just resizing and normalization\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36adb9",
   "metadata": {
    "id": "db36adb9"
   },
   "source": [
    "### 14.3 Function: Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21a0e8",
   "metadata": {
    "id": "9c21a0e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def generate_submission(model, submission_folder, method='max_confidence', output_csv=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Generates a submission file by predicting on patches and aggregating results.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        submission_folder: Path to folder containing test patches.\n",
    "        method: 'majority_voting' or 'max_confidence'.\n",
    "        output_csv: Filename for the output CSV.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Get list of test patches\n",
    "    patch_files = sorted([f for f in os.listdir(submission_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    print(f\"Found {len(patch_files)} patches in {submission_folder}\")\n",
    "    print(f\"Aggregation Method: {method}\")\n",
    "\n",
    "    # Store predictions per image\n",
    "    # Structure: { 'img_0001': {'votes': [], 'probs': []}, ... }\n",
    "    image_predictions = {}\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(patch_files):\n",
    "            filepath = os.path.join(submission_folder, filename)\n",
    "\n",
    "            try:\n",
    "                # Extract Sample ID (e.g., \"img_0015_p12.png\" -> \"img_0015\")\n",
    "                # Adjust split logic if your naming convention is different\n",
    "                sample_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "                # Initialize dictionary for this sample if new\n",
    "                if sample_id not in image_predictions:\n",
    "                    image_predictions[sample_id] = {'probs': []}\n",
    "\n",
    "                # Load and Transform\n",
    "                image = Image.open(filepath).convert('RGB')\n",
    "                input_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                # Predict\n",
    "                outputs = model(input_tensor)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "                # Store probabilities for this patch\n",
    "                image_predictions[sample_id]['probs'].append(probs.cpu().numpy()[0])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    # 3. Aggregate Results\n",
    "    final_results = []\n",
    "\n",
    "    print(f\"Aggregating results for {len(image_predictions)} unique samples...\")\n",
    "\n",
    "    for sample_id, data in image_predictions.items():\n",
    "        all_probs = np.array(data['probs']) # Shape: [Num_Patches, Num_Classes]\n",
    "\n",
    "        # Ensure sample_index has .png extension as requested\n",
    "        sample_index_name = f\"{sample_id}.png\"\n",
    "\n",
    "        if len(all_probs) == 0:\n",
    "            final_results.append({'sample_index': sample_index_name, 'label': \"Luminal A\"}) # Default fallback\n",
    "            continue\n",
    "\n",
    "        if method == 'majority_voting':\n",
    "            # Get class prediction for each patch\n",
    "            patch_preds = np.argmax(all_probs, axis=1)\n",
    "            # Find most frequent class\n",
    "            counts = np.bincount(patch_preds)\n",
    "            final_class_idx = np.argmax(counts)\n",
    "\n",
    "        elif method == 'max_confidence':\n",
    "            # Option A: Average probabilities (Soft Voting) - usually best/safest\n",
    "            avg_probs = np.mean(all_probs, axis=0)\n",
    "            final_class_idx = np.argmax(avg_probs)\n",
    "\n",
    "            # Option B: Strict Max Confidence (Uncomment if you prefer this)\n",
    "            # max_probs = np.max(all_probs, axis=0)\n",
    "            # final_class_idx = np.argmax(max_probs)\n",
    "\n",
    "        # Decode Label\n",
    "        pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
    "        final_results.append({'sample_index': sample_index_name, 'label': pred_label})\n",
    "\n",
    "    # 4. Create Pandas DataFrame and Save\n",
    "    df_submission = pd.DataFrame(final_results)\n",
    "\n",
    "    # Ensure correct column order\n",
    "    df_submission = df_submission[['sample_index', 'label']]\n",
    "\n",
    "    # Sort by sample_index for neatness\n",
    "    df_submission.sort_values('sample_index', inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_submission.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Submission saved to {output_csv}\")\n",
    "    print(df_submission.head())\n",
    "\n",
    "    return df_submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078d160",
   "metadata": {
    "id": "7078d160"
   },
   "source": [
    "### 14.3 Create the Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e05e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "187e05e0",
    "outputId": "f8fb865f-86ae-4157-8cbc-a93efa39be5f"
   },
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# Replace 'path/to/SUBMISSION_PATCHES' with the actual path\n",
    "# If your folder is just named SUBMISSION_PATCHES in current dir:\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "date_time_str = now.strftime(\"%d_%b-%H_%M\")\n",
    "\n",
    "sub_dir = os.path.join(os.path.pardir, \"submission_csvs\")\n",
    "OUTPUT_NAME = os.path.join(sub_dir, f\"submission_ft--{date_time_str}.csv\")\n",
    "\n",
    "\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "# Check if folder exists\n",
    "if os.path.exists(SUBMISSION_PATCHES_OUT):\n",
    "    # Method 1: Max Confidence / Average Probability (Recommended)\n",
    "    df_sub_max_conf = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='max_confidence', output_csv=OUTPUT_NAME)\n",
    "    print(f\"Submission CSV saved to: {OUTPUT_NAME}\")\n",
    "\n",
    "    # Method 2: Majority Voting (Optional, uncomment to run)\n",
    "    # df_sub_majority_voting = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='majority_voting', output_csv=\"submission_voting.csv\")\n",
    "else:\n",
    "    print(f\"Directory '{SUBMISSION_PATCHES_OUT}' not found. Please create it or set the correct path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac7fe0-af4f-4b6d-baff-74b3864ca569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634e89c-8df7-4b82-898b-4d01972d0085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
