{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3293c812",
      "metadata": {
        "id": "3293c812"
      },
      "source": [
        "## **1. Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4631b342",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4631b342",
        "outputId": "0e68bb19-fc1e-4091-f1e6-439dde5cb7d7"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#cur_dir = \"/content/drive/MyDrive/CH2/Notebooks\"\n",
        "#%cd $cur_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8bfd385",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8bfd385",
        "outputId": "21e25648-2e8c-4b98-dfa1-62a7571691b4"
      },
      "outputs": [],
      "source": [
        "#%pip install torchview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edee15c7",
      "metadata": {
        "id": "edee15c7"
      },
      "source": [
        "## **2. Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963c7403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963c7403",
        "outputId": "f7791470-9109-429d-c95d-5b5c3d9286c2"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchview import draw_graph\n",
        "from scipy import ndimage\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import cv2\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.gridspec as gridspec\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b5b1ee8",
      "metadata": {
        "id": "3b5b1ee8"
      },
      "source": [
        "## **3. Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xdsqFwFHgM49",
      "metadata": {
        "id": "xdsqFwFHgM49"
      },
      "outputs": [],
      "source": [
        "USE_MASKED_PATCHES = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3da911",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3da911",
        "outputId": "88625ef0-d721-493b-c45e-24c1541fe480"
      },
      "outputs": [],
      "source": [
        "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
        "\n",
        "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
        "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
        "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
        "\n",
        "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
        "SOURCE_FOLDER = train_data_path\n",
        "\n",
        "if USE_MASKED_PATCHES:\n",
        "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"train_patches_masked\")\n",
        "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"submission_patches_masked\")\n",
        "else:\n",
        "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\")\n",
        "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\")\n",
        "\n",
        "print(f\"Dataset path: {datasets_path}\")\n",
        "print(f\"Train data path: {train_data_path}\")\n",
        "print(f\"Train labels path: {train_labels_path}\")\n",
        "print(f\"Test data path: {test_data_path}\")\n",
        "print(f\"Patches output path: {PATCHES_OUT}\")\n",
        "print(f\"Submission patches output path: {SUBMISSION_PATCHES_OUT}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e427b09e",
      "metadata": {
        "id": "e427b09e"
      },
      "source": [
        "## **4. Train/Val Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc7455a",
      "metadata": {
        "id": "7cc7455a"
      },
      "outputs": [],
      "source": [
        "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
        "    \"\"\"\n",
        "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
        "    \"\"\"\n",
        "    # 1. Load the labels CSV\n",
        "    # Assuming CSV structure: [image_id, label] or similar\n",
        "    df_labels = pd.read_csv(labels_csv_path)\n",
        "\n",
        "    # Standardize column names for easier merging\n",
        "    # We assume the first column is the ID and the second is the Label\n",
        "    id_col = df_labels.columns[0]\n",
        "    label_col = df_labels.columns[1]\n",
        "\n",
        "    # Ensure IDs in CSV are strings (to match filenames)\n",
        "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
        "\n",
        "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
        "    # because our parsed Bag IDs won't have them.\n",
        "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
        "\n",
        "    # 2. List all patch files\n",
        "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
        "\n",
        "    # 3. Parse filenames to get Bag IDs\n",
        "    data = []\n",
        "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
        "\n",
        "    for filename in patch_files:\n",
        "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
        "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
        "\n",
        "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
        "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
        "        try:\n",
        "            bag_id = filename.rsplit('_p', 1)[0]\n",
        "\n",
        "            data.append({\n",
        "                'filename': filename,\n",
        "                'sample_id': bag_id,\n",
        "                'path': os.path.join(patches_dir, filename)\n",
        "            })\n",
        "        except IndexError:\n",
        "            print(f\"Skipping malformed filename: {filename}\")\n",
        "\n",
        "    # Create temporary patches DataFrame\n",
        "    df_patches = pd.DataFrame(data)\n",
        "\n",
        "    # 4. Merge patches with labels\n",
        "    # This assigns the correct Bag Label to every Patch in that Bag\n",
        "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
        "\n",
        "    # 5. Clean up and Rename\n",
        "    # Keep only required columns\n",
        "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
        "\n",
        "    # Rename label column to standard 'label' if it isn't already\n",
        "    df = df.rename(columns={label_col: 'label'})\n",
        "\n",
        "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "633b8e05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633b8e05",
        "outputId": "19fba602-8760-478c-a65c-f3541f8f044b"
      },
      "outputs": [],
      "source": [
        "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
        "\n",
        "# Verify the result\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(patches_metadata_df.head().drop(columns=['path']))\n",
        "print(\"\\nPatches per Bag (Distribution):\")\n",
        "print(patches_metadata_df['sample_id'].value_counts().describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6c6cd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc6c6cd3",
        "outputId": "dd7a643b-2eee-4ba7-958b-837aca725d5a"
      },
      "outputs": [],
      "source": [
        "# Add Label Encoding\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Label Encoding\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
        "\n",
        "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
        "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
        "print(f\"\\nLabel Mapping:\")\n",
        "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
        "    print(f\"  {orig} -> {enc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52314ae9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52314ae9",
        "outputId": "3a7d0847-1360-4a0d-ca98-e19fb158bbfc"
      },
      "outputs": [],
      "source": [
        "# Train/Val Split on Original Images (not patches)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Train/Val Split on Original Images\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get unique sample IDs\n",
        "unique_samples = patches_metadata_df['sample_id'].unique()\n",
        "print(f\"\\nTotal unique samples (original images): {len(unique_samples)}\")\n",
        "\n",
        "# Split samples into train (80%) and val (20%)\n",
        "train_samples, val_samples = train_test_split(\n",
        "    unique_samples,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples, 'label_encoded'].values\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_samples)}\")\n",
        "print(f\"Val samples: {len(val_samples)}\")\n",
        "\n",
        "# Create train and val DataFrames by filtering patches\n",
        "df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_samples)].reset_index(drop=True)\n",
        "df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_samples)].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nTrain patches: {len(df_train)}\")\n",
        "print(f\"Val patches: {len(df_val)}\")\n",
        "print(f\"\\nTrain label distribution:\\n{df_train['label'].value_counts()}\")\n",
        "print(f\"\\nVal label distribution:\\n{df_val['label'].value_counts()}\")\n",
        "\n",
        "# Print percentage distribution\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"Percentage Distribution\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nTrain label percentage:\\n{df_train['label'].value_counts(normalize=True) * 100}\")\n",
        "print(f\"\\nVal label percentage:\\n{df_val['label'].value_counts(normalize=True) * 100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b797aff",
      "metadata": {
        "id": "4b797aff"
      },
      "source": [
        "## **5. Transformations & Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94459d26",
      "metadata": {
        "id": "94459d26"
      },
      "outputs": [],
      "source": [
        "# Define augmentation for training with enhanced transformations\n",
        "train_augmentation = transforms.Compose([\n",
        "    # Geometric transformations\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    #transforms.RandomRotation(degrees=15),  # Small rotations to handle orientation variations\n",
        "    #transforms.RandomAffine(\n",
        "    #    degrees=0,\n",
        "    #    translate=(0.1, 0.1),  # Reduced from 0.2 for more conservative shifts\n",
        "    #    scale=None,  # Add scale variation\n",
        "    #    shear=10  # Add shear transformation\n",
        "    #),\n",
        "\n",
        "    # Color/appearance transformations\n",
        "    #transforms.ColorJitter(\n",
        "    #    brightness=0.2,  # Adjust brightness\n",
        "    #    contrast=0.2,    # Adjust contrast\n",
        "    #    saturation=0.2,  # Adjust saturation\n",
        "    #    hue=0.1          # Slight hue variation\n",
        "    #),\n",
        "    #transforms.RandomGrayscale(p=0.1),  # Occasionally convert to grayscale to improve robustness\n",
        "\n",
        "    # Occlusion simulation\n",
        "    #transforms.RandomErasing(\n",
        "    #    p=0.1,  # Reduced probability for more balanced augmentation\n",
        "    #    scale=(0.02, 0.15),  # Reduced max scale\n",
        "    #    ratio=(0.3, 3.3)  # Aspect ratio range\n",
        "    #),\n",
        "\n",
        "    # Optional: Add Gaussian blur for noise robustness\n",
        "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8cdd92",
      "metadata": {
        "id": "dc8cdd92"
      },
      "source": [
        "## **6. Custom Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b69bba",
      "metadata": {
        "id": "29b69bba"
      },
      "outputs": [],
      "source": [
        "# Load the Phikon image processor\n",
        "phikon_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
        "\n",
        "# ImageNet normalization statistics (fallback)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "class TissueDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for loading images with Phikon preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self, df, augmentation=None, use_phikon_processor=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame with 'path' and 'label_encoded' columns.\n",
        "            augmentation (callable, optional): Augmentation transforms to be applied on a sample.\n",
        "            use_phikon_processor (bool): If True, uses Phikon's official preprocessor.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.augmentation = augmentation\n",
        "        self.use_phikon_processor = use_phikon_processor\n",
        "\n",
        "        if use_phikon_processor:\n",
        "            self.processor = phikon_processor\n",
        "        else:\n",
        "            # Fallback: manual preprocessing\n",
        "            self.to_tensor = transforms.Compose([\n",
        "                transforms.Resize(TARGET_SIZE),\n",
        "                transforms.ToImage(),\n",
        "                transforms.ToDtype(torch.float32, scale=True)\n",
        "            ])\n",
        "            self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['path']\n",
        "        label = self.df.iloc[idx]['label_encoded']\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.use_phikon_processor:\n",
        "            # Use Phikon's official processor\n",
        "            processed = self.processor(image, return_tensors=\"pt\")\n",
        "            image_tensor = processed['pixel_values'].squeeze(0)\n",
        "        else:\n",
        "            # Manual preprocessing\n",
        "            image_tensor = self.to_tensor(image)\n",
        "            if self.augmentation:\n",
        "                image_tensor = self.augmentation(image_tensor)\n",
        "            image_tensor = self.normalize(image_tensor)\n",
        "\n",
        "        # Apply augmentation if using Phikon processor\n",
        "        if self.use_phikon_processor and self.augmentation:\n",
        "            image_tensor = self.augmentation(image_tensor)\n",
        "\n",
        "        return image_tensor, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa4cb5a",
      "metadata": {
        "id": "ffa4cb5a"
      },
      "source": [
        "## **7. Data Loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19525f4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19525f4f",
        "outputId": "013e1318-efec-467b-fe1f-587dbf8dcfef"
      },
      "outputs": [],
      "source": [
        "# Instantiate Datasets with Phikon processor\n",
        "train_dataset = TissueDataset(df_train, augmentation=train_augmentation, use_phikon_processor=True)\n",
        "val_dataset = TissueDataset(df_val, augmentation=None, use_phikon_processor=True)\n",
        "\n",
        "# Batch Size: 32 or 64 is standard for ResNet18/50 on 1MP images\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "cpu_cores = os.cpu_count() or 2\n",
        "num_workers = max(2, min(4, cpu_cores))\n",
        "# Instantiate Loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,          # Shuffle patches to break batch correlations\n",
        "    num_workers=0,         # Adjust based on your CPU\n",
        "    pin_memory=True        # Faster data transfer to CUDA\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,         # No need to shuffle validation\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train Loader: {len(train_loader)} batches\")\n",
        "print(f\"Val Loader: {len(val_loader)} batches\")\n",
        "print(f\"Num workers: {train_loader.num_workers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4212bfef",
      "metadata": {
        "id": "4212bfef"
      },
      "outputs": [],
      "source": [
        "def show_batch(loader, count=4):\n",
        "    images, labels = next(iter(loader))\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Denormalize for visualization\n",
        "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
        "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
        "\n",
        "    for i in range(count):\n",
        "        ax = plt.subplot(1, count, i + 1)\n",
        "\n",
        "        img = images[i]\n",
        "        #img = img * std + mean  # Un-normalize\n",
        "\n",
        "        img = torch.clamp(img, 0, 1)  # Clip to valid range\n",
        "\n",
        "        plt.imshow(img.permute(1, 2, 0)) # CHW -> HWC\n",
        "        plt.title(f\"Label: {labels[i].item()}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2RPTp88c0Arh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "2RPTp88c0Arh",
        "outputId": "67b80295-47e4-44f5-d7e8-568e8eeae94b"
      },
      "outputs": [],
      "source": [
        "print(\"\\nVisualizing Training Batch (Augmented):\")\n",
        "show_batch(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c89d235",
      "metadata": {
        "id": "7c89d235"
      },
      "source": [
        "## **8. Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e381193",
      "metadata": {
        "id": "9e381193"
      },
      "outputs": [],
      "source": [
        "DROPOUT_RATE = 0.3\n",
        "HIDDEN_SIZE = 512\n",
        "L2_REG = 1e-4\n",
        "\n",
        "NUM_EPOCHS = 100  # Increased since we have early stopping\n",
        "PATIENCE = 8    # Stop if val_loss doesn't improve for 5 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1d978b",
      "metadata": {
        "id": "4f1d978b"
      },
      "source": [
        "## **9. Model Definition (Transfer Learning - PhikonV2)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6c4ec0",
      "metadata": {
        "id": "5c6c4ec0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, embedding_dim]\n",
        "        weights = torch.softmax(self.attention(x), dim=1)  # [batch_size, seq_len, 1]\n",
        "        return (x * weights).sum(dim=1)  # [batch_size, embedding_dim]\n",
        "\n",
        "class PhikonClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, model_name=\"owkin/phikon\", dropout_rate=0.3, freeze_backbone=True):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Loading Phikon backbone: {model_name}...\")\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        if \"v2\" in model_name:\n",
        "            self.embedding_dim = 1024\n",
        "        else:\n",
        "            self.embedding_dim = 768\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Attention pooling instead of just taking CLS token\n",
        "        self.attention_pool = AttentionPooling(self.embedding_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.embedding_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.backbone(pixel_values=x)\n",
        "        tokens = outputs.last_hidden_state  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # Apply attention pooling\n",
        "        pooled = self.attention_pool(tokens)  # [batch_size, embedding_dim]\n",
        "\n",
        "        return self.classifier(pooled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439ba3db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "439ba3db",
        "outputId": "3e5c6eff-1163-40d5-963c-56c7e3a2976e"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "# Ensure device is defined (usually from previous cells, but safe to redefine if standalone)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming label_encoder is defined in your notebook scope\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model = PhikonClassifier(\n",
        "    num_classes=num_classes,\n",
        "    model_name=\"owkin/phikon\",  # Change to \"owkin/phikon-v2\" for the larger model\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model initialized (Phikon) with {num_classes} output classes.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfb49a6",
      "metadata": {
        "id": "fcfb49a6"
      },
      "source": [
        "## **10. Loss and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3VVe1M_Nw5oa",
      "metadata": {
        "id": "3VVe1M_Nw5oa"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = self.ce(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b270c87c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b270c87c",
        "outputId": "0b660f0c-d985-4d96-9d1a-b008c7dc4b91"
      },
      "outputs": [],
      "source": [
        "# Re-define loss and optimizer for the new model\n",
        "\n",
        "# Set a scalar alpha for Focal Loss\n",
        "\n",
        "alpha = None\n",
        "criterion = FocalLoss(alpha=alpha, gamma=2.3)\n",
        "\n",
        "# Only optimize parameters that require gradients (the head)\n",
        "# The backbone is frozen by default in the class above\n",
        "optimizer = torch.optim.RAdam(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=1e-3,\n",
        "    weight_decay=L2_REG\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3\n",
        ")\n",
        "\n",
        "print(\"Optimizer and Scheduler reset for Phikon with Focal Loss (scalar alpha).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7aa519",
      "metadata": {
        "id": "9c7aa519"
      },
      "source": [
        "## **11. Function: Training & Validation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37abb343",
      "metadata": {
        "id": "37abb343"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Lists to store all predictions and labels for F1 calculation\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    loop = tqdm(loader, leave=False)\n",
        "\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics accumulation\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Move to CPU and convert to numpy for sklearn metrics\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    # Calculate F1 Score (Macro for imbalanced data)\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_f1\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e440fc05",
      "metadata": {
        "id": "e440fc05"
      },
      "source": [
        "## **12. Training Loop: Transfer Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d6689e",
      "metadata": {
        "id": "41d6689e"
      },
      "source": [
        "### 12.1 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897de7e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "897de7e8",
        "outputId": "881ce96b-2e94-49c2-a1fc-5786df0ac4db"
      },
      "outputs": [],
      "source": [
        "best_val_f1 = 0.0\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_tl_epoch = 0\n",
        "model_saved = False\n",
        "\n",
        "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
        "\n",
        "print(f\"Starting Training with Phikonv2 (Patience: {PATIENCE})...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update Scheduler (based on Loss)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "\n",
        "    # --- Checkpointing (Save Best Model based on F1) ---\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model.state_dict(), 'models/best_model_phikonv2_tl.pt')\n",
        "        model_saved = True\n",
        "        patience_counter = 0  # Reset counter\n",
        "        best_tl_epoch = epoch + 1\n",
        "    else:\n",
        "        model_saved = False\n",
        "        patience_counter += 1\n",
        "\n",
        "    if model_saved:\n",
        "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE} âœ“\")\n",
        "    else:\n",
        "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(\"   >>> Early Stopping Triggered! Training stopped.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437dbdd6",
      "metadata": {
        "id": "437dbdd6"
      },
      "source": [
        "### 12.2 Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6263355",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "a6263355",
        "outputId": "0fe490a5-4b1a-4b3d-c375-12090a07749b"
      },
      "outputs": [],
      "source": [
        "# Plotting results\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_f1'], label='Train F1 (Macro)')\n",
        "plt.plot(history['val_f1'], label='Val F1 (Macro)')\n",
        "plt.legend()\n",
        "plt.title('F1 Score')\n",
        "plt.show()\n",
        "\n",
        "print(\"Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67506d70",
      "metadata": {
        "id": "67506d70"
      },
      "source": [
        "### 12.3 Function: Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1be552",
      "metadata": {
        "id": "3e1be552"
      },
      "outputs": [],
      "source": [
        "def get_image_predictions(model, loader, device):\n",
        "    \"\"\"\n",
        "    Aggregates patch-level predictions to image-level.\n",
        "    Strategy: Average the Softmax probabilities of all patches in a bag,\n",
        "    then take the argmax. This handles 'noisy/benign' patches well.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Access the dataframe and transform stored in the dataset\n",
        "    dataset = loader.dataset\n",
        "    df = dataset.df\n",
        "    transform = dataset.transform\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Get unique sample IDs (original images)\n",
        "    sample_ids = df['sample_id'].unique()\n",
        "\n",
        "    print(f\"\\nAggregating predictions for {len(sample_ids)} unique images...\")\n",
        "\n",
        "    for sample_id in tqdm(sample_ids, leave=False):\n",
        "        # Get all patches belonging to this image\n",
        "        sample_patches = df[df['sample_id'] == sample_id]\n",
        "\n",
        "        # Ground Truth (all patches share the image label)\n",
        "        true_label = sample_patches.iloc[0]['label_encoded']\n",
        "        y_true.append(true_label)\n",
        "\n",
        "        # Load and process all patches for this image\n",
        "        patches = []\n",
        "        for img_path in sample_patches['path']:\n",
        "            try:\n",
        "                # Load image (ensure RGB)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                if transform:\n",
        "                    img = transform(img)\n",
        "                patches.append(img)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if not patches:\n",
        "            y_pred.append(true_label) # Fallback (should not happen)\n",
        "            continue\n",
        "\n",
        "        # Stack patches into a single batch: [Num_Patches, 3, 224, 224]\n",
        "        batch = torch.stack(patches).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(batch)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "            # --- Aggregation: Mean Probability ---\n",
        "            # Averaging probabilities reduces the impact of outliers (benign patches)\n",
        "            avg_probs = torch.mean(probs, dim=0)\n",
        "            pred_label = torch.argmax(avg_probs).item()\n",
        "\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "    return y_true, y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2068748",
      "metadata": {
        "id": "c2068748"
      },
      "source": [
        "### 12.4 Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd88a0a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "cd88a0a7",
        "outputId": "ac4414f6-b453-49d7-8123-599f2b1c150d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3. Calculate and Plot Confusion Matrix\n",
        "print(\"Generating Confusion Matrix on Original Images...\")\n",
        "y_true_img, y_pred_img = get_image_predictions(model, val_loader, device)\n",
        "\n",
        "# Compute Matrix\n",
        "cm = confusion_matrix(y_true_img, y_pred_img)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Aggregated per Image)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8319bc10",
      "metadata": {
        "id": "8319bc10"
      },
      "outputs": [],
      "source": [
        "def plot_sample_with_predictions(model, loader, device, label_encoder, sample_id=None, aggregation_method='max_confidence', normalize_imagenet=False):\n",
        "    \"\"\"Plot all patches of a single sample and the aggregated image prediction.\"\"\"\n",
        "    import math\n",
        "    model.eval()\n",
        "    dataset = loader.dataset\n",
        "    df = dataset.df\n",
        "\n",
        "    # Pick a sample_id\n",
        "    if sample_id is None:\n",
        "        sample_id = np.random.choice(df['sample_id'].unique())\n",
        "    sample_patches = df[df['sample_id'] == sample_id].reset_index(drop=True)\n",
        "\n",
        "    # Load and transform all patches\n",
        "    images_tensors = []\n",
        "    display_imgs = []\n",
        "    for _, row in sample_patches.iterrows():\n",
        "        img = Image.open(row['path']).convert('RGB')\n",
        "        if dataset.transform:\n",
        "            tensor_img = dataset.transform(img)\n",
        "        else:\n",
        "            tensor_img = transforms.ToTensor()(img)\n",
        "        images_tensors.append(tensor_img)\n",
        "        # denormalize for display\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
        "        if normalize_imagenet:\n",
        "            display_imgs.append(torch.clamp(tensor_img * std + mean, 0, 1))\n",
        "        else:\n",
        "            display_imgs.append(tensor_img)\n",
        "\n",
        "    batch = torch.stack(images_tensors).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(batch)\n",
        "        probs = torch.softmax(logits, dim=1).cpu()\n",
        "\n",
        "    patch_preds = probs.argmax(dim=1).numpy()\n",
        "    patch_confs = probs.max(dim=1).values.numpy()\n",
        "\n",
        "    if aggregation_method == 'max_confidence':\n",
        "        # Average probabilities (Soft Voting)\n",
        "        image_probs = probs.mean(dim=0).numpy()\n",
        "        image_pred = image_probs.argmax()\n",
        "        image_conf = image_probs[image_pred]\n",
        "    elif aggregation_method == 'majority_voting':\n",
        "        # Hard Voting\n",
        "        counts = np.bincount(patch_preds, minlength=len(label_encoder.classes_))\n",
        "        image_pred = counts.argmax()\n",
        "        # Normalize counts for visualization purposes\n",
        "        image_probs = counts / counts.sum()\n",
        "        image_conf = image_probs[image_pred]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n",
        "\n",
        "    image_pred_label = label_encoder.inverse_transform([image_pred])[0]\n",
        "    true_label = label_encoder.inverse_transform([sample_patches.iloc[0]['label_encoded']])[0]\n",
        "\n",
        "    cols = min(6, len(sample_patches))\n",
        "    rows = math.ceil(len(sample_patches) / cols)\n",
        "    fig = plt.figure(figsize=(3*cols + 4, 3*rows))\n",
        "    gs = fig.add_gridspec(rows, cols + 1, width_ratios=[1]*cols + [1.3])\n",
        "\n",
        "    # Patch grid\n",
        "    for idx, (img_disp, pred, conf) in enumerate(zip(display_imgs, patch_preds, patch_confs)):\n",
        "        ax = fig.add_subplot(gs[idx // cols, idx % cols])\n",
        "        ax.imshow(img_disp.permute(1,2,0))\n",
        "        lbl = label_encoder.inverse_transform([pred])[0]\n",
        "        ax.set_title(f\"{lbl}\\n{conf:.2%}\", fontsize=9)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Aggregated distribution\n",
        "    ax_bar = fig.add_subplot(gs[:, -1])\n",
        "    class_names = label_encoder.classes_\n",
        "    colors = ['green' if i == image_pred else 'steelblue' for i in range(len(class_names))]\n",
        "    ax_bar.barh(class_names, image_probs, color=colors)\n",
        "    ax_bar.set_xlabel('Probability' if aggregation_method == 'max_confidence' else 'Vote Share')\n",
        "    ax_bar.set_xlim([0,1])\n",
        "    ax_bar.set_title(f\"Sample: {sample_id}\\nTrue: {true_label} | Pred: {image_pred_label} ({image_conf:.2%})\\nMethod: {aggregation_method}\")\n",
        "    for i, prob in enumerate(image_probs):\n",
        "        ax_bar.text(prob + 0.02, i, f\"{prob:.3f}\", va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad76976",
      "metadata": {
        "id": "2ad76976"
      },
      "source": [
        "## **13. Training Loop: Fine Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af01cc5a",
      "metadata": {
        "id": "af01cc5a"
      },
      "source": [
        "### 13.1 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ed2bdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90ed2bdb",
        "outputId": "721fc73b-309a-4907-8a15-7a06e0ad0290"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize the NEW model instance\n",
        "#    (Make sure to use the same class definition you used for training)\n",
        "ft_model = PhikonClassifier(\n",
        "    num_classes=num_classes,\n",
        "    model_name=\"owkin/phikon\",  # Change to \"owkin/phikon-v2\" for the larger model\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "# 2. Load the best weights from the first phase\n",
        "ft_model.load_state_dict(torch.load(\"models/best_model_phikonv2_tl.pt\"), strict=True)\n",
        "\n",
        "# 3. Unfreeze parameters\n",
        "#    First, ensure everything is frozen\n",
        "for param in ft_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#    Unfreeze the Classifier (Head)\n",
        "for param in ft_model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "#    Unfreeze the Classifier (Head)\n",
        "for param in ft_model.attention_pool.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "#    Unfreeze the last 2 blocks of the Backbone (Features)\n",
        "#    For a Vision Transformer (ViT) like Phikon, the layers are in `backbone.encoder.layer`\n",
        "for block in ft_model.backbone.encoder.layer[-2:]:\n",
        "    for param in block.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Optimizer\n",
        "ft_optimizer = torch.optim.RAdam(\n",
        "    [p for p in ft_model.parameters() if p.requires_grad],\n",
        "    lr=1e-3,\n",
        "    weight_decay=L2_REG\n",
        ")\n",
        "\n",
        "# 5. New Scheduler for the new optimizer\n",
        "ft_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    ft_optimizer, mode='min', factor=0.1, patience=3,\n",
        ")\n",
        "\n",
        "print(\"Starting Fine-Tuning\")\n",
        "\n",
        "# Reset history\n",
        "ft_history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
        "best_val_f1 = 0.0\n",
        "patience_counter_ft = 0\n",
        "best_ft_epoch = 0\n",
        "model_saved = False\n",
        "# Training Loop using ft_model and ft_optimizer\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss, train_f1 = train_one_epoch(ft_model, train_loader, criterion, ft_optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_f1 = validate(ft_model, val_loader, criterion, device)\n",
        "\n",
        "    # Update Scheduler\n",
        "    ft_scheduler.step(val_loss)\n",
        "\n",
        "    ft_history['train_loss'].append(train_loss)\n",
        "    ft_history['train_f1'].append(train_f1)\n",
        "    ft_history['val_loss'].append(val_loss)\n",
        "    ft_history['val_f1'].append(val_f1)\n",
        "\n",
        "\n",
        "\n",
        "    # --- Checkpointing (Save Best Model based on F1) ---\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(ft_model.state_dict(), 'models/best_model_phikonv2_ft.pt')\n",
        "        best_ft_epoch = epoch + 1\n",
        "        model_saved = True\n",
        "        patience_counter_ft = 0  # Reset counter\n",
        "    else:\n",
        "        model_saved = False\n",
        "        patience_counter_ft += 1\n",
        "        # --- Early Stopping (Monitor Val Loss) ---\n",
        "    # We monitor Loss for stopping because F1 can fluctuate even if model is converging\n",
        "\n",
        "\n",
        "    if model_saved:\n",
        "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE} | âœ“\")\n",
        "    else:\n",
        "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter_ft >= PATIENCE:\n",
        "        print(\"Early Stopping Triggered! Best FT Epoch: {} with Val F1: {:.4f}\".format(best_ft_epoch, best_val_f1))\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2bb7a76",
      "metadata": {
        "id": "b2bb7a76"
      },
      "source": [
        "### 13.2 Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8dc006f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "f8dc006f",
        "outputId": "add32e1d-fb47-4f11-8965-a30db256ce7c"
      },
      "outputs": [],
      "source": [
        "# Plotting results\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ft_history['train_loss'], label='Train Loss')\n",
        "plt.plot(ft_history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ft_history['train_f1'], label='Train F1 (Macro)')\n",
        "plt.plot(ft_history['val_f1'], label='Val F1 (Macro)')\n",
        "plt.legend()\n",
        "plt.title('F1 Score')\n",
        "plt.show()\n",
        "\n",
        "print(\"Best Fine-Tuned Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_ft_epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f742a50",
      "metadata": {
        "id": "5f742a50"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "808c2dac",
      "metadata": {
        "id": "808c2dac"
      },
      "source": [
        "### 13.3 Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "230087f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "230087f0",
        "outputId": "484da88b-f9fc-4d11-c802-f073d7d9fcc7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Get predictions for the Transfer Learning model (from original `model`)\n",
        "print(\"Generating Confusion Matrix for Transfer Learning Model...\")\n",
        "y_true_tl, y_pred_tl = get_image_predictions(model, val_loader, device)\n",
        "cm_tl = confusion_matrix(y_true_tl, y_pred_tl)\n",
        "\n",
        "# 2. Get predictions for the Fine-Tuning model (from `ft_model`)\n",
        "print(\"Generating Confusion Matrix for Fine-Tuning Model...\")\n",
        "y_true_ft, y_pred_ft = get_image_predictions(ft_model, val_loader, device)\n",
        "cm_ft = confusion_matrix(y_true_ft, y_pred_ft)\n",
        "\n",
        "# 3. Plotting side-by-side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Plot Transfer Learning Confusion Matrix\n",
        "sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_, ax=axes[0])\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_title('Confusion Matrix (Transfer Learning)')\n",
        "\n",
        "# Plot Fine-Tuning Confusion Matrix\n",
        "sns.heatmap(cm_ft, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_, ax=axes[1])\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "axes[1].set_ylabel('True Label')\n",
        "axes[1].set_title('Confusion Matrix (Fine-Tuning)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a710db55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "a710db55",
        "outputId": "86fd8309-37cd-412b-ecc5-ec227a5717cd"
      },
      "outputs": [],
      "source": [
        "# Visualize a random validation sample\n",
        "print(\"Transfer Learning:\")\n",
        "sample_id_plot = np.random.choice(val_loader.dataset.df['sample_id'].unique())\n",
        "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)\n",
        "print(\"Fine Tuning:\")\n",
        "plot_sample_with_predictions(ft_model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06322077",
      "metadata": {
        "id": "06322077"
      },
      "source": [
        "## **14. Submission Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e9edaa",
      "metadata": {
        "id": "05e9edaa"
      },
      "source": [
        "### 14.1 Create Submission Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0942dad1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0942dad1",
        "outputId": "88e714f7-68f5-4a8b-e46b-06125b5ea6e5"
      },
      "outputs": [],
      "source": [
        "sub_model =PhikonClassifier(\n",
        "    num_classes=num_classes,\n",
        "    model_name=\"owkin/phikon\",  # Change to \"owkin/phikon-v2\" for the larger model\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ").to(device)\n",
        "# 2. Load the best weights from the first phase\n",
        "sub_model.load_state_dict(torch.load('models/best_model_phikonv2_ft.pt'), strict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4f4093",
      "metadata": {
        "id": "8a4f4093"
      },
      "source": [
        "### 14.2 Define Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80cd99e8",
      "metadata": {
        "id": "80cd99e8"
      },
      "outputs": [],
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(TARGET_SIZE),\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c21a0e8",
      "metadata": {
        "id": "9c21a0e8"
      },
      "outputs": [],
      "source": [
        "def generate_submission(model, submission_folder, method='max_confidence', output_csv=\"submission.csv\"):\n",
        "    \"\"\"\n",
        "    Generates a submission file by predicting on patches and aggregating results.\n",
        "    Uses Phikon preprocessing.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Get list of test patches\n",
        "    patch_files = sorted([f for f in os.listdir(submission_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    print(f\"Found {len(patch_files)} patches in {submission_folder}\")\n",
        "    print(f\"Aggregation Method: {method}\")\n",
        "\n",
        "    # Store predictions per image\n",
        "    # Structure: { 'img_0001': {'votes': [], 'probs': []}, ... }\n",
        "    image_predictions = {}\n",
        "\n",
        "    print(\"Running inference with Phikon preprocessing...\")\n",
        "    with torch.no_grad():\n",
        "        for filename in tqdm(patch_files):\n",
        "            filepath = os.path.join(submission_folder, filename)\n",
        "\n",
        "            try:\n",
        "                sample_id = filename.rsplit('_p', 1)[0]\n",
        "\n",
        "                if sample_id not in image_predictions:\n",
        "                    image_predictions[sample_id] = {'probs': []}\n",
        "\n",
        "                # Load image and apply Phikon preprocessing\n",
        "                image = Image.open(filepath).convert('RGB')\n",
        "                processed = phikon_processor(image, return_tensors=\"pt\")\n",
        "                input_tensor = processed['pixel_values'].to(device)\n",
        "\n",
        "                # Predict\n",
        "                outputs = model(input_tensor)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                image_predictions[sample_id]['probs'].append(probs.cpu().numpy()[0])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # 3. Aggregate Results\n",
        "    final_results = []\n",
        "\n",
        "    print(f\"Aggregating results for {len(image_predictions)} unique samples...\")\n",
        "\n",
        "    for sample_id, data in image_predictions.items():\n",
        "        all_probs = np.array(data['probs']) # Shape: [Num_Patches, Num_Classes]\n",
        "\n",
        "        # Ensure sample_index has .png extension as requested\n",
        "        sample_index_name = f\"{sample_id}.png\"\n",
        "\n",
        "        if len(all_probs) == 0:\n",
        "            final_results.append({'sample_index': sample_index_name, 'label': \"Luminal A\"}) # Default fallback\n",
        "            continue\n",
        "\n",
        "        if method == 'majority_voting':\n",
        "            # Get class prediction for each patch\n",
        "            patch_preds = np.argmax(all_probs, axis=1)\n",
        "            # Find most frequent class\n",
        "            counts = np.bincount(patch_preds)\n",
        "            final_class_idx = np.argmax(counts)\n",
        "\n",
        "        elif method == 'max_confidence':\n",
        "            # Option A: Average probabilities (Soft Voting) - usually best/safest\n",
        "            avg_probs = np.mean(all_probs, axis=0)\n",
        "            final_class_idx = np.argmax(avg_probs)\n",
        "\n",
        "            # Option B: Strict Max Confidence (Uncomment if you prefer this)\n",
        "            # max_probs = np.max(all_probs, axis=0)\n",
        "            # final_class_idx = np.argmax(max_probs)\n",
        "\n",
        "        # Decode Label\n",
        "        pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
        "        final_results.append({'sample_index': sample_index_name, 'label': pred_label})\n",
        "\n",
        "    # 4. Create Pandas DataFrame and Save\n",
        "    df_submission = pd.DataFrame(final_results)\n",
        "\n",
        "    # Ensure correct column order\n",
        "    df_submission = df_submission[['sample_index', 'label']]\n",
        "\n",
        "    # Sort by sample_index for neatness\n",
        "    df_submission.sort_values('sample_index', inplace=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_submission.to_csv(output_csv, index=False)\n",
        "\n",
        "    print(f\"Submission saved to {output_csv}\")\n",
        "    print(df_submission.head())\n",
        "\n",
        "    return df_submission\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7078d160",
      "metadata": {
        "id": "7078d160"
      },
      "source": [
        "### 14.3 Create the Submission CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187e05e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "187e05e0",
        "outputId": "4dca328b-e4a2-4c9f-8b01-bb401ecaa3dd"
      },
      "outputs": [],
      "source": [
        "# Example Usage:\n",
        "# Replace 'path/to/SUBMISSION_PATCHES' with the actual path\n",
        "# If your folder is just named SUBMISSION_PATCHES in current dir:\n",
        "\n",
        "now = datetime.now()\n",
        "date_time_str = now.strftime(\"%d_%b-%H_%M\")\n",
        "\n",
        "sub_dir = os.path.join(os.path.pardir, \"submission_csvs\")\n",
        "OUTPUT_NAME = os.path.join(sub_dir, f\"submission_ft--{date_time_str}.csv\")\n",
        "\n",
        "\n",
        "os.makedirs(sub_dir, exist_ok=True)\n",
        "\n",
        "# Check if folder exists\n",
        "if os.path.exists(SUBMISSION_PATCHES_OUT):\n",
        "    # Method 1: Max Confidence / Average Probability (Recommended)\n",
        "    df_sub_max_conf = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='max_confidence', output_csv=OUTPUT_NAME)\n",
        "    print(f\"Submission CSV saved to: {OUTPUT_NAME}\")\n",
        "\n",
        "    # Method 2: Majority Voting (Optional, uncomment to run)\n",
        "    # df_sub_majority_voting = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='majority_voting', output_csv=\"submission_voting.csv\")\n",
        "else:\n",
        "    print(f\"Directory '{SUBMISSION_PATCHES_OUT}' not found. Please create it or set the correct path.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
