{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020adc77",
   "metadata": {},
   "source": [
    "## **I. Google Colab Initializtion (Only on Colab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# cur_dir = \"/content/drive/Othercomputers/My laptop/POLIMI/AN2DL/AN2DL_CH_2/Notebooks\"\n",
    "# %cd $cur_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada10e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969644a",
   "metadata": {},
   "source": [
    "## **1. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import timm\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "import torchvision.transforms as T\n",
    "import csv\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087eb0c",
   "metadata": {},
   "source": [
    "## **2. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fcb47",
   "metadata": {},
   "source": [
    "- Preprocessing pipeline : \n",
    "    - Get Loaded Images\n",
    "    - Create Goo Masks\n",
    "    - Apply Goo Removal + Resizing\n",
    "    - Discard Shrek Images\n",
    "    - Apply the external Masks (optional)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6208d",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2ca48",
   "metadata": {},
   "source": [
    "#### 2.1.1 _get_smart_goo_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_smart_goo_mask(img_bgr):\n",
    "    \"\"\"\n",
    "    Internal helper to detect goo using Core & Shell logic + 1px Nudge.\n",
    "    Returns a binary mask (White = Goo, Black = Safe).\n",
    "    \"\"\"\n",
    "    # 1. Convert to HSV\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # 2. Define Ranges\n",
    "    # CORE: Solid Green (Strict)\n",
    "    core_lower = np.array([35, 100, 50])\n",
    "    core_upper = np.array([85, 255, 255])\n",
    "    \n",
    "    # SHELL: Faint Halo (Loose/Transparent)\n",
    "    shell_lower = np.array([30, 30, 30])\n",
    "    shell_upper = np.array([95, 255, 255])\n",
    "\n",
    "    # 3. Create initial masks\n",
    "    mask_core = cv2.inRange(hsv, core_lower, core_upper)\n",
    "    mask_shell = cv2.inRange(hsv, shell_lower, shell_upper)\n",
    "\n",
    "    # 4. Smart Combine (Connected Components)\n",
    "    # Keep 'Shell' blobs ONLY if they touch 'Core' blobs\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_shell, connectivity=8)\n",
    "    smart_mask = np.zeros_like(mask_core)\n",
    "\n",
    "    for label_id in range(1, num_labels): # Skip background (0)\n",
    "        blob_mask = (labels == label_id).astype(np.uint8) * 255\n",
    "        \n",
    "        # Check overlap with Core\n",
    "        overlap = cv2.bitwise_and(blob_mask, mask_core)\n",
    "        \n",
    "        # If there is ANY overlap, keep the blob\n",
    "        if cv2.countNonZero(overlap) > 0:\n",
    "            smart_mask = cv2.bitwise_or(smart_mask, blob_mask)\n",
    "\n",
    "    # 5. Fill Holes (in case the goo has shiny reflections)\n",
    "    contours, _ = cv2.findContours(smart_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    final_filled_mask = np.zeros_like(smart_mask)\n",
    "    for contour in contours:\n",
    "        # Minimum area filter (200px) to remove tiny stray noise\n",
    "        if cv2.contourArea(contour) > 200:\n",
    "            cv2.drawContours(final_filled_mask, [contour], -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # 6. The \"1-Pixel Nudge\"\n",
    "    # Safely expand by 1 pixel to cover the final anti-aliased fringe\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    final_expanded_mask = cv2.dilate(final_filled_mask, kernel, iterations=1)\n",
    "\n",
    "    return final_expanded_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4f365",
   "metadata": {},
   "source": [
    "#### 2.1.2 remove_goo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_goo(input_dir, output_dir, target_size=(224, 224), remove_goo=True, save_masks=True, replacement_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Iterates through input_dir, finds 'img_xxxx', resizes them to target_size, \n",
    "    and saves the result to output_dir.\n",
    "    If remove_goo is True, it replaces green pixels (using Smart Core/Shell logic) with replacement_color.\n",
    "    replacement_color: Tuple of (B, G, R) values. Default is black (0, 0, 0).\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extensions to look for\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    \n",
    "    # 1. Gather all valid image files first\n",
    "    print(f\"Scanning for images in: {input_dir}...\")\n",
    "    image_files = [\n",
    "        f for f in input_dir.iterdir() \n",
    "        if f.name.startswith('img_') and f.suffix.lower() in valid_extensions\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found starting with 'img_' in the directory.\")\n",
    "        return\n",
    "\n",
    "    # 2. Iterate with tqdm\n",
    "    count = 0\n",
    "    \n",
    "    for file_path in tqdm(image_files, desc=\"Removing Goo from Images\", unit=\"img\"):\n",
    "        output_path = output_dir / file_path.name\n",
    "        \n",
    "        if output_path.exists():\n",
    "            # Skip silently\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(file_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "            \n",
    "        if target_size is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "            \n",
    "        if remove_goo:\n",
    "            # --- NEW SMART GOO LOGIC ---\n",
    "            # Get the smart mask (White = Goo)\n",
    "            goo_mask = _get_smart_goo_mask(img)\n",
    "            \n",
    "            # Invert Goo Mask (White = Safe)\n",
    "            not_goo_mask = cv2.bitwise_not(goo_mask)\n",
    "            \n",
    "            # Apply Mask to keep safe areas (Goo areas become black/0)\n",
    "            img_safe = cv2.bitwise_and(img, img, mask=not_goo_mask)\n",
    "            \n",
    "            # Create background with replacement color\n",
    "            bg = np.full_like(img, replacement_color)\n",
    "            \n",
    "            # Keep background only where Goo is\n",
    "            bg_goo = cv2.bitwise_and(bg, bg, mask=goo_mask)\n",
    "            \n",
    "            # Combine: Safe Image + Colored Goo Areas\n",
    "            img = cv2.add(img_safe, bg_goo)\n",
    "\n",
    "            if save_masks:\n",
    "                # Save the mask (White = Safe/Tissue, Black = Goo)\n",
    "                mask_name = file_path.name.replace('img_', 'goo_mask_', 1)\n",
    "                mask_output_path = os.path.join(output_dir, \"goo_masks\", mask_name)\n",
    "                Path(os.path.dirname(mask_output_path)).mkdir(parents=True, exist_ok=True)\n",
    "                cv2.imwrite(str(mask_output_path), not_goo_mask)\n",
    "            \n",
    "        cv2.imwrite(str(output_path), img)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Resizing complete. Processed {count} new images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb8937",
   "metadata": {},
   "source": [
    "#### 2.1.3 clean_and_save_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fdde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_masks(goo_masks_dir, external_masks_dir, output_dir, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads goo masks (White=Safe) generated by remove_goo and original external masks.\n",
    "    Removes goo areas from external masks (intersection of Safe and External).\n",
    "    Saves the cleaned masks to output_dir.\n",
    "    TODO: Add check for if the masks already exist to skip processing.\n",
    "    \"\"\"\n",
    "    goo_masks_dir = Path(goo_masks_dir)\n",
    "    external_masks_dir = Path(external_masks_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Find all goo masks\n",
    "    goo_mask_files = list(goo_masks_dir.glob('goo_mask_*.png'))\n",
    "    \n",
    "    if not goo_mask_files:\n",
    "        print(f\"No goo masks found in {goo_masks_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(goo_mask_files)} goo masks. Processing...\")\n",
    "\n",
    "    count = 0\n",
    "    for goo_mask_path in tqdm(goo_mask_files, desc=\"Cleaning External Masks\"):\n",
    "        # Derive external mask filename: goo_mask_xxxx.png -> mask_xxxx.png\n",
    "        mask_name = goo_mask_path.name.replace('goo_mask_', 'mask_', 1)\n",
    "        external_mask_path = external_masks_dir / mask_name\n",
    "        \n",
    "        # Fallback: check for .png if not found (though goo_mask is .png)\n",
    "        if not external_mask_path.exists():\n",
    "             # Try finding with same stem but different extension if needed\n",
    "             pass\n",
    "\n",
    "        if not external_mask_path.exists():\n",
    "            # tqdm.write(f\"External mask not found for {goo_mask_path.name}\")\n",
    "            continue\n",
    "\n",
    "        # Load masks\n",
    "        # Goo mask: White = Safe, Black = Goo\n",
    "        goo_mask = cv2.imread(str(goo_mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        # External mask: White = ROI, Black = Background\n",
    "        external_mask = cv2.imread(str(external_mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if goo_mask is None or external_mask is None:\n",
    "            continue\n",
    "\n",
    "        # Resize external mask if target_size is provided\n",
    "        # Note: goo_mask is already at the size produced by remove_goo\n",
    "        if target_size is not None:\n",
    "             external_mask = cv2.resize(external_mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "             # Ensure goo_mask matches if it wasn't already (e.g. if remove_goo used different settings)\n",
    "             if goo_mask.shape[:2] != (target_size[1], target_size[0]):\n",
    "                 goo_mask = cv2.resize(goo_mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Ensure binary (0 or 255)\n",
    "        _, external_mask = cv2.threshold(external_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        _, goo_mask = cv2.threshold(goo_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Combine: Result is White only if BOTH are White\n",
    "        # i.e. It is in the External Mask AND it is Safe (not goo)\n",
    "        cleaned_mask = cv2.bitwise_and(external_mask, goo_mask)\n",
    "\n",
    "        # Save\n",
    "        output_path = output_dir / mask_name\n",
    "        cv2.imwrite(str(output_path), cleaned_mask)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Cleaned masks saved to {output_dir}. Processed {count} masks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4f03f",
   "metadata": {},
   "source": [
    "#### 2.1.4 apply_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef40d9",
   "metadata": {},
   "source": [
    "#### 2.1.5 filter_bright_green_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e66cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bright_green_areas(image, lg_H=20, lg_S=45, lg_V=0, ug_H=84, ug_S=255, ug_V=255, dilate_iterations=2):\n",
    "    \"\"\"\n",
    "    Filters out bright green areas from the input image with improved residual removal.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image in RGB format (0-1 range)\n",
    "        lg_H, lg_S, lg_V: Lower bounds for HSV green detection\n",
    "        ug_H, ug_S, ug_V: Upper bounds for HSV green detection\n",
    "        dilate_iterations: Number of dilation iterations to expand mask (removes edge artifacts)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert from RGB (0-1) to BGR (0-255) for OpenCV\n",
    "    original_bgr = (image * 255).astype(np.uint8)[..., ::-1]\n",
    "\n",
    "    # 1. Convert to HSV (Hue, Saturation, Value)\n",
    "    hsv = cv2.cvtColor(original_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # 2. Define the \"Bright Green\" Range\n",
    "    lower_green = (lg_H, lg_S, lg_V)\n",
    "    upper_green = (ug_H, ug_S, ug_V)\n",
    "\n",
    "    # Create the initial mask\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "    # 3. Morphological operations to clean up the mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "    # OPEN: Remove small noise\n",
    "    clean_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    # DILATE: Expand the mask to catch edge artifacts and residuals\n",
    "    # This ensures we remove green pixels at the boundaries\n",
    "    if dilate_iterations > 0:\n",
    "        clean_mask = cv2.dilate(clean_mask, kernel, iterations=dilate_iterations)\n",
    "\n",
    "    # 4. Additional step: Detect any remaining green-ish pixels\n",
    "    # Create a more aggressive mask for subtle green tones\n",
    "    lower_green_subtle = (max(0, lg_H - 10), max(0, lg_S - 10), 0)\n",
    "    upper_green_subtle = (min(180, ug_H + 10), 255, 255)\n",
    "    subtle_mask = cv2.inRange(hsv, lower_green_subtle, upper_green_subtle)\n",
    "    \n",
    "    # Only keep subtle green pixels that are near the main green area\n",
    "    subtle_mask = cv2.morphologyEx(subtle_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # Combine masks\n",
    "    combined_mask = cv2.bitwise_or(clean_mask, subtle_mask)\n",
    "\n",
    "    # 5. Invert mask to keep the useful parts\n",
    "    mask_inv = cv2.bitwise_not(combined_mask)\n",
    "\n",
    "    # 6. Apply the mask\n",
    "    result_bgr = cv2.bitwise_and(original_bgr, original_bgr, mask=mask_inv)\n",
    "\n",
    "    return result_bgr, combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a9a86",
   "metadata": {},
   "source": [
    "#### 2.1.6 analyze_dataset_for_shreks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cbc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_for_shreks(directory, ratio_threshold=0.0125):\n",
    "    shrek_images = []\n",
    "    tissue_images = []\n",
    "    \n",
    "    image_files = glob.glob(os.path.join(directory, 'img_*.png'))\n",
    "    print(f\"Found {len(image_files)} images in {directory}\")\n",
    "\n",
    "    for f in tqdm(image_files, desc=\"Analyzing for Shreks\"):\n",
    "        try:\n",
    "            # Load image (BGR)\n",
    "            img = cv2.imread(f)\n",
    "            if img is None: continue\n",
    "            \n",
    "            # Prepare image for the new filter: Convert BGR to RGB (0-1 float)\n",
    "            img_rgb_norm = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "            \n",
    "            # Apply Filter\n",
    "            result_bgr, mask = filter_bright_green_areas(img_rgb_norm)\n",
    "            \n",
    "            # Calculate Ratio of Green Pixels from the combined mask\n",
    "            total_pixels = img.shape[0] * img.shape[1]\n",
    "            green_pixels = np.count_nonzero(mask)\n",
    "            ratio = green_pixels / total_pixels\n",
    "            \n",
    "            # Convert BGR to RGB for plotting\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            entry = {\n",
    "                'name': os.path.basename(f), \n",
    "                'path': f,\n",
    "                'img': img_rgb, \n",
    "                'ratio': ratio,\n",
    "                'mask': mask\n",
    "            }\n",
    "\n",
    "            # === CLASSIFICATION LOGIC ===\n",
    "            if ratio > ratio_threshold:\n",
    "                shrek_images.append(entry)\n",
    "            else:\n",
    "                tissue_images.append(entry)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f}: {e}\")\n",
    "\n",
    "    return shrek_images, tissue_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c27e2e",
   "metadata": {},
   "source": [
    "#### 2.1.8 process_classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_classification_results(shrek_list, tissue_list, shrek_dir, tissue_dir, threshold, visualize = True):\n",
    "    \"\"\"\n",
    "    Saves classified images to respective directories and visualizes the results.\n",
    "    \n",
    "    Args:\n",
    "        shrek_list (list): List of dicts containing Shrek image data.\n",
    "        tissue_list (list): List of dicts containing Tissue image data.\n",
    "        shrek_dir (str): Path to save Shrek images.\n",
    "        tissue_dir (str): Path to save Tissue images.\n",
    "        threshold (float): The green ratio threshold used for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Print Summary\n",
    "    print(f\"Classified {len(shrek_list)} as Shrek\")\n",
    "    print(f\"Classified {len(tissue_list)} as Tissue\")\n",
    "\n",
    "    # Ensure directories exist\n",
    "    os.makedirs(shrek_dir, exist_ok=True)\n",
    "    os.makedirs(tissue_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Save Shrek images\n",
    "    print(f\"Saving {len(shrek_list)} Shrek images to {shrek_dir}...\")\n",
    "    for item in tqdm(shrek_list, desc=\"Saving Shrek Images\"):\n",
    "        dest_path = os.path.join(shrek_dir, item['name'])\n",
    "        \n",
    "        # Check if file exists to prevent overwriting\n",
    "        if os.path.exists(dest_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            shutil.copy2(item['path'], dest_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {item['name']} to shrek folder: {e}\")\n",
    "\n",
    "    # 3. Save Tissue images\n",
    "    print(f\"Saving {len(tissue_list)} Tissue images to {tissue_dir}...\")\n",
    "    for item in tqdm(tissue_list, desc=\"Saving Tissue Images\"):\n",
    "        dest_path = os.path.join(tissue_dir, item['name'])\n",
    "        \n",
    "        # Check if file exists to prevent overwriting\n",
    "        if os.path.exists(dest_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            shutil.copy2(item['path'], dest_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {item['name']} to tissue folder: {e}\")\n",
    "    if visualize == True:\n",
    "        # 4. Visualize Examples (2x2 Grid)\n",
    "        if len(shrek_list) >= 2 and len(tissue_list) >= 2:\n",
    "            fig_ex, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "            fig_ex.suptitle(f\"Classification Results (Threshold: {threshold:.1%})\", fontsize=16)\n",
    "\n",
    "            def show_img(ax, item, label):\n",
    "                ax.imshow(item['img'])\n",
    "                # Show the Green Ratio in the title so you can see WHY it was classified\n",
    "                ax.set_title(f\"{label}\\n{item['name']}\\nGreen Pixels: {item['ratio']:.2%}\")\n",
    "                ax.axis('off')\n",
    "\n",
    "            # Row 1: Detected Shrek\n",
    "            show_img(axes[0, 0], shrek_list[0], \"Detected Shrek\")\n",
    "            show_img(axes[0, 1], shrek_list[1], \"Detected Shrek\")\n",
    "\n",
    "            # Row 2: Detected Tissue\n",
    "            show_img(axes[1, 0], tissue_list[0], \"Detected Tissue\")\n",
    "            show_img(axes[1, 1], tissue_list[1], \"Detected Tissue\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Not enough images in one or both classes to generate 2x2 sample grid.\")\n",
    "\n",
    "        # 5. Plot Scatter Distribution for Tuning\n",
    "        shrek_ratios = [x['ratio'] for x in shrek_list]\n",
    "        tissue_ratios = [x['ratio'] for x in tissue_list]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot Tissue points (Blue)\n",
    "        plt.scatter(range(len(tissue_ratios)), tissue_ratios, color='blue', alpha=0.6, label='Classified as Tissue')\n",
    "\n",
    "        # Plot Shrek points (Green) - Shifted on x-axis to be distinct\n",
    "        # We shift the x-axis index for Shrek so they appear after the tissue points\n",
    "        plt.scatter(range(len(tissue_ratios), len(tissue_ratios) + len(shrek_ratios)), shrek_ratios, color='green', alpha=0.6, label='Classified as Shrek')\n",
    "\n",
    "        # Draw the Threshold Line\n",
    "        plt.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold:.1%})')\n",
    "\n",
    "        plt.title('Green Pixel Ratio per Image', fontsize=14)\n",
    "        plt.ylabel('Ratio of Green Pixels (0.0 - 1.0)')\n",
    "        plt.xlabel('Image Index')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Shrek removal visulization OFF.\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# shrek_list, tissue_list = analyze_dataset(DATASET_PATH) # Assuming this runs before\n",
    "# process_classification_results(shrek_list, tissue_list, SHREK_DIR, TISSUE_DIR, RATIO_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15c4ec",
   "metadata": {},
   "source": [
    "#### 2.1.9 copy_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_masks(image_list, masks_dir, output_dir):\n",
    "    image_names = image_list\n",
    "    mask_names = [name.replace('img_', 'mask_', 1) for name in image_names]\n",
    "\n",
    "    for mask_name in tqdm(mask_names, desc=\"Copying Masks\"):\n",
    "        src_path = os.path.join(masks_dir, mask_name)\n",
    "        dst_path = os.path.join(output_dir, mask_name)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42cc6b5",
   "metadata": {},
   "source": [
    "#### 2.1.10 extract_smart_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_smart_patches(img_path, mask_path, patch_size=224, stride=224, threshold=0.30):\n",
    "    \"\"\"\n",
    "    Intelligently extracts patches. \n",
    "    UPDATED: Groups nearby tumor spots and centers the patch on the region.\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    except FileNotFoundError:\n",
    "        # Fallback for common extension swap if .png not found\n",
    "        if img_path.endswith(\".png\"):\n",
    "            img = Image.open(img_path.replace(\".png\", \".jpg\")).convert(\"RGB\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    img_arr = np.array(img)\n",
    "    mask_arr = np.array(mask)\n",
    "\n",
    "    # Normalize mask\n",
    "    if mask_arr.max() <= 1:\n",
    "        mask_check = mask_arr * 255\n",
    "    else:\n",
    "        mask_check = mask_arr\n",
    "    \n",
    "    h, w, _ = img_arr.shape\n",
    "    \n",
    "    if isinstance(patch_size, int):\n",
    "        ph, pw = patch_size, patch_size\n",
    "    else:\n",
    "        ph, pw = patch_size\n",
    "        \n",
    "    if isinstance(stride, int):\n",
    "        sh, sw = stride, stride\n",
    "    else:\n",
    "        sh, sw = stride\n",
    "\n",
    "    # --- Intelligent Extraction Logic ---\n",
    "    \n",
    "    # 1. GROUPING: Dilate the mask to merge nearby small dots into larger regions.\n",
    "    # This prevents generating 1 patch per pixel-sized dot.\n",
    "    # iterations=15 means dots within ~15 pixels of each other get merged.\n",
    "    dilated_mask = ndimage.binary_dilation(mask_check > 128, iterations=15)\n",
    "    \n",
    "    # 2. Label the merged regions\n",
    "    labeled_mask, num_features = ndimage.label(dilated_mask)\n",
    "    objects = ndimage.find_objects(labeled_mask)\n",
    "    \n",
    "    candidate_coords = set()\n",
    "    \n",
    "    def get_valid_start(val, max_val, p_dim):\n",
    "        return max(0, min(val, max_val - p_dim))\n",
    "\n",
    "    # print(f\"Found {num_features} clustered tumor regions.\") # Commented out for batch processing\n",
    "\n",
    "    for i, slice_obj in enumerate(objects):\n",
    "        y_slice, x_slice = slice_obj\n",
    "        \n",
    "        # Region boundaries\n",
    "        y_min, y_max = y_slice.start, y_slice.stop\n",
    "        x_min, x_max = x_slice.start, x_slice.stop\n",
    "        \n",
    "        # --- Strategy: Center on Blob ---\n",
    "        # We calculate the center of the blob and place the patch there.\n",
    "        \n",
    "        blob_cy = (y_min + y_max) // 2\n",
    "        blob_cx = (x_min + x_max) // 2\n",
    "        \n",
    "        # Top-left corner for the patch to be centered on the blob center\n",
    "        start_y = blob_cy - ph // 2\n",
    "        start_x = blob_cx - pw // 2\n",
    "        \n",
    "        valid_y = get_valid_start(start_y, h, ph)\n",
    "        valid_x = get_valid_start(start_x, w, pw)\n",
    "        \n",
    "        candidate_coords.add((valid_x, valid_y))\n",
    "\n",
    "    # 3. Final Validation\n",
    "    patches = []\n",
    "    coords = []\n",
    "    \n",
    "    for (x, y) in candidate_coords:\n",
    "        mask_patch = mask_check[y:y+ph, x:x+pw]\n",
    "        img_patch = img_arr[y:y+ph, x:x+pw]\n",
    "        \n",
    "        # Use Tissue Threshold (non-white pixels)\n",
    "        img_gray = np.mean(img_patch, axis=2)\n",
    "        tissue_ratio = np.sum(img_gray < 235) / (ph * pw)\n",
    "        \n",
    "        # Use Mask Threshold (tumor pixels)\n",
    "        mask_ratio = np.sum(mask_patch > 128) / (ph * pw)\n",
    "        \n",
    "        # Keep patch only if it has enough tumor AND enough tissue\n",
    "        if mask_ratio >= threshold and tissue_ratio > 0.15:\n",
    "            patches.append(img_patch)\n",
    "            coords.append((x, y))\n",
    "\n",
    "    return patches, coords, img_arr, mask_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261850b3",
   "metadata": {},
   "source": [
    "#### 2.1.11 create_patches_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches_dataset(input_dir, output_dir, mask_dir, patch_size=224, stride=224, threshold=0.30):\n",
    "    \"\"\"\n",
    "    Iterates over images in input_dir, finds corresponding masks in mask_dir, \n",
    "    extracts smart patches, and saves them to output_dir.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    mask_dir = Path(mask_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Filter for image files\n",
    "    image_files = [\n",
    "        f for f in input_dir.iterdir() \n",
    "        if f.name.startswith('img_') and f.suffix.lower() in {'.png', '.jpg', '.jpeg'}\n",
    "    ]\n",
    "    \n",
    "    count = 0\n",
    "    print(f\"Starting patch extraction from {input_dir} to {output_dir} using masks from {mask_dir}...\")\n",
    "    \n",
    "    for file_path in tqdm(image_files, desc=\"Extracting Patches\"):\n",
    "        # Construct mask filename (assuming mask_xxxx.png matches img_xxxx.png)\n",
    "        mask_name = file_path.name.replace('img_', 'mask_', 1)\n",
    "        mask_path = mask_dir / mask_name\n",
    "        \n",
    "        # Fallback if mask has different extension or wasn't found\n",
    "        if not mask_path.exists():\n",
    "             mask_path = mask_dir / (file_path.stem.replace('img_', 'mask_', 1) + \".png\")\n",
    "\n",
    "        if mask_path.exists():\n",
    "            # Extract patches\n",
    "            patches, coords, _, _ = extract_smart_patches(\n",
    "                str(file_path), \n",
    "                str(mask_path), \n",
    "                patch_size=patch_size, \n",
    "                stride=stride, \n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            # Save each patch\n",
    "            base_name = file_path.stem\n",
    "            for i, patch in enumerate(patches):\n",
    "                # patch is a numpy array (H, W, 3), convert to PIL Image to save\n",
    "                patch_img = Image.fromarray(patch)\n",
    "                \n",
    "                # Construct a unique filename for the patch\n",
    "                save_name = f\"{base_name}_p{i}.png\"\n",
    "                patch_img.save(output_dir / save_name)\n",
    "                count += 1\n",
    "        else:\n",
    "            # Optional: Print if mask is missing\n",
    "            # tqdm.write(f\"Mask not found for {file_path.name}\")\n",
    "            pass\n",
    "            \n",
    "    print(f\"Extraction complete. Saved {count} patches to {output_dir}\")\n",
    "\n",
    "# --- Execute Patch Extraction ---\n",
    "# We use TISSUE_OUT because it contains the images we want to process (after goo/shrek removal)\n",
    "# and their corresponding masks (copied in step 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99692554",
   "metadata": {},
   "source": [
    "### 2.2 Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "\n",
    "\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path\n",
    "\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "\n",
    "\n",
    "\n",
    "#preprocessing step 1 output path\n",
    "GOO_REMOVAL_OUT = os.path.join(datasets_path, \"train_nogoo\")\n",
    "\n",
    "#preprocessing step 2 output path\n",
    "SHREK_REMOVAL_OUT = os.path.join(datasets_path, \"train_noshreks\")\n",
    "SHREKS_OUT = os.path.join(SHREK_REMOVAL_OUT, \"train_shreks\")\n",
    "TISSUE_OUT = os.path.join(SHREK_REMOVAL_OUT, \"train_tissue\")\n",
    "\n",
    "#preprocessing step 3 output path\n",
    "FINAL_TRAIN_OUT = os.path.join(datasets_path, \"train_masked_noshreks\")\n",
    "  # Where the resized unmasked images will be saved\n",
    "PATCHES_OUT = os.path.join(datasets_path, \"train_patches\")\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54719952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove goo and do not resize images\n",
    "remove_goo(SOURCE_FOLDER,GOO_REMOVAL_OUT, target_size=None, remove_goo=True, save_masks=True, replacement_color=(195, 195, 195))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_save_masks(\n",
    "    goo_masks_dir=os.path.join(GOO_REMOVAL_OUT, \"goo_masks\"), \n",
    "    external_masks_dir=SOURCE_FOLDER, \n",
    "    output_dir=os.path.join(GOO_REMOVAL_OUT, \"cleaned_masks\"), \n",
    "    target_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6008bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Discard Shrek Images\n",
    "shreks_list, tissue_list = analyze_dataset_for_shreks(GOO_REMOVAL_OUT, ratio_threshold=0.0125)\n",
    "\n",
    "process_classification_results(shreks_list, tissue_list, SHREKS_OUT, TISSUE_OUT, 0.0125, visualize=False)\n",
    "\n",
    "tissue_image_names = [item['name'] for item in tissue_list]\n",
    "copy_masks(tissue_image_names, SOURCE_FOLDER, TISSUE_OUT)\n",
    "\n",
    "# Clean up memory\n",
    "del shreks_list\n",
    "del tissue_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_MASKS_DIR = os.path.join(GOO_REMOVAL_OUT, \"cleaned_masks\")\n",
    "create_patches_dataset(\n",
    "    TISSUE_OUT, \n",
    "    PATCHES_OUT, \n",
    "    mask_dir=CLEANED_MASKS_DIR,\n",
    "    patch_size=224, \n",
    "    stride=224, \n",
    "    threshold=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b5f0c",
   "metadata": {},
   "source": [
    "## **5. Train/Val Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac44162",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
    "    \"\"\"\n",
    "    # 1. Load the labels CSV\n",
    "    # Assuming CSV structure: [image_id, label] or similar\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "    \n",
    "    # Standardize column names for easier merging\n",
    "    # We assume the first column is the ID and the second is the Label\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "    \n",
    "    # Ensure IDs in CSV are strings (to match filenames)\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "    \n",
    "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
    "    # because our parsed Bag IDs won't have them.\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # 2. List all patch files\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
    "    \n",
    "    # 3. Parse filenames to get Bag IDs\n",
    "    data = []\n",
    "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
    "    \n",
    "    for filename in patch_files:\n",
    "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
    "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
    "        \n",
    "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
    "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
    "        try:\n",
    "            bag_id = filename.rsplit('_p', 1)[0]\n",
    "            \n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(f\"Skipping malformed filename: {filename}\")\n",
    "\n",
    "    # Create temporary patches DataFrame\n",
    "    df_patches = pd.DataFrame(data)\n",
    "    \n",
    "    # 4. Merge patches with labels\n",
    "    # This assigns the correct Bag Label to every Patch in that Bag\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
    "    \n",
    "    # 5. Clean up and Rename\n",
    "    # Keep only required columns\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "    \n",
    "    # Rename label column to standard 'label' if it isn't already\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "    \n",
    "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a11ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(patches_metadata_df.head().drop(columns=['path']))\n",
    "print(\"\\nPatches per Bag (Distribution):\")\n",
    "print(patches_metadata_df['sample_id'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "\n",
    "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"  {orig} -> {enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bag_lists(df):\n",
    "    \"\"\"\n",
    "    Groups the dataframe by 'sample_id' and returns lists of paths and labels.\n",
    "    Uses encoded labels for training.\n",
    "    \"\"\"\n",
    "    # Get unique bag IDs in this dataframe slice\n",
    "    unique_bags = df['sample_id'].unique()\n",
    "    \n",
    "    bag_paths_list = []\n",
    "    bag_labels_list = []\n",
    "    \n",
    "    print(f\"Processing {len(unique_bags)} bags...\")\n",
    "    \n",
    "    for bag_id in unique_bags:\n",
    "        # Get all rows for this specific bag\n",
    "        group = df[df['sample_id'] == bag_id]\n",
    "        \n",
    "        # Get all file paths for this bag\n",
    "        paths = group['path'].tolist()\n",
    "        \n",
    "        # Get the ENCODED label (they are all the same for one bag)\n",
    "        label = group['label_encoded'].iloc[0]\n",
    "        \n",
    "        bag_paths_list.append(paths)\n",
    "        bag_labels_list.append(label)\n",
    "        \n",
    "    return bag_paths_list, bag_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split unique Bag IDs (NOT images)\n",
    "unique_bag_ids = patches_metadata_df['sample_id'].unique()\n",
    "# Create a mapping: Bag ID -> Label (take first occurrence since all patches in a bag have the same label)\n",
    "bag_id_to_label = patches_metadata_df.groupby('sample_id')['label'].first()\n",
    "# Get labels for stratification (in the same order as unique_bag_ids)\n",
    "stratify_labels = [bag_id_to_label[bag_id] for bag_id in unique_bag_ids]\n",
    "\n",
    "train_ids, val_ids = train_test_split(unique_bag_ids, test_size=0.2, random_state=42, stratify=stratify_labels)\n",
    "# Note: If you have class imbalance, pass the bag labels to 'stratify' above\n",
    "\n",
    "# 2. Slice the main DataFrame based on these IDs\n",
    "train_df = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_ids)]\n",
    "val_df = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_ids)]\n",
    "\n",
    "print(f\"Total Bags: {len(unique_bag_ids)}\")\n",
    "print(f\"Train Bags: {len(train_ids)} | Val Bags: {len(val_ids)}\")\n",
    "\n",
    "# 3. Convert DataFrames to Lists for the Dataset class\n",
    "train_paths, train_labels = prepare_bag_lists(train_df)\n",
    "val_paths, val_labels = prepare_bag_lists(val_df)\n",
    "\n",
    "# 4. Define Transforms\n",
    "# Train: heavy augmentation to prevent overfitting\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224)), # Ensure size matches ResNet input\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Val: No augmentation, just resize and normalize\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b3ac2",
   "metadata": {},
   "source": [
    "### 4.1 Set Input Size and Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape based on the training data\n",
    "input_shape = (3, 224, 224)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Number of Classes:\", num_classes)\n",
    "print(\"Class Names:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e7e74",
   "metadata": {},
   "source": [
    "### 4.2 Create MIL Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMILDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, bag_paths, bag_labels, transform=None):\n",
    "        self.bag_paths = bag_paths\n",
    "        self.bag_labels = bag_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bag_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get paths and label for this bag\n",
    "        paths = self.bag_paths[idx]\n",
    "        label = self.bag_labels[idx]\n",
    "        \n",
    "        # 2. Load images on the fly\n",
    "        images = []\n",
    "        for p in paths:\n",
    "            img = Image.open(p).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "            \n",
    "        # 3. Stack into (N, C, H, W)\n",
    "        bag_tensor = torch.stack(images)\n",
    "        \n",
    "        # FIX: Return tensor label directly (no need for .long() call)\n",
    "        return bag_tensor, label  # label is already int from list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mil_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for variable-length bags.\n",
    "    Since batch_size=1, this just unpacks the single item.\n",
    "    \"\"\"\n",
    "    # batch is a list with 1 element: [(bag_tensor, label)]\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DynamicMILDataset(train_paths, train_labels, transform=train_transform)\n",
    "val_dataset = DynamicMILDataset(val_paths, val_labels, transform=val_transform)\n",
    "\n",
    "# FIX: Use collate_fn to properly handle the batch\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=mil_collate_fn  # <- ADDED\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=mil_collate_fn  # <- ADDED\n",
    ")\n",
    "# Test one batch to verify\n",
    "data, label = next(iter(train_loader))\n",
    "# Shape should be: [1, N_patches, 3, 224, 224]\n",
    "print(f\"Bag Shape: {data.shape}\") \n",
    "# We usually need to squeeze the batch dimension for the model: [N_patches, 3, 224, 224]\n",
    "print(f\"Model Input Shape: {data.squeeze(0).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhikonFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed version that handles all ViT output types correctly.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super(PhikonFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.backbone_type = None\n",
    "        \n",
    "        # Try loading Phikon from Hugging Face\n",
    "        try:\n",
    "            from transformers import ViTModel\n",
    "            print(\"Attempting to load Phikon from Hugging Face...\")\n",
    "            \n",
    "            self.backbone = ViTModel.from_pretrained(\n",
    "                \"owkin/phikon\",\n",
    "                add_pooling_layer=False\n",
    "            )\n",
    "            self.backbone_type = \"hf_vit\"\n",
    "            self.feature_dim = 768\n",
    "            print(\" Phikon loaded successfully from Hugging Face!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not load Phikon: {e}\")\n",
    "            print(\"Loading ImageNet ViT-B/16 as fallback...\")\n",
    "            \n",
    "            # Fallback to timm\n",
    "            try:\n",
    "                import timm\n",
    "                self.backbone = timm.create_model(\n",
    "                    'vit_base_patch16_224',\n",
    "                    pretrained=True,\n",
    "                    num_classes=0  # Remove classification head\n",
    "                )\n",
    "                self.backbone_type = \"timm_vit\"\n",
    "                self.feature_dim = 768\n",
    "                print(\" ImageNet ViT-B/16 loaded as fallback\")\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError(f\"Could not load any ViT model: {e2}\")\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            self._freeze_backbone()\n",
    "    \n",
    "    def _freeze_backbone(self):\n",
    "        \"\"\"Freeze all backbone parameters.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        frozen = sum(p.numel() for p in self.backbone.parameters() if not p.requires_grad)\n",
    "        print(f\" Backbone frozen ({frozen:,} parameters)\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Extract features from patches.\n",
    "        \n",
    "        Args:\n",
    "            x: (N_patches, 3, 224, 224) - batch of image patches\n",
    "            \n",
    "        Returns:\n",
    "            features: (N_patches, 768) - extracted features\n",
    "        \"\"\"\n",
    "        # Get raw outputs from backbone\n",
    "        outputs = self.backbone(x)\n",
    "        \n",
    "        # Handle different output types\n",
    "        if self.backbone_type == \"hf_vit\":\n",
    "            # Hugging Face ViT returns BaseModelOutputWithPooling\n",
    "            # Access last_hidden_state and take CLS token (index 0)\n",
    "            features = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "        elif self.backbone_type == \"timm_vit\":\n",
    "            # timm with num_classes=0 returns tensor directly\n",
    "            features = outputs\n",
    "            \n",
    "        else:\n",
    "            # Generic fallback\n",
    "            if isinstance(outputs, torch.Tensor):\n",
    "                features = outputs\n",
    "            elif hasattr(outputs, 'last_hidden_state'):\n",
    "                features = outputs.last_hidden_state[:, 0, :]\n",
    "            elif hasattr(outputs, 'pooler_output'):\n",
    "                features = outputs.pooler_output\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected output type: {type(outputs)}\")\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Test the Feature Extractor\n",
    "# ============================================================\n",
    "\n",
    "def test_feature_extractor():\n",
    "    \"\"\"Test that feature extraction works correctly.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing Feature Extractor\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create model\n",
    "    extractor = PhikonFeatureExtractor(freeze_backbone=True).to(device)\n",
    "    extractor.eval()\n",
    "    \n",
    "    # Create dummy input (10 patches)\n",
    "    dummy_bag = torch.randn(10, 3, 224, 224).to(device)\n",
    "    \n",
    "    print(f\"\\nInput shape: {dummy_bag.shape}\")\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = extractor(dummy_bag)\n",
    "    \n",
    "    print(f\"Output shape: {features.shape}\")\n",
    "    print(f\"Expected shape: (10, 768)\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert features.shape == (10, 768), f\"Wrong shape! Got {features.shape}\"\n",
    "    print(\"\\n Feature extraction works correctly!\")\n",
    "    \n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6059ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMILAggregator(nn.Module):\n",
    "    \"\"\"Gated attention mechanism for MIL.\"\"\"\n",
    "    def __init__(self, feature_dim=768, hidden_dim=256, num_classes=4):\n",
    "        super(AttentionMILAggregator, self).__init__()\n",
    "        \n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (N_patches, 768)\n",
    "        Returns:\n",
    "            logits: (1, num_classes)\n",
    "            attention: (1, N_patches)\n",
    "        \"\"\"\n",
    "        A_V = self.attention_V(features)\n",
    "        A_U = self.attention_U(features)\n",
    "        A = self.attention_weights(A_V * A_U)\n",
    "        A = torch.transpose(A, 1, 0)\n",
    "        A = torch.softmax(A, dim=1)\n",
    "        \n",
    "        M = torch.mm(A, features)\n",
    "        logits = self.classifier(M)\n",
    "        \n",
    "        return logits, A\n",
    "\n",
    "\n",
    "class PhikonMIL(nn.Module):\n",
    "    \"\"\"Complete Phikon-based MIL model.\"\"\"\n",
    "    def __init__(self, num_classes=4, freeze_backbone=True):\n",
    "        super(PhikonMIL, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = PhikonFeatureExtractor(freeze_backbone=freeze_backbone)\n",
    "        self.aggregator = AttentionMILAggregator(\n",
    "            feature_dim=768,\n",
    "            hidden_dim=256,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, bag):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bag: (N_patches, 3, 224, 224)\n",
    "        Returns:\n",
    "            logits: (1, num_classes)\n",
    "            attention: (1, N_patches)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(bag)\n",
    "        logits, attention = self.aggregator(features)\n",
    "        return logits, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW CODE (Phikon):\n",
    "mil_model = PhikonMIL(num_classes=num_classes, freeze_backbone=True).to(device)\n",
    "# Test it first!\n",
    "test_bag = torch.randn(5, 3, 224, 224).to(device)\n",
    "test_logits, test_attn = mil_model(test_bag)\n",
    "print(f\"Test output shape: {test_logits.shape}\")  # Should be (1, 4)\n",
    "\n",
    "# Setup optimizer (only trainable parameters)\n",
    "trainable_params = [p for p in mil_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Print parameter counts\n",
    "total = sum(p.numel() for p in mil_model.parameters())\n",
    "trainable = sum(p.numel() for p in trainable_params)\n",
    "print(f\"\\nTotal parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 5\n",
    "# Then proceed with training as before!\n",
    "\n",
    "# Setup optimizer - ONLY train the aggregator\n",
    "trainable_params = [p for p in mil_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Add learning rate scheduler (IMPORTANT for transfer learning)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=epochs, \n",
    "    eta_min=1e-4\n",
    ")\n",
    "\n",
    "# Print parameter counts\n",
    "total = sum(p.numel() for p in mil_model.parameters())\n",
    "trainable = sum(p.numel() for p in trainable_params)\n",
    "print(f\"Total: {total:,} | Trainable: {trainable:,} ({100*trainable/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af88e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, mil_model.parameters()),\n",
    "    lr=1e-3, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "# Early stopping variables\n",
    "patience = 10\n",
    "best_val_f1 = -np.inf\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Track metrics for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    mil_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels_list = []\n",
    "    \n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "    \n",
    "    for bag, label in train_loop:\n",
    "        bag = bag.to(device)\n",
    "        if not isinstance(label, torch.Tensor):\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = mil_model(bag)\n",
    "        \n",
    "        loss = criterion(logits.squeeze(0), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        train_preds.append(predicted.cpu().item())\n",
    "        train_labels_list.append(label.cpu().item() if isinstance(label, torch.Tensor) else label)\n",
    "        \n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_f1 = f1_score(train_labels_list, train_preds, average='weighted', zero_division=0)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_f1_scores.append(train_f1)\n",
    "    \n",
    "    # Validation (same as before)\n",
    "    mil_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for bag, label in val_loader:\n",
    "            bag = bag.to(device)\n",
    "            if not isinstance(label, torch.Tensor):\n",
    "                label = torch.tensor(label, dtype=torch.long)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            logits, _ = mil_model(bag)\n",
    "            loss = criterion(logits.squeeze(0), label)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_preds.append(predicted.cpu().item())\n",
    "            val_labels_list.append(label.cpu().item() if isinstance(label, torch.Tensor) else label)\n",
    "    \n",
    "    val_f1 = f1_score(val_labels_list, val_preds, average='weighted', zero_division=0)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_f1_scores.append(val_f1)\n",
    "    \n",
    "    # NEW: Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Train F1: {train_f1:.4f} | \"\n",
    "          f\"Val F1: {val_f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\", end=\"\")\n",
    "    \n",
    "    # Early stopping (same as before)\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        best_model_state = copy.deepcopy(mil_model.state_dict())\n",
    "        print(\" (Best F1!)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\" (Patience: {patience_counter}/{patience})\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered! Best Val F1: {best_val_f1:.4f}\")\n",
    "            mil_model.load_state_dict(best_model_state)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "\n",
    "MODEL_NAME = \"attention_mil_best_model.pt\"\n",
    "model_save_path = os.path.join(\"models\", MODEL_NAME)\n",
    "torch.save(best_model_state, model_save_path)\n",
    "print(f\"Best model saved to {model_save_path}\")\n",
    "print(f\"Best Validation F1 Score: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# Generate confusion matrix on validation set\n",
    "mil_model.eval()\n",
    "final_val_preds = []\n",
    "final_val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bag, label in val_loader:\n",
    "        bag = bag.to(device)\n",
    "        if not isinstance(label, torch.Tensor):\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "        label = label.to(device)\n",
    "        logits, _ = mil_model(bag)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        final_val_preds.append(predicted.cpu().item())\n",
    "        final_val_labels.append(label.cpu().item())\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(final_val_labels, final_val_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=True)\n",
    "axes[0].set_title('Validation Confusion Matrix (Bag Level)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# 2. Loss Plot\n",
    "axes[1].plot(range(1, epoch+2), train_losses, 'o-', label='Train Loss', linewidth=2, markersize=6)\n",
    "axes[1].plot(range(1, epoch+2), val_losses, 's-', label='Val Loss', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. F1 Score Plot\n",
    "axes[2].plot(range(1, epoch+2), train_f1_scores, 'o-', label='Train F1', linewidth=2, markersize=6)\n",
    "axes[2].plot(range(1, epoch+2), val_f1_scores, 's-', label='Val F1', linewidth=2, markersize=6)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[2].set_title('Training vs Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd07b1f",
   "metadata": {},
   "source": [
    "## x. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_PATH = os.path.join(datasets_path, \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50736e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_csv_phikon(model, submission_path, label_encoder, output_csv=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Updated submission function for Phikon-MIL model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    image_files = sorted([\n",
    "        f for f in os.listdir(submission_path) \n",
    "        if f.startswith('img_') and f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "    \n",
    "    # Define transform (MUST match training transform!)\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images in {submission_path}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(image_files, desc=\"Processing Submission Images\"):\n",
    "            img_path = os.path.join(submission_path, filename)\n",
    "            mask_filename = filename.replace('img_', 'mask_')\n",
    "            mask_path = os.path.join(submission_path, mask_filename)\n",
    "            \n",
    "            try:\n",
    "                patches, _, _, _ = extract_smart_patches(\n",
    "                    img_path, \n",
    "                    mask_path, \n",
    "                    patch_size=224, \n",
    "                    stride=224, \n",
    "                    threshold=0.01 \n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting patches for {filename}: {e}\")\n",
    "                patches = []\n",
    "\n",
    "            if len(patches) == 0:\n",
    "                pred_idx = 0\n",
    "            else:\n",
    "                # Convert patches to tensor\n",
    "                patches_list = []\n",
    "                for patch in patches:\n",
    "                    patch_pil = Image.fromarray(patch)\n",
    "                    patch_tensor = transform(patch_pil)\n",
    "                    patches_list.append(patch_tensor)\n",
    "                \n",
    "                bag_tensor = torch.stack(patches_list).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                logits, _ = model(bag_tensor)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                pred_idx = predicted.item()\n",
    "            \n",
    "            pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "            results.append([filename, pred_label])\n",
    "            \n",
    "    # Write CSV\n",
    "    import csv\n",
    "    with open(output_csv, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"sample_index\", \"label\"])\n",
    "        writer.writerows(results)\n",
    "        \n",
    "    print(f\"Submission file saved to {output_csv}\")\n",
    "\n",
    "# Run submission\n",
    "create_submission_csv_phikon(mil_model, SUBMISSION_PATH, label_encoder, \"phikon_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
