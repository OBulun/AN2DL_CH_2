{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3293c812",
   "metadata": {
    "id": "3293c812"
   },
   "source": [
    "## **1. Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631b342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4631b342",
    "outputId": "58fbc501-7799-471f-9a5a-e64689609281"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#cur_dir = \"/content/drive/MyDrive/CH2/Notebooks\"\n",
    "#%cd $cur_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfd385",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8bfd385",
    "outputId": "b4006194-5220-4fbf-9b47-0be0b5673790"
   },
   "outputs": [],
   "source": [
    "#%pip install torchview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee15c7",
   "metadata": {
    "id": "edee15c7"
   },
   "source": [
    "## **2. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c7403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "963c7403",
    "outputId": "b6727f0e-41f8-4a6b-dea0-f74d54e6a861"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tfs\n",
    "import math\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ee8",
   "metadata": {
    "id": "3b5b1ee8"
   },
   "source": [
    "## **3. Config and  Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6b4fa",
   "metadata": {},
   "source": [
    "### 3.1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d674a2d",
   "metadata": {
    "id": "7d674a2d"
   },
   "outputs": [],
   "source": [
    "USE_MASKED_PATCHES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3da911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df3da911",
    "outputId": "8f0365f9-913a-4b45-9039-d4b26ac59c53"
   },
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path\n",
    "\n",
    "if USE_MASKED_PATCHES:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"train_patches_masked\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"submission_patches_masked\")\n",
    "else:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\")\n",
    "SUBMISSION_PATCH_MASKS = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\",\"masks\")\n",
    "\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "print(f\"Test data path: {test_data_path}\")\n",
    "print(f\"Patches output path: {PATCHES_OUT}\")\n",
    "print(f\"Submission patches output path: {SUBMISSION_PATCHES_OUT}\")\n",
    "print(f\"Submission patch masks path: {SUBMISSION_PATCH_MASKS}\")\n",
    "\n",
    "\n",
    "# Define where your Ground Truth binary masks are stored\n",
    "# Assuming filenames match: img_001.png -> mask_001.png (or similar)\n",
    "MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\", \"train_patches\", \"masks\")\n",
    "\n",
    "# ImageNet normalization statistics\n",
    "IMAGENET_MEAN = [float(x) for x in [0.485, 0.456, 0.406]]  # or convert your current values to float\n",
    "IMAGENET_STD  = [float(x) for x in [0.229, 0.224, 0.225]]\n",
    "\n",
    "\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfa94d",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_with_predictions(model, loader, device, label_encoder, sample_id=None, aggregation_method='max_confidence'):\n",
    "    \"\"\"Plot all patches of a single sample and the aggregated image prediction (Two-Stream Model).\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Robust Dataset Access\n",
    "    dataset = loader.dataset\n",
    "    while hasattr(dataset, 'dataset'): # Unwrap Subsets\n",
    "        dataset = dataset.dataset\n",
    "    df = dataset.df\n",
    "    masks_dir = getattr(dataset, 'masks_location', None)\n",
    "\n",
    "    # 2. Pick a sample_id\n",
    "    if sample_id is None:\n",
    "        sample_id = np.random.choice(df['sample_id'].unique())\n",
    "    sample_patches = df[df['sample_id'] == sample_id].reset_index(drop=True)\n",
    "\n",
    "    # 3. Define Transforms\n",
    "    # Image: Resize -> ToTensor -> Normalize\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Mask: Resize -> ToTensor (No Normalize)\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 4. Load Data\n",
    "    images_tensors = []\n",
    "    masks_tensors = []\n",
    "    display_imgs = []\n",
    "    \n",
    "    # Stats for denormalization (Display only)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "    for _, row in sample_patches.iterrows():\n",
    "        try:\n",
    "            # --- A. Image ---\n",
    "            img_path = row['path']\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            tensor_img = inference_transform(img)\n",
    "            images_tensors.append(tensor_img)\n",
    "            \n",
    "            # Display image (Denormalize)\n",
    "            disp_img = torch.clamp(tensor_img * std + mean, 0, 1)\n",
    "            display_imgs.append(disp_img)\n",
    "            \n",
    "            # --- B. Mask ---\n",
    "            if masks_dir:\n",
    "                img_filename = os.path.basename(img_path)\n",
    "                mask_filename = img_filename.replace('img_', 'mask_')\n",
    "                mask_path = os.path.join(masks_dir, mask_filename)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    mask = Image.open(mask_path).convert('L')\n",
    "                else:\n",
    "                    mask = Image.new('L', (224, 224), 0)\n",
    "            else:\n",
    "                 mask = Image.new('L', (224, 224), 0)\n",
    "            \n",
    "            tensor_mask = mask_transform(mask)\n",
    "            masks_tensors.append(tensor_mask)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {row['path']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not images_tensors:\n",
    "        print(\"No valid images found for this sample.\")\n",
    "        return\n",
    "\n",
    "    # 5. Batch Inference\n",
    "    batch_imgs = torch.stack(images_tensors).to(device)\n",
    "    batch_masks = torch.stack(masks_tensors).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- FIX: Pass BOTH pillars ---\n",
    "        # Model returns ONLY logits for Two-Stream\n",
    "        logits = model(batch_imgs, batch_masks) \n",
    "        probs = torch.softmax(logits, dim=1).cpu()\n",
    "\n",
    "    # 6. Process Predictions\n",
    "    patch_preds = probs.argmax(dim=1).numpy()\n",
    "    patch_confs = probs.max(dim=1).values.numpy()\n",
    "\n",
    "    # --- Aggregation Logic ---\n",
    "    if aggregation_method == 'max_confidence':\n",
    "        image_probs = probs.mean(dim=0).numpy()\n",
    "        image_pred = image_probs.argmax()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    elif aggregation_method == 'majority_voting':\n",
    "        counts = np.bincount(patch_preds, minlength=len(label_encoder.classes_))\n",
    "        image_pred = counts.argmax()\n",
    "        image_probs = counts / counts.sum()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n",
    "\n",
    "    # Labels\n",
    "    image_pred_label = label_encoder.inverse_transform([image_pred])[0]\n",
    "    true_label_idx = sample_patches.iloc[0]['label_encoded']\n",
    "    true_label = label_encoder.inverse_transform([true_label_idx])[0]\n",
    "\n",
    "    # 7. Plotting\n",
    "    cols = min(6, len(sample_patches))\n",
    "    rows = math.ceil(len(sample_patches) / cols)\n",
    "    \n",
    "    fig = plt.figure(figsize=(3*cols + 4, 3*rows))\n",
    "    gs = fig.add_gridspec(rows, cols + 1, width_ratios=[1]*cols + [1.3])\n",
    "\n",
    "    # Patch grid\n",
    "    for idx, (img_disp, pred, conf) in enumerate(zip(display_imgs, patch_preds, patch_confs)):\n",
    "        ax = fig.add_subplot(gs[idx // cols, idx % cols])\n",
    "        ax.imshow(img_disp.permute(1,2,0))\n",
    "        \n",
    "        lbl = label_encoder.inverse_transform([pred])[0]\n",
    "        color = 'green' if pred == image_pred else 'red'\n",
    "        \n",
    "        ax.set_title(f\"{lbl}\\n{conf:.2%}\", fontsize=9, color='black', backgroundcolor='white')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Aggregated Bar Chart\n",
    "    ax_bar = fig.add_subplot(gs[:, -1])\n",
    "    class_names = label_encoder.classes_\n",
    "    colors = ['green' if i == image_pred else 'lightgray' for i in range(len(class_names))]\n",
    "    \n",
    "    y_pos = np.arange(len(class_names))\n",
    "    ax_bar.barh(y_pos, image_probs, color=colors)\n",
    "    ax_bar.set_yticks(y_pos)\n",
    "    ax_bar.set_yticklabels(class_names)\n",
    "    ax_bar.invert_yaxis()\n",
    "    \n",
    "    ax_bar.set_xlabel('Probability' if aggregation_method == 'max_confidence' else 'Vote Share')\n",
    "    ax_bar.set_xlim([0,1])\n",
    "    \n",
    "    ax_bar.set_title(f\"Sample: {sample_id}\\nTrue: {true_label}\\nPred: {image_pred_label} ({image_conf:.2%})\\n{aggregation_method}\")\n",
    "    \n",
    "    for i, prob in enumerate(image_probs):\n",
    "        ax_bar.text(prob + 0.02, i, f\"{prob:.3f}\", va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78eafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions(model, loader, device):\n",
    "    \"\"\"\n",
    "    Aggregates patch-level predictions to image-level for Two-Stream (Pillar) models.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1. robustly retrieve the underlying dataset (handles torch.utils.data.Subset)\n",
    "    dataset = loader.dataset\n",
    "    while hasattr(dataset, 'dataset'):\n",
    "        dataset = dataset.dataset\n",
    "        \n",
    "    df = dataset.df\n",
    "    \n",
    "    # Retrieve the specific mask folder from the dataset logic\n",
    "    # This ensures we look in '.../masks/' regardless of where images are\n",
    "    masks_dir = getattr(dataset, 'masks_location', None)\n",
    "    print(f\"\\nUsing masks from: {masks_dir if masks_dir else 'No masks directory specified, using black masks.'}\")\n",
    "    # 2. Define Transforms\n",
    "    # Image: Standard normalization\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Mask: Resize + ToTensor only\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    sample_ids = df['sample_id'].unique()\n",
    "    print(f\"\\nAggregating predictions for {len(sample_ids)} unique images...\")\n",
    "\n",
    "    for sample_id in tqdm(sample_ids, leave=False):\n",
    "        # Get all patches for this image\n",
    "        sample_patches = df[df['sample_id'] == sample_id]\n",
    "\n",
    "        # Ground Truth\n",
    "        true_label = sample_patches.iloc[0]['label_encoded']\n",
    "        y_true.append(true_label)\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "\n",
    "        for img_path in sample_patches['path']:\n",
    "            try:\n",
    "                # --- A. Load Image ---\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = inference_transform(img)\n",
    "                \n",
    "                # --- B. Load Mask (Robust Path Construction) ---\n",
    "                if masks_dir:\n",
    "                    # Logic: /path/to/masks/ + mask_filename.png\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    mask_filename = img_filename.replace('img_', 'mask_')\n",
    "                    mask_path = os.path.join(masks_dir, mask_filename)\n",
    "                    \n",
    "                    if os.path.exists(mask_path):\n",
    "                        mask = Image.open(mask_path).convert('L')\n",
    "                    else:\n",
    "                        # Fallback: Create black mask if file missing\n",
    "                        mask = Image.new('L', (224, 224), 0)\n",
    "                else:\n",
    "                    # Fallback: No mask directory known -> Black mask\n",
    "                    mask = Image.new('L', (224, 224), 0)\n",
    "                \n",
    "                mask = mask_transform(mask)\n",
    "                \n",
    "                patches_img.append(img)\n",
    "                patches_mask.append(mask)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if not patches_img:\n",
    "            y_pred.append(true_label) # Fallback\n",
    "            continue\n",
    "\n",
    "        # Stack batch\n",
    "        batch_imgs = torch.stack(patches_img).to(device)\n",
    "        batch_masks = torch.stack(patches_mask).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # --- Pass BOTH Image and Mask Pillars ---\n",
    "            logits = model(batch_imgs, batch_masks)\n",
    "            \n",
    "            # Softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # Average pooling of probabilities\n",
    "            avg_probs = torch.mean(probs, dim=0)\n",
    "            pred_label = torch.argmax(avg_probs).item()\n",
    "\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple function to handle this\n",
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427b09e",
   "metadata": {
    "id": "e427b09e"
   },
   "source": [
    "## **4. Train/Val Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7455a",
   "metadata": {
    "id": "7cc7455a"
   },
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
    "    \"\"\"\n",
    "    # 1. Load the labels CSV\n",
    "    # Assuming CSV structure: [image_id, label] or similar\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "\n",
    "    # Standardize column names for easier merging\n",
    "    # We assume the first column is the ID and the second is the Label\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "\n",
    "    # Ensure IDs in CSV are strings (to match filenames)\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "\n",
    "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
    "    # because our parsed Bag IDs won't have them.\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # 2. List all patch files\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
    "\n",
    "    # 3. Parse filenames to get Bag IDs\n",
    "    data = []\n",
    "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
    "\n",
    "    for filename in patch_files:\n",
    "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
    "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
    "\n",
    "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
    "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
    "        try:\n",
    "            bag_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(f\"Skipping malformed filename: {filename}\")\n",
    "\n",
    "    # Create temporary patches DataFrame\n",
    "    df_patches = pd.DataFrame(data)\n",
    "\n",
    "    # 4. Merge patches with labels\n",
    "    # This assigns the correct Bag Label to every Patch in that Bag\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
    "\n",
    "    # 5. Clean up and Rename\n",
    "    # Keep only required columns\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "\n",
    "    # Rename label column to standard 'label' if it isn't already\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "\n",
    "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b8e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633b8e05",
    "outputId": "7d671b8b-3e81-446b-d478-67847824bf41"
   },
   "outputs": [],
   "source": [
    "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(patches_metadata_df.head().drop(columns=['path']))\n",
    "print(\"\\nPatches per Bag (Distribution):\")\n",
    "print(patches_metadata_df['sample_id'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6c6cd3",
    "outputId": "be243fc3-59b3-4318-ed4b-09eb2ce21d3d"
   },
   "outputs": [],
   "source": [
    "# Add Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "\n",
    "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"  {orig} -> {enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52314ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52314ae9",
    "outputId": "ce81e6f4-f3fe-4311-ebd2-11946f50a08e"
   },
   "outputs": [],
   "source": [
    "# Train/Val Split on Original Images (not patches)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train/Val Split on Original Images\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get unique sample IDs\n",
    "unique_samples = patches_metadata_df['sample_id'].unique()\n",
    "print(f\"\\nTotal unique samples (original images): {len(unique_samples)}\")\n",
    "\n",
    "# Split samples into train (80%) and val (20%)\n",
    "train_samples, val_samples = train_test_split(\n",
    "    unique_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples, 'label_encoded'].values\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples: {len(val_samples)}\")\n",
    "\n",
    "# Create train and val DataFrames by filtering patches\n",
    "df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_samples)].reset_index(drop=True)\n",
    "df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_samples)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain patches: {len(df_train)}\")\n",
    "print(f\"Val patches: {len(df_val)}\")\n",
    "print(f\"\\nTrain label distribution:\\n{df_train['label'].value_counts()}\")\n",
    "print(f\"\\nVal label distribution:\\n{df_val['label'].value_counts()}\")\n",
    "\n",
    "# Print percentage distribution\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Percentage Distribution\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTrain label percentage:\\n{df_train['label'].value_counts(normalize=True) * 100}\")\n",
    "print(f\"\\nVal label percentage:\\n{df_val['label'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b797aff",
   "metadata": {
    "id": "4b797aff"
   },
   "source": [
    "## **5. Transformations & Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94459d26",
   "metadata": {
    "id": "94459d26"
   },
   "outputs": [],
   "source": [
    "# Define augmentation for training with enhanced transformations\n",
    "train_augmentation = transforms.Compose([\n",
    "    # Geometric transformations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Small rotations to handle orientation variations\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),  # Reduced from 0.2 for more conservative shifts\n",
    "        scale=None,  # Add scale variation\n",
    "        shear=10  # Add shear transformation\n",
    "    ),\n",
    "\n",
    "    # Color/appearance transformations\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,  # Adjust brightness\n",
    "        contrast=0.2,    # Adjust contrast\n",
    "        saturation=0.2,  # Adjust saturation\n",
    "        hue=0.1          # Slight hue variation\n",
    "    ),\n",
    "    #transforms.RandomGrayscale(p=0.1),  # Occasionally convert to grayscale to improve robustness\n",
    "\n",
    "    # Occlusion simulation\n",
    "    #transforms.RandomErasing(\n",
    "    #    p=0.3,  # Reduced probability for more balanced augmentation\n",
    "    #    scale=(0.02, 0.15),  # Reduced max scale\n",
    "    #    ratio=(0.3, 3.3)  # Aspect ratio range\n",
    "    #),\n",
    "\n",
    "    # Optional: Add Gaussian blur for noise robustness\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cdd92",
   "metadata": {
    "id": "dc8cdd92"
   },
   "source": [
    "## **6. Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca78fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class TissueDataset(Dataset):\n",
    "    def __init__(self, df, img_dir=None, masks_dir=None, augmentation=None, normalize_imagenet=True, target_size=(224, 224), label_col='label_encoded'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing metadata.\n",
    "            img_dir: Root directory for images.\n",
    "            masks_dir: (Backwards compat) Root directory.\n",
    "            augmentation: Augmentation pipeline.\n",
    "            normalize_imagenet: Boolean to apply ImageNet norm.\n",
    "            target_size: Tuple (H, W).\n",
    "            label_col: Column name containing the Integer labels.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        \n",
    "        # --- Handle Directory Logic ---\n",
    "        root_dir = img_dir if img_dir is not None else masks_dir\n",
    "        \n",
    "        # Look for masks in root/masks/\n",
    "        if root_dir:\n",
    "            self.masks_location = os.path.join(root_dir, 'masks')\n",
    "        else:\n",
    "            self.masks_location = None\n",
    "\n",
    "        self.do_augmentation = augmentation is not None \n",
    "        self.normalize_imagenet = normalize_imagenet\n",
    "        self.target_size = target_size\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # 1. Standard Normalization\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "        # 2. Image-Only Augmentations\n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.0, contrast=0.2, saturation=0.0, hue=0.1\n",
    "        )\n",
    "        \n",
    "        # 3. Random Erasing\n",
    "        self.random_erasing = transforms.RandomErasing(\n",
    "            p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- A. Load Image ---\n",
    "        img_path = row['path']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # --- B. Load Mask (Handle Naming Difference) ---\n",
    "        if self.masks_location:\n",
    "            # 1. Get image filename (e.g., 'img_1234_p5.png')\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            \n",
    "            # 2. Convert to mask filename (e.g., 'mask_1234_p5.png')\n",
    "            # This is the critical fix for your naming convention\n",
    "            mask_filename = img_filename.replace('img_', 'mask_')\n",
    "            \n",
    "            mask_path = os.path.join(self.masks_location, mask_filename)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                mask = Image.open(mask_path).convert(\"L\") \n",
    "            else:\n",
    "                # Debugging tip: Print this if your masks are all black/empty to verify path\n",
    "                # print(f\"Missing mask: {mask_path}\") \n",
    "                mask = Image.new('L', image.size, 0)\n",
    "        else:\n",
    "            mask = Image.new('L', image.size, 0)\n",
    "\n",
    "        # --- C. Resize ---\n",
    "        image = TF.resize(image, self.target_size)\n",
    "        mask = TF.resize(mask, self.target_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "        # --- D. SYNCHRONIZED AUGMENTATION ---\n",
    "        if self.do_augmentation:\n",
    "            if random.random() > 0.5: # Flip\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "            angle = transforms.RandomRotation.get_params(degrees=[-15, 15]) # Rotate\n",
    "            image = TF.rotate(image, angle)\n",
    "            mask = TF.rotate(mask, angle)\n",
    "\n",
    "            # Affine\n",
    "            ret_angle, translations, scale, shear = transforms.RandomAffine.get_params(\n",
    "                degrees=[0, 0], translate=(0.1, 0.1), scale_ranges=None, shears=[-10, 10], img_size=image.size\n",
    "            )\n",
    "            image = TF.affine(image, angle=ret_angle, translate=translations, scale=scale, shear=shear)\n",
    "            mask = TF.affine(mask, angle=ret_angle, translate=translations, scale=scale, shear=shear)\n",
    "\n",
    "            image = self.color_jitter(image) # Color Jitter (Image Only)\n",
    "\n",
    "        # --- E. Convert to Tensor ---\n",
    "        img_tensor = TF.to_tensor(image)\n",
    "        mask_tensor = TF.to_tensor(mask)\n",
    "\n",
    "        ## --- F. Random Erasing (Image Tensor Only) ---\n",
    "        #if self.do_augmentation:\n",
    "        #    img_tensor = self.random_erasing(img_tensor)\n",
    "\n",
    "        # --- G. Final Prep ---\n",
    "        mask_tensor = (mask_tensor > 0.5).float()\n",
    "        \n",
    "        if self.normalize_imagenet:\n",
    "            img_tensor = self.normalize(img_tensor)\n",
    "\n",
    "        # --- H. Return Label ---\n",
    "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
    "        \n",
    "        return img_tensor, label, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4cb5a",
   "metadata": {
    "id": "ffa4cb5a"
   },
   "source": [
    "## **7. Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19525f4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19525f4f",
    "outputId": "6eb2758d-db78-4ec9-9135-2b5effb6c297"
   },
   "outputs": [],
   "source": [
    "# Instantiate Datasets\n",
    "train_dataset = TissueDataset(df_train, augmentation=train_augmentation, normalize_imagenet=True, img_dir=PATCHES_OUT   )\n",
    "val_dataset = TissueDataset(df_val, augmentation=None, normalize_imagenet=True, img_dir=PATCHES_OUT   )\n",
    "\n",
    "# Batch Size: 32 or 64 is standard for ResNet18/50 on 1MP images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "cpu_cores = os.cpu_count() or 2\n",
    "num_workers = cpu_cores//2\n",
    "# Instantiate Loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,          # Shuffle patches to break batch correlations\n",
    "    num_workers=num_workers,         # Adjust based on your CPU\n",
    "    pin_memory=True        # Faster data transfer to CUDA\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,         # No need to shuffle validation\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train Loader: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader: {len(val_loader)} batches\")\n",
    "print(f\"Num workers: {train_loader.num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bfef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "4212bfef",
    "outputId": "89015ce8-f70c-419c-c798-0243efeb0983"
   },
   "outputs": [],
   "source": [
    "def show_batch(loader, count=4):\n",
    "    # 1. Robust Unpacking\n",
    "    # The loader might return 2 items (Image, Label) or 3 (Image, Label, Mask)\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch[0], batch[1]\n",
    "    masks = batch[2] if len(batch) > 2 else None\n",
    "\n",
    "    # Determine layout: 1 row if no masks, 2 rows if masks exist\n",
    "    nrows = 2 if masks is not None else 1\n",
    "    plt.figure(figsize=(15, 5 * nrows))\n",
    "\n",
    "    # Denormalize for visualization (ImageNet stats)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    for i in range(count):\n",
    "        # --- Plot Image ---\n",
    "        ax = plt.subplot(nrows, count, i + 1)\n",
    "        \n",
    "        img = images[i]\n",
    "        img = img * std + mean  # Un-normalize\n",
    "        img = torch.clamp(img, 0, 1)  # Clip to valid range\n",
    "        \n",
    "        plt.imshow(img.permute(1, 2, 0)) # CHW -> HWC\n",
    "        plt.title(f\"Label: {labels[i].item()}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # --- Plot Mask (if available) ---\n",
    "        if masks is not None:\n",
    "            ax = plt.subplot(nrows, count, i + 1 + count) # Second row\n",
    "            mask = masks[i]\n",
    "            # Mask is (1, H, W), squeeze to (H, W) for matplotlib\n",
    "            plt.imshow(mask.squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Augmented Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing Training Batch (Augmented):\")\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89d235",
   "metadata": {
    "id": "7c89d235"
   },
   "source": [
    "## **8. Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e381193",
   "metadata": {
    "id": "9e381193"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_FT = 100\n",
    "NUM_EPOCHS_TL = 5  \n",
    "PATIENCE = 5 \n",
    "\n",
    "\n",
    "DROPOUT_RATE = 0.3\n",
    "HIDDEN_SIZE = 512\n",
    "L2_REG = 1e-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d978b",
   "metadata": {
    "id": "4f1d978b"
   },
   "source": [
    "## **9. Model Definition: Parallel - ResNet50 + ResNet18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4ec0",
   "metadata": {
    "id": "5c6c4ec0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetParallel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes=4, \n",
    "                 freeze_backbone=False,\n",
    "                 dropout_rate=0.3,):\n",
    "        super(ResNetParallel, self).__init__()\n",
    "        \n",
    "        # --- Pillar 1: The Color Stream (RGB Image) ---\n",
    "        # We use ResNet50 for the complex texture features\n",
    "        img_resnet = models.resnet50(weights='DEFAULT')\n",
    "        \n",
    "        # Remove the Classification Head (fc) and avgpool\n",
    "        # We want the spatial features or the flattened vector before the final layer\n",
    "        # Here we take the layers up to the final pooling\n",
    "        self.img_encoder = nn.Sequential(*list(img_resnet.children())[:-1])\n",
    "        img_out_dim = 2048 # ResNet50 output size\n",
    "        \n",
    "        # --- Pillar 2: The Geometry Stream (Mask) ---\n",
    "        # We use ResNet18 for the mask (shapes are simpler than textures)\n",
    "        mask_resnet = models.resnet18(weights='DEFAULT')\n",
    "        \n",
    "        # MODIFY FIRST LAYER: ResNet expects 3 channels (RGB), Mask is 1 channel (Gray)\n",
    "        # We sum the weights of the RGB channels to initialize the 1-channel weight\n",
    "        original_conv1 = mask_resnet.conv1\n",
    "        new_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        with torch.no_grad():\n",
    "            new_conv1.weight[:] = torch.sum(original_conv1.weight, dim=1, keepdim=True)\n",
    "        mask_resnet.conv1 = new_conv1\n",
    "        \n",
    "        # Remove head\n",
    "        self.mask_encoder = nn.Sequential(*list(mask_resnet.children())[:-1])\n",
    "        mask_out_dim = 512 # ResNet18 output size\n",
    "        \n",
    "        # --- Freeze Logic (Optional) ---\n",
    "        if freeze_backbone:\n",
    "            for param in self.img_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.mask_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Both pillars are frozen. Only the summit (classifier) will learn.\")\n",
    "\n",
    "        # --- The Summit: Fusion & Classification ---\n",
    "        # We concatenate the two vectors: 2048 (Image) + 512 (Mask) = 2560\n",
    "        fusion_dim = img_out_dim + mask_out_dim\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # Block 1: Compression & Non-Linearity\n",
    "            nn.Linear(fusion_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),              # Modern activation (better than ReLU)\n",
    "            nn.Dropout(p=dropout_rate+0.1),      # High dropout to force robust features\n",
    "            \n",
    "            # Block 2: Refinement\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            # Block 3: Final Decision\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def _init_weights(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        # 1. Walk the Path of Color\n",
    "        # img shape: [Batch, 3, 224, 224] -> [Batch, 2048, 1, 1]\n",
    "        img_feat = self.img_encoder(img)\n",
    "        img_feat = torch.flatten(img_feat, 1) # [Batch, 2048]\n",
    "        \n",
    "        # 2. Walk the Path of Geometry\n",
    "        # mask shape: [Batch, 1, 224, 224] -> [Batch, 512, 1, 1]\n",
    "        mask_feat = self.mask_encoder(mask)\n",
    "        mask_feat = torch.flatten(mask_feat, 1) # [Batch, 512]\n",
    "        \n",
    "        # 3. The Fusion (Meet at the Summit)\n",
    "        combined_feat = torch.cat((img_feat, mask_feat), dim=1) # [Batch, 2560]\n",
    "        \n",
    "        # 4. Final Decision\n",
    "        logits = self.classifier(combined_feat)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        \"\"\"Allow the pillars to adapt their foundations.\"\"\"\n",
    "        for param in self.img_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.mask_encoder.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ba3db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "439ba3db",
    "outputId": "e91a18e2-eb93-4cef-cf29-7c221c88ccf7"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Ensure device is defined (usually from previous cells, but safe to redefine if standalone)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming label_encoder is defined in your notebook scope\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = ResNetParallel(num_classes=num_classes,  freeze_backbone=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model initialized (ResNetParallel) with {num_classes} output classes.\")\n",
    "print_model_stats(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "model_graph = draw_graph(\n",
    "    model, \n",
    "    input_size=[(1, 3, TARGET_SIZE[0], TARGET_SIZE[1]), (1, 1, TARGET_SIZE[0], TARGET_SIZE[1])],\n",
    "    device=device.type\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb49a6",
   "metadata": {
    "id": "fcfb49a6"
   },
   "source": [
    "## **10. Criterion and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697106a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get Counts (from your snippet)\n",
    "class_counts = df_train['label_encoded'].value_counts().sort_index().values\n",
    "total_samples = sum(class_counts)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "# 2. Define Manual Tuning Factors (The \"weight\" knob)\n",
    "# 1.0 = Default (Pure Inverse Frequency)\n",
    "# > 1.0 = Force model to focus MORE on this class (e.g., critical error)\n",
    "# < 1.0 = Force model to focus LESS on this class (e.g., noisy label)\n",
    "# Ensure this list length matches n_classes (4 in your case)\n",
    "tuning_factors = torch.tensor([1.0, 1.0, 1.0, 0.6], dtype=torch.float32)\n",
    "\n",
    "# 3. Calculate Base Weights (Standard Inverse Frequency)\n",
    "# Formula: N / (C * freq)\n",
    "base_weights = torch.tensor(\n",
    "    [total_samples / (n_classes * c) for c in class_counts],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 4. Apply Tuning\n",
    "# Final Weight = Inverse_Freq_Weight * Manual_Tuning_Factor\n",
    "final_weights = base_weights * tuning_factors\n",
    "\n",
    "# 5. Move to device\n",
    "final_weights = final_weights.to(device)\n",
    "\n",
    "print(f\"Base Weights:  {base_weights}\")\n",
    "print(f\"Tuning Factors:{tuning_factors}\")\n",
    "print(f\"Final Weights: {final_weights}\")\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RAdam(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=1e-3,\n",
    "    # You can also include other Adam parameters like betas, eps, weight_decay\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=L2_REG\n",
    ")\n",
    "\n",
    "\n",
    "criterion_cls = torch.nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aa519",
   "metadata": {
    "id": "9c7aa519"
   },
   "source": [
    "## **11. Function: Training & Validation Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f066d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(preds, targets):\n",
    "    # Move to CPU and convert to numpy\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    # Calculate Macro F1 (balanced for all classes)\n",
    "    return f1_score(targets, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abb343",
   "metadata": {
    "id": "37abb343"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion_cls, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, labels, masks in loop:\n",
    "        images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- FIX 1: Pass BOTH Image and Mask ---\n",
    "        # The model \"fuses\" them to produce a single classification result\n",
    "        logits = model(images, masks) \n",
    "        \n",
    "        # --- FIX 2: Classification Loss Only ---\n",
    "        # We do not calculate segmentation loss because the mask is an INPUT here, not an output.\n",
    "        loss = criterion_cls(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate(model, loader, criterion_cls, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, masks in loader:\n",
    "            images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "            \n",
    "            # Pass both inputs\n",
    "            logits = model(images, masks)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion_cls(logits, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440fc05",
   "metadata": {
    "id": "e440fc05"
   },
   "source": [
    "## **12. Training Loop: Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6689e",
   "metadata": {
    "id": "41d6689e"
   },
   "source": [
    "### 12.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training Variables ---\n",
    "best_val_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_tl_epoch = 0\n",
    "model_saved = False\n",
    "\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "print(f\"Starting Training with ResNetParallel (Auxiliary Attention) (Patience: {PATIENCE})...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_TL):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion_cls, optimizer, device, )\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion_cls, device)\n",
    "\n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # --- Checkpointing (Save Best Model based on F1) ---\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0  # Reset counter\n",
    "        best_tl_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'models/best_model_ResNetParallel_tl.pt')\n",
    "        model_saved = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Print matching your requested format\n",
    "    if model_saved:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS_TL} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE} Best:{best_val_f1:.4f} \")\n",
    "    else:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS_TL} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"   >>> Early Stopping Triggered! Training stopped.\")\n",
    "        break\n",
    "    \n",
    "SUB_MODEL = 'models/best_model_ResNetParallel_tl.pt'\n",
    "print(f\"Submodel saved to {SUB_MODEL} at epoch {best_tl_epoch} with Val F1: {best_val_f1:.4f} for now. Will update if better model found in fine tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dbdd6",
   "metadata": {
    "id": "437dbdd6"
   },
   "source": [
    "### 12.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "a6263355",
    "outputId": "e4b0b334-f800-48f0-ee2d-303d639ad12b"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2068748",
   "metadata": {
    "id": "c2068748"
   },
   "source": [
    "### 12.4 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88a0a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "cd88a0a7",
    "outputId": "4e04e6e1-67f1-45aa-e1ae-84c28ea6bf6b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Calculate and Plot Confusion Matrix\n",
    "print(\"Generating Confusion Matrix on Original Images...\")\n",
    "y_true_img, y_pred_img = get_image_predictions(model, val_loader, device)\n",
    "\n",
    "# Compute Matrix\n",
    "cm = confusion_matrix(y_true_img, y_pred_img)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Aggregated per Image)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b568a48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "9b568a48",
    "outputId": "3f9a323d-c874-4a44-efa4-d2b6d501dbde"
   },
   "outputs": [],
   "source": [
    "# Visualize a random validation sample\n",
    "print(\"Plotting random validation sample with prediction distribution:\")\n",
    "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad76976",
   "metadata": {
    "id": "2ad76976"
   },
   "source": [
    "## **13. Training Loop: Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01cc5a",
   "metadata": {
    "id": "af01cc5a"
   },
   "source": [
    "### 13.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# How many blocks to unfreeze from the end of each backbone?\n",
    "# 0 = Frozen, 1 = Last Block, 2 = Last 2 Blocks, etc.\n",
    "UNFREEZE_LAYERS_IMG = 2  # For ResNet50 (The Color Pillar)\n",
    "UNFREEZE_LAYERS_MASK = 1  # For ResNet18 (The Geometry Pillar)\n",
    "\n",
    "# --- 1. Initialize the NEW model instance ---\n",
    "# Ensure you use the exact same class name you defined (TwoStreamResNet or ResNetParallel)\n",
    "ft_model = ResNetParallel(num_classes=num_classes, freeze_backbone=True).to(device)\n",
    "\n",
    "# --- 2. Load the best weights from the first phase ---\n",
    "ft_model.load_state_dict(torch.load(\"models/best_model_ResNetParallel_tl.pt\"), strict=True)\n",
    "\n",
    "print(f\"Fine-Tuning Configuration:\")\n",
    "print(f\" - Image Pillar (ResNet50): Unfreezing last {UNFREEZE_LAYERS_IMG} blocks\")\n",
    "print(f\" - Mask Pillar (ResNet18): Unfreezing last {UNFREEZE_LAYERS_MASK} blocks\")\n",
    "\n",
    "# --- 3. Unfreeze parameters ---\n",
    "\n",
    "# A. Unfreeze the Classifier (Always unfreeze the summit)\n",
    "for param in ft_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# B. Unfreeze Image Pillar (ResNet50)\n",
    "if UNFREEZE_LAYERS_IMG > 0:\n",
    "    # Get the last N children (blocks)\n",
    "    layers_to_train = list(ft_model.img_encoder.children())[-UNFREEZE_LAYERS_IMG:]\n",
    "    for block in layers_to_train:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "# C. Unfreeze Mask Pillar (ResNet18)\n",
    "if UNFREEZE_LAYERS_MASK > 0:\n",
    "    layers_to_train = list(ft_model.mask_encoder.children())[-UNFREEZE_LAYERS_MASK:]\n",
    "    for block in layers_to_train:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# --- 4. Optimizer ---\n",
    "ft_optimizer = torch.optim.RAdam(\n",
    "    [p for p in ft_model.parameters() if p.requires_grad],\n",
    "    lr=1e-5, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "# --- 5. Scheduler ---\n",
    "ft_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    ft_optimizer, mode='min', factor=0.1, patience=3, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ed2bdb",
    "outputId": "bba7f2ce-d9c2-4249-d9a1-fe1d04603a8f"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting Fine-Tuning Phase...\")\n",
    "\n",
    "# Reset history\n",
    "ft_history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1_ft = 0.0\n",
    "patience_counter_ft = 0\n",
    "best_ft_epoch = 0\n",
    "model_saved = False\n",
    "ft_better_than_tl = False\n",
    "\n",
    "# --- 6. Fine-Tuning Loop ---\n",
    "for epoch in range(NUM_EPOCHS_FT):\n",
    "    # Train (Pass BOTH criteria)\n",
    "    train_loss, train_f1 = train_one_epoch(ft_model, train_loader, criterion_cls , ft_optimizer, device)\n",
    "\n",
    "    # Validate (Pass BOTH criteria)\n",
    "    val_loss, val_f1 = validate(ft_model, val_loader, criterion_cls, device)\n",
    "\n",
    "    # Update Scheduler based on Validation Loss\n",
    "    ft_scheduler.step(val_loss)\n",
    "\n",
    "    # Store history\n",
    "    ft_history['train_loss'].append(train_loss)\n",
    "    ft_history['train_f1'].append(train_f1)\n",
    "    ft_history['val_loss'].append(val_loss)\n",
    "    ft_history['val_f1'].append(val_f1)\n",
    "\n",
    "    # --- Checkpointing & Early Stopping ---\n",
    "    if val_f1 > best_val_f1_ft:\n",
    "        best_val_f1_ft = val_f1\n",
    "        best_ft_epoch = epoch + 1\n",
    "        patience_counter_ft = 0 \n",
    "        \n",
    "        # Save the fine-tuned model\n",
    "        torch.save(ft_model.state_dict(), 'models/best_model_ResNetParallel_ft.pt')\n",
    "        model_saved = True\n",
    "        \n",
    "        # Check if we beat the Transfer Learning phase\n",
    "        if best_val_f1_ft > best_val_f1:\n",
    "            ft_better_than_tl = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter_ft += 1\n",
    "\n",
    "    # Print Status\n",
    "    status_mark = \"\" if model_saved else \"\"\n",
    "    print(f\"FT Epoch {epoch+1}/{NUM_EPOCHS_FT} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE} {status_mark}\")\n",
    "\n",
    "    if patience_counter_ft >= PATIENCE:\n",
    "        print(f\"\\nEarly Stopping Triggered! Best FT Epoch: {best_ft_epoch} with Val F1: {best_val_f1_ft:.4f}\")\n",
    "        break\n",
    "\n",
    "# Final Summary\n",
    "SUB_MODEL = 'models/best_model_ResNetParallel_ft.pt' if ft_better_than_tl else 'models/best_model_ResNetParallel_tl.pt'\n",
    "print(f\"\\nFine-Tuning Complete.\")\n",
    "print(f\"Best TL F1: {best_val_f1:.4f}\")\n",
    "print(f\"Best FT F1: {best_val_f1_ft:.4f}\")\n",
    "print(f\"Best Model Saved to: {SUB_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb7a76",
   "metadata": {
    "id": "b2bb7a76"
   },
   "source": [
    "### 13.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc006f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "f8dc006f",
    "outputId": "3c0c6a86-695a-4a21-b45c-a5b9bd0153f3"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ft_history['train_loss'], label='Train Loss')\n",
    "plt.plot(ft_history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ft_history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(ft_history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Fine-Tuned Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_ft_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c2dac",
   "metadata": {
    "id": "808c2dac"
   },
   "source": [
    "### 13.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230087f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "230087f0",
    "outputId": "246d19d8-ea87-4708-de4a-5931cbc85a5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get predictions for the Transfer Learning model (from original `model`)\n",
    "print(\"Generating Confusion Matrix for Transfer Learning Model...\")\n",
    "y_true_tl, y_pred_tl = get_image_predictions(model, val_loader, device)\n",
    "cm_tl = confusion_matrix(y_true_tl, y_pred_tl)\n",
    "\n",
    "# 2. Get predictions for the Fine-Tuning model (from `ft_model`)\n",
    "print(\"Generating Confusion Matrix for Fine-Tuning Model...\")\n",
    "y_true_ft, y_pred_ft = get_image_predictions(ft_model, val_loader, device)\n",
    "cm_ft = confusion_matrix(y_true_ft, y_pred_ft)\n",
    "\n",
    "# 3. Plotting side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot Transfer Learning Confusion Matrix\n",
    "sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_title('Confusion Matrix (Transfer Learning)')\n",
    "\n",
    "# Plot Fine-Tuning Confusion Matrix\n",
    "sns.heatmap(cm_ft, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_title('Confusion Matrix (Fine-Tuning)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710db55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "a710db55",
    "outputId": "88b89cb5-7868-4e30-e496-1da01ee65c19"
   },
   "outputs": [],
   "source": [
    "# Visualize a random validation sample\n",
    "print(\"Transfer Learning:\")\n",
    "sample_id_plot = np.random.choice(val_loader.dataset.df['sample_id'].unique())\n",
    "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)\n",
    "print(\"Fine Tuning:\")\n",
    "plot_sample_with_predictions(ft_model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923f870",
   "metadata": {},
   "source": [
    "## **14. Class Activation Maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c870f",
   "metadata": {},
   "source": [
    "### **14.1 Cam Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# --- 1. Grad-CAM Helper Class (Updated for Two-Stream) ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Hooks\n",
    "        self.handle_fwd = self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.handle_bwd = self.target_layer.register_full_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def __call__(self, x, mask, class_idx=None):\n",
    "        # 1. Forward pass (Pass BOTH inputs)\n",
    "        # Two-Stream model returns ONLY logits.\n",
    "        output = self.model(x, mask)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output)\n",
    "            \n",
    "        # 2. Backward pass\n",
    "        self.model.zero_grad()\n",
    "        score = output[0, class_idx]\n",
    "        score.backward()\n",
    "        \n",
    "        # 3. Generate CAM\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        \n",
    "        if gradients is None or activations is None:\n",
    "            raise RuntimeError(\"Hooks failed. Ensure target_layer is visited.\")\n",
    "\n",
    "        b, k, u, v = gradients.size()\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        alpha = gradients.view(b, k, -1).mean(2)\n",
    "        weights = alpha.view(b, k, 1, 1)\n",
    "        \n",
    "        # Linear combination\n",
    "        cam = (weights * activations).sum(1, keepdim=True)\n",
    "        cam = F.relu(cam) \n",
    "        \n",
    "        # 4. Upsample to image size\n",
    "        cam = F.interpolate(cam, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 5. Normalize\n",
    "        cam = cam.view(1, -1)\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-7)\n",
    "        cam = cam.view(1, 1, x.shape[2], x.shape[3])\n",
    "        \n",
    "        return cam.detach().cpu().numpy()[0, 0], output\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.handle_fwd.remove()\n",
    "        self.handle_bwd.remove()\n",
    "\n",
    "# --- 2. Mask Overlay Function ---\n",
    "def get_mask_overlay(cam_mask, patch_path, masks_dir, img_fallback=None, alpha=0.6):\n",
    "    \"\"\"Overlays CAM onto the ground truth mask.\"\"\"\n",
    "    bg_img = None\n",
    "    mask_found = False\n",
    "\n",
    "    if masks_dir:\n",
    "        # Robust filename logic\n",
    "        filename = os.path.basename(patch_path)\n",
    "        mask_filename = filename.replace('img_', 'mask_')\n",
    "        mask_path = os.path.join(masks_dir, mask_filename)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            try:\n",
    "                mask_pil = Image.open(mask_path).convert('L') \n",
    "                mask_pil = mask_pil.resize((224, 224))\n",
    "                mask_np = np.array(mask_pil)\n",
    "                mask_display = mask_np.astype(np.float32) / 255.0 \n",
    "                bg_img = np.stack([mask_display]*3, axis=-1)\n",
    "                mask_found = True\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    # Fallback\n",
    "    if bg_img is None:\n",
    "        bg_img = img_fallback if img_fallback is not None else np.zeros((224, 224, 3), dtype=np.float32)\n",
    "\n",
    "    # Heatmap\n",
    "    heatmap = cv2.resize(cam_mask, (224, 224))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_colored = np.float32(heatmap_colored) / 255\n",
    "\n",
    "    overlay = (alpha * heatmap_colored) + ((1 - alpha) * bg_img)\n",
    "    overlay = overlay / np.max(overlay) \n",
    "    \n",
    "    return overlay, mask_found\n",
    "\n",
    "# --- 3. Main Visualization Function (Two-Stream Updated) ---\n",
    "def visualize_sample_analysis(model, df_metadata, sample_id, label_encoder, device, masks_dir=None):\n",
    "    \"\"\"\n",
    "    Visualizes analysis for a sample using the Two-Stream Model.\n",
    "    Args:\n",
    "        model: The trained Two-Stream model.\n",
    "        df_metadata: The pandas DataFrame containing image paths and labels.\n",
    "        sample_id: The ID of the sample to visualize.\n",
    "        label_encoder: The encoder used for labels.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        masks_dir: (Optional) Path to the masks folder.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # --- 1. Filter DataFrame directly ---\n",
    "    sample_rows = df_metadata[df_metadata['sample_id'] == sample_id]\n",
    "    \n",
    "    if len(sample_rows) == 0:\n",
    "        print(f\"Sample {sample_id} not found in the provided DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Define Transforms ---\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                             std=[1/0.229, 1/0.224, 1/0.225])\n",
    "    ])\n",
    "\n",
    "    # --- 3. Batch Inference ---\n",
    "    patch_probs = []\n",
    "    patch_data = [] \n",
    "    \n",
    "    # Target Layer Selection\n",
    "    try:\n",
    "        # Try to grab the last bottleneck block of the image encoder\n",
    "        target_layer = model.img_encoder[-2][-1] \n",
    "    except:\n",
    "        # Fallback\n",
    "        target_layer = list(model.img_encoder.children())[-2]\n",
    "\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    print(f\"Processing sample {sample_id} with {len(sample_rows)} patches...\")\n",
    "    \n",
    "    for _, row in sample_rows.iterrows():\n",
    "        try:\n",
    "            # Load Image\n",
    "            img_pil = Image.open(row['path']).convert('RGB')\n",
    "            img_tensor = img_transform(img_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Load Mask\n",
    "            mask_pil = Image.new('L', (224, 224), 0)\n",
    "            if masks_dir:\n",
    "                fname = os.path.basename(row['path'])\n",
    "                mname = fname.replace('img_', 'mask_')\n",
    "                mpath = os.path.join(masks_dir, mname)\n",
    "                if os.path.exists(mpath):\n",
    "                    mask_pil = Image.open(mpath).convert('L')\n",
    "            \n",
    "            mask_tensor = mask_transform(mask_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            patch_data.append({\n",
    "                'img_tensor': img_tensor, \n",
    "                'mask_tensor': mask_tensor, \n",
    "                'path': row['path']\n",
    "            })\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(img_tensor, mask_tensor)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                patch_probs.append(probs.cpu().numpy())\n",
    "                \n",
    "        except Exception as e:\n",
    "            # print(f\"Skipping patch: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not patch_probs:\n",
    "        print(\"No valid patches found.\")\n",
    "        grad_cam.remove_hooks()\n",
    "        return\n",
    "\n",
    "    patch_probs = np.vstack(patch_probs)\n",
    "    avg_probs = np.mean(patch_probs, axis=0) \n",
    "    \n",
    "    classes = label_encoder.classes_\n",
    "    pred_class_idx = np.argmax(avg_probs)\n",
    "    pred_label = classes[pred_class_idx]\n",
    "    \n",
    "    # Get True Label\n",
    "    if 'label_encoded' in sample_rows.columns:\n",
    "        true_label_idx = sample_rows.iloc[0]['label_encoded']\n",
    "        true_label = label_encoder.inverse_transform([true_label_idx])[0]\n",
    "    else:\n",
    "        true_label = \"Unknown\"\n",
    "    \n",
    "    # --- 4. Select Best Patch ---\n",
    "    best_patch_idx = np.argmax(patch_probs[:, pred_class_idx])\n",
    "    best_data = patch_data[best_patch_idx]\n",
    "    \n",
    "    # Prepare active tensors for GradCAM\n",
    "    img_tensor_active = best_data['img_tensor'].clone().detach()\n",
    "    mask_tensor_active = best_data['mask_tensor'].clone().detach()\n",
    "    img_tensor_active.requires_grad = True \n",
    "    patch_path = best_data['path']\n",
    "\n",
    "    # --- 5. Generate GradCAM ---\n",
    "    # A. Predicted Class CAM\n",
    "    pred_cam_mask, _ = grad_cam(img_tensor_active, mask_tensor_active, pred_class_idx)\n",
    "    \n",
    "    # Display Image (Denormalized)\n",
    "    img_display = inv_normalize(best_data['img_tensor'][0]).cpu().detach().numpy()\n",
    "    img_display = np.transpose(img_display, (1, 2, 0))\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    # Overlay on Image\n",
    "    heatmap = cv2.resize(pred_cam_mask, (224, 224))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_colored = np.float32(heatmap_colored) / 255\n",
    "    cam_on_image = (0.5 * heatmap_colored) + (0.5 * img_display)\n",
    "    cam_on_image = cam_on_image / np.max(cam_on_image)\n",
    "    \n",
    "    # Overlay on Mask\n",
    "    cam_on_mask_main, mask_found_main = get_mask_overlay(\n",
    "        pred_cam_mask, patch_path, masks_dir, img_fallback=img_display\n",
    "    )\n",
    "\n",
    "    # B. Other Classes CAMs\n",
    "    other_indices = [i for i in range(len(classes)) if i != pred_class_idx]\n",
    "    cam_others_data = {}\n",
    "    \n",
    "    for i in other_indices:\n",
    "        img_tensor_active.grad = None \n",
    "        mask_map, _ = grad_cam(img_tensor_active, mask_tensor_active, i)\n",
    "        \n",
    "        overlay, is_mask = get_mask_overlay(\n",
    "            mask_map, patch_path, masks_dir, img_fallback=img_display\n",
    "        )\n",
    "        cam_others_data[i] = (overlay, is_mask)\n",
    "        \n",
    "    grad_cam.remove_hooks() \n",
    "\n",
    "    # --- 6. Plotting ---\n",
    "    fig = plt.figure(figsize=(24, 10))\n",
    "    gs = gridspec.GridSpec(2, max(4, len(other_indices)), height_ratios=[1.2, 0.8])\n",
    "\n",
    "    # Row 1\n",
    "    # 1. Original\n",
    "    ax0 = plt.subplot(gs[0, 0])\n",
    "    ax0.imshow(img_display)\n",
    "    ax0.set_title(f\"Best Patch\\nTrue: {true_label}\", fontsize=12, fontweight='bold')\n",
    "    ax0.axis('off')\n",
    "\n",
    "    # 2. Predicted CAM on Image\n",
    "    ax1 = plt.subplot(gs[0, 1])\n",
    "    ax1.imshow(cam_on_image)\n",
    "    ax1.set_title(f\"Focus (Img): {pred_label}\\n({avg_probs[pred_class_idx]:.1%})\", fontsize=12, fontweight='bold', color='darkblue')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 3. Predicted CAM on Mask\n",
    "    ax2 = plt.subplot(gs[0, 2])\n",
    "    if mask_found_main:\n",
    "        ax2.imshow(cam_on_mask_main)\n",
    "        ax2.set_title(f\"Focus (Mask): {pred_label}\", fontsize=12, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"Mask Not Found\\n(Using Image BG)\", ha='center')\n",
    "        ax2.imshow(cam_on_mask_main) # Show overlay on fallback image\n",
    "        ax2.axis('off')\n",
    "\n",
    "    # 4. Stats\n",
    "    ax3 = plt.subplot(gs[0, 3])\n",
    "    colors = ['#d3d3d3'] * len(classes)\n",
    "    colors[pred_class_idx] = '#4CAF50' if pred_label == true_label else '#F44336'\n",
    "    \n",
    "    bars = ax3.bar(classes, avg_probs, color=colors, alpha=0.85, edgecolor='black')\n",
    "    ax3.set_title(f\"Sample ID: {sample_id}\", fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                 f'{height:.1%}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Row 2: Other Classes\n",
    "    if not other_indices:\n",
    "        # Handle binary case where there are no \"other\" classes if we only plot top 1 vs others\n",
    "        pass\n",
    "    else:\n",
    "        for idx, class_idx in enumerate(other_indices):\n",
    "            ax = plt.subplot(gs[1, idx])\n",
    "            overlay, is_mask = cam_others_data[class_idx]\n",
    "            \n",
    "            ax.imshow(overlay)\n",
    "            class_name = classes[class_idx]\n",
    "            ax.set_title(f\"{class_name}\\n({avg_probs[class_idx]:.1%})\", fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1efec",
   "metadata": {},
   "source": [
    "### **14.2 CAM Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample ID from your validation set\n",
    "if SUB_MODEL is None:\n",
    "    SUB_MODEL = 'models/best_model_ResNetMTL_tl.pt'  # Default to TL model if none specified\n",
    "PATCH_MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\",\"masks\")\n",
    "cam_model = ResNetParallel(num_classes, freeze_backbone=True).to(device)\n",
    "cam_model.load_state_dict(torch.load(SUB_MODEL), strict=True)\n",
    "cam_idx = random.randint(0, len(df_val['sample_id'].unique()) - 1)\n",
    "sample_id_to_test = df_val['sample_id'].iloc[cam_idx] \n",
    "visualize_sample_analysis(cam_model, patches_metadata_df, sample_id_to_test, label_encoder, device)\n",
    "visualize_sample_analysis(cam_model, patches_metadata_df, sample_id_to_test, label_encoder, device, masks_dir=PATCH_MASKS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06322077",
   "metadata": {
    "id": "06322077"
   },
   "source": [
    "## **14. Submission Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9edaa",
   "metadata": {
    "id": "05e9edaa"
   },
   "source": [
    "### 14.1 Create Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_model = ResNetParallel(num_classes, freeze_backbone=True).to(device)\n",
    "\n",
    "# 2. Load the best weights from the training\n",
    "sub_model.load_state_dict(torch.load(SUB_MODEL), strict=True)\n",
    "print(f\"Submodel loaded from {SUB_MODEL} for final evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f4093",
   "metadata": {
    "id": "8a4f4093"
   },
   "source": [
    "### 14.2 Define Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b22e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Validation Transform (Compatible with V2 or Standard)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.ToTensor(), # Robust standard equivalent to ToImage + ToDtype\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36adb9",
   "metadata": {
    "id": "db36adb9"
   },
   "source": [
    "### 14.3 Function: Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21a0e8",
   "metadata": {
    "id": "9c21a0e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "def generate_submission(model, submission_folder, masks_folder=None, method='max_confidence', output_csv=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Generates a submission file for the Two-Stream (Two Pillar) model.\n",
    "    Args:\n",
    "        model: Trained TwoStream/ResNetParallel model.\n",
    "        submission_folder: Folder containing test patch images.\n",
    "        masks_folder: (Optional) Folder containing corresponding masks. \n",
    "                      If None, tries to find them or uses black masks.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Get list of test patches\n",
    "    patch_files = sorted([f for f in os.listdir(submission_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    print(f\"Found {len(patch_files)} patches in {submission_folder}\")\n",
    "    if masks_folder:\n",
    "        print(f\"Looking for masks in: {masks_folder}\")\n",
    "    else:\n",
    "        print(\"No mask folder provided. Using black masks (or inferred paths).\")\n",
    "\n",
    "    # 2. Define Transforms\n",
    "    # Image: Normalize\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Mask: No Normalize\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Store predictions per image\n",
    "    image_predictions = {}\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(patch_files):\n",
    "            filepath = os.path.join(submission_folder, filename)\n",
    "\n",
    "            try:\n",
    "                # Extract Sample ID\n",
    "                if '_p' in filename:\n",
    "                    sample_id = filename.rsplit('_p', 1)[0]\n",
    "                else:\n",
    "                    sample_id = os.path.splitext(filename)[0]\n",
    "\n",
    "                if sample_id not in image_predictions:\n",
    "                    image_predictions[sample_id] = {'probs': []}\n",
    "\n",
    "                # --- A. Load Image ---\n",
    "                image = Image.open(filepath).convert('RGB')\n",
    "                input_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                # --- B. Load Mask ---\n",
    "                # Default: Black mask\n",
    "                mask_pil = Image.new('L', (224, 224), 0)\n",
    "                \n",
    "                if masks_folder:\n",
    "                    # Construct mask filename (e.g., img_X.png -> mask_X.png)\n",
    "                    mask_filename = filename.replace('img_', 'mask_')\n",
    "                    mask_path = os.path.join(masks_folder, mask_filename)\n",
    "                    \n",
    "                    if os.path.exists(mask_path):\n",
    "                        mask_pil = Image.open(mask_path).convert('L')\n",
    "                \n",
    "                mask_tensor = mask_transform(mask_pil).unsqueeze(0).to(device)\n",
    "\n",
    "                # --- C. Model Inference (Two Pillars) ---\n",
    "                # Pass BOTH tensors\n",
    "                logits = model(input_tensor, mask_tensor)\n",
    "                \n",
    "                # Softmax to get probabilities\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "                image_predictions[sample_id]['probs'].append(probs.cpu().numpy()[0])\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error processing {filename}: {e}\")\n",
    "                pass\n",
    "\n",
    "    # 3. Aggregate Results\n",
    "    final_results = []\n",
    "\n",
    "    print(f\"Aggregating results for {len(image_predictions)} unique samples...\")\n",
    "\n",
    "    for sample_id, data in image_predictions.items():\n",
    "        all_probs = np.array(data['probs']) \n",
    "\n",
    "        # Reconstruct full filename for submission (e.g. img_0015.png)\n",
    "        sample_index_name = f\"{sample_id}.png\"\n",
    "\n",
    "        if len(all_probs) == 0:\n",
    "            final_results.append({'sample_index': sample_index_name, 'label': \"Luminal A\"}) \n",
    "            continue\n",
    "\n",
    "        if method == 'majority_voting':\n",
    "            patch_preds = np.argmax(all_probs, axis=1)\n",
    "            counts = np.bincount(patch_preds)\n",
    "            final_class_idx = np.argmax(counts)\n",
    "\n",
    "        elif method == 'max_confidence':\n",
    "            avg_probs = np.mean(all_probs, axis=0)\n",
    "            final_class_idx = np.argmax(avg_probs)\n",
    "\n",
    "        # Decode Label\n",
    "        pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
    "        final_results.append({'sample_index': sample_index_name, 'label': pred_label})\n",
    "\n",
    "    # 4. Save\n",
    "    df_submission = pd.DataFrame(final_results)\n",
    "    \n",
    "    if 'sample_index' in df_submission.columns:\n",
    "        df_submission.sort_values('sample_index', inplace=True)\n",
    "    \n",
    "    df_submission.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Submission saved to {output_csv}\")\n",
    "    # print(df_submission.head())\n",
    "\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078d160",
   "metadata": {
    "id": "7078d160"
   },
   "source": [
    "### 14.3 Create the Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e05e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "187e05e0",
    "outputId": "f8fb865f-86ae-4157-8cbc-a93efa39be5f"
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_time_str = now.strftime(\"%d_%b-%H_%M\")\n",
    "\n",
    "sub_dir = os.path.join(os.path.pardir, \"submission_csvs\")\n",
    "OUTPUT_NAME = os.path.join(sub_dir, f\"submission_ft--{date_time_str}.csv\")\n",
    "\n",
    "\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "# Check if folder exists\n",
    "if os.path.exists(SUBMISSION_PATCHES_OUT):\n",
    "    # Method 1: Max Confidence / Average Probability (Recommended)\n",
    "    df_sub_max_conf = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, SUBMISSION_PATCH_MASKS, method='max_confidence', output_csv=OUTPUT_NAME)\n",
    "    print(f\"Submission CSV saved to: {OUTPUT_NAME}\")\n",
    "\n",
    "    # Method 2: Majority Voting (Optional, uncomment to run)\n",
    "    # df_sub_majority_voting = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='majority_voting', output_csv=\"submission_voting.csv\")\n",
    "else:\n",
    "    print(f\"Directory '{SUBMISSION_PATCHES_OUT}' not found. Please create it or set the correct path.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
