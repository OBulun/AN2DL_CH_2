{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4969644a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fcb3f",
   "metadata": {},
   "source": [
    "## **2. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_UNMASKED = False  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "\n",
    "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
    "\n",
    "output_path = os.path.join(datasets_path, \"train_masked\")\n",
    "\n",
    "if USE_UNMASKED:\n",
    "    resized_path = os.path.join(datasets_path, \"train_resized\")\n",
    "\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "print(f\"Test data path: {test_data_path}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "if USE_UNMASKED:\n",
    "    print(f\"Resized path: {resized_path}\")\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path             # Folder containing img_xxxx and mask_xxxx\n",
    "OUTPUT_FOLDER = output_path\n",
    "if USE_UNMASKED:                   # Where the resized and masked images will be saved            \n",
    "    RESIZED_FOLDER = resized_path               # Where the resized unmasked images will be saved\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(image_path, mask_path, output_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads an image and a mask, resizes them, applies the mask, and saves the result.\n",
    "    \"\"\"\n",
    "    # 1. Load Image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        tqdm.write(f\"Error: Could not load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # 2. Load Mask (Read as grayscale)\n",
    "    mask = cv2.imread(str(mask_path), 0)\n",
    "    if mask is None:\n",
    "        tqdm.write(f\"Error: Could not load mask at {mask_path}\")\n",
    "        return\n",
    "\n",
    "    # 3. Resize both to target size\n",
    "    if target_size is not None:\n",
    "        # Resize image using linear interpolation (better for photos)\n",
    "        img = cv2.resize(img, target_size)\n",
    "        # Resize mask using nearest neighbor to preserve sharp edges\n",
    "        mask = cv2.resize(mask, target_size)\n",
    "\n",
    "    # 4. Ensure mask is strictly binary (0 or 255)\n",
    "    # Values > 127 become 255 (White), others become 0 (Black)\n",
    "    _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 5. Apply the mask\n",
    "    # cv2.bitwise_and keeps the pixel where the mask is 255, and makes it 0 where mask is 0\n",
    "    masked_img = cv2.bitwise_and(img, img, mask=binary_mask)\n",
    "\n",
    "    # 6. Save result\n",
    "    cv2.imwrite(str(output_path), masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Iterates through input_dir, finds pairs of 'img_xxxx' and 'mask_xxxx',\n",
    "    resizes them to target_size, and saves the masked result to output_dir.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extensions to look for\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    \n",
    "    # 1. Gather all valid image files first\n",
    "    print(f\"Scanning for images in: {input_dir}...\")\n",
    "    image_files = [\n",
    "        f for f in input_dir.iterdir() \n",
    "        if f.name.startswith('img_') and f.suffix.lower() in valid_extensions\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found starting with 'img_' in the directory.\")\n",
    "        return\n",
    "\n",
    "    # 2. Iterate with tqdm\n",
    "    count = 0\n",
    "    \n",
    "    for file_path in tqdm(image_files, desc=\"Processing Images\", unit=\"img\"):\n",
    "        # Construct expected mask filename: img_123.jpg -> mask_123.jpg\n",
    "        mask_name = file_path.name.replace('img_', 'mask_', 1)\n",
    "        mask_path = input_dir / mask_name\n",
    "        \n",
    "        # Fallback: If mask_123.jpg doesn't exist, try mask_123.png\n",
    "        if not mask_path.exists():\n",
    "            mask_stem = file_path.stem.replace('img_', 'mask_', 1)\n",
    "            mask_path = input_dir / (mask_stem + \".png\")\n",
    "        \n",
    "        if mask_path.exists():\n",
    "            output_path = output_dir / file_path.name\n",
    "            \n",
    "            if output_path.exists():\n",
    "                # Skip silently\n",
    "                continue\n",
    "\n",
    "            apply_mask(file_path, mask_path, output_path, target_size=target_size)\n",
    "            count += 1\n",
    "        else:\n",
    "            # Use tqdm.write to log errors without breaking the progress bar\n",
    "            tqdm.write(f\"Skipping {file_path.name}: No matching mask found (looked for {mask_name})\")\n",
    "\n",
    "    print(f\"Batch processing complete. Processed {count} new images.\")\n",
    "\n",
    "def resize_batch(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Iterates through input_dir, finds 'img_xxxx', resizes them to target_size, \n",
    "    and saves the result to output_dir.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extensions to look for\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    \n",
    "    # 1. Gather all valid image files first\n",
    "    print(f\"Scanning for images in: {input_dir}...\")\n",
    "    image_files = [\n",
    "        f for f in input_dir.iterdir() \n",
    "        if f.name.startswith('img_') and f.suffix.lower() in valid_extensions\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found starting with 'img_' in the directory.\")\n",
    "        return\n",
    "\n",
    "    # 2. Iterate with tqdm\n",
    "    count = 0\n",
    "    \n",
    "    for file_path in tqdm(image_files, desc=\"Resizing Images\", unit=\"img\"):\n",
    "        output_path = output_dir / file_path.name\n",
    "        \n",
    "        if output_path.exists():\n",
    "            # Skip silently\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(file_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "            \n",
    "        if target_size is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "            \n",
    "        cv2.imwrite(str(output_path), img)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Resizing complete. Processed {count} new images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbf077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, resize=None, filter_prefix=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from a specified folder with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Path to the folder containing images\n",
    "        resize (tuple): Target size (width, height) or None\n",
    "        filter_prefix (str): Prefix to filter filenames (e.g. 'img_')\n",
    "\n",
    "    Returns:\n",
    "        tuple: (np.ndarray of images, list of filenames)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    loaded_filenames = []\n",
    "    \n",
    "    # Get the list of files to iterate over\n",
    "    file_list = sorted(os.listdir(folder))\n",
    "    \n",
    "    # Filter files before iterating to show correct progress\n",
    "    if filter_prefix:\n",
    "        file_list = [f for f in file_list if f.startswith(filter_prefix)]\n",
    "\n",
    "    # Iterate through files with a tqdm progress bar\n",
    "    for filename in tqdm(file_list, desc=f\"Loading images from {os.path.basename(folder)}\"):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Check if image was loaded successfully right away\n",
    "        if img is None:\n",
    "            # print(f\"Warning: Failed to load image at {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Normalize image pixel values to a float range [0, 1]\n",
    "        img = (img / 255.0).astype(np.float32)\n",
    "\n",
    "        # Convert image from BGR to RGB\n",
    "        img = img[..., ::-1]\n",
    "\n",
    "        if resize:\n",
    "            img = cv2.resize(img, resize)\n",
    "\n",
    "        images.append(img)\n",
    "        loaded_filenames.append(filename)\n",
    "\n",
    "    return np.array(images), loaded_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_batch(SOURCE_FOLDER, OUTPUT_FOLDER, target_size=TARGET_SIZE)\n",
    "MASKED_IMAGE_PATH = OUTPUT_FOLDER\n",
    "if USE_UNMASKED:\n",
    "    resize_batch(SOURCE_FOLDER, RESIZED_FOLDER, target_size=TARGET_SIZE)\n",
    "    RESIZED_IMAGE_PATH = RESIZED_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd395a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load masked images (already resized in process_batch)\n",
    "# We add filter_prefix='img_' to ensure we don't load any mask files that might be in the folder\n",
    "masked_images, masked_filenames = load_images_from_folder(MASKED_IMAGE_PATH, filter_prefix='img_')\n",
    "\n",
    "# Load original images (already resized in resize_batch)\n",
    "if USE_UNMASKED:\n",
    "    original_images, original_filenames = load_images_from_folder(RESIZED_IMAGE_PATH, filter_prefix='img_')\n",
    "    # Combine images\n",
    "    train_images = np.concatenate([masked_images, original_images], axis=0)\n",
    "    # Combine filenames\n",
    "    filenames = masked_filenames + original_filenames\n",
    "else:\n",
    "    original_images = []\n",
    "    train_images = masked_images\n",
    "    filenames = masked_filenames\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(masked_images)} masked images and {len(original_images)} original images.\")\n",
    "print(f\"Total images: {len(train_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4255eb",
   "metadata": {},
   "source": [
    "## **3. Exploration and Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9577b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ title Mask Verification\n",
    "# Select a few random images from the output folder to verify the process\n",
    "masked_files = os.listdir(OUTPUT_FOLDER)\n",
    "num_samples = 3\n",
    "selected_files = random.sample(masked_files, num_samples)\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(15, 3 * num_samples))\n",
    "\n",
    "for i, filename in enumerate(selected_files):\n",
    "    # 1. Define paths\n",
    "    masked_image_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "    original_image_path = os.path.join(SOURCE_FOLDER, filename)\n",
    "    \n",
    "    # Logic to find the mask path (replicating logic from Cell 5)\n",
    "    mask_name = filename.replace('img_', 'mask_', 1)\n",
    "    mask_path = os.path.join(SOURCE_FOLDER, mask_name)\n",
    "    \n",
    "    # Check if mask exists, if not try .png fallback as per Cell 5 logic\n",
    "    if not os.path.exists(mask_path):\n",
    "        mask_stem = os.path.splitext(filename)[0].replace('img_', 'mask_', 1)\n",
    "        mask_path = os.path.join(SOURCE_FOLDER, mask_stem + \".png\")\n",
    "\n",
    "    # 2. Load images\n",
    "    # Original\n",
    "    orig_img = cv2.imread(original_image_path)\n",
    "    if orig_img is not None:\n",
    "        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Mask\n",
    "    mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Masked (Result)\n",
    "    masked_img = cv2.imread(masked_image_path)\n",
    "    if masked_img is not None:\n",
    "        masked_img = cv2.cvtColor(masked_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 3. Plot\n",
    "    # Original\n",
    "    if orig_img is not None:\n",
    "        axes[i, 0].imshow(orig_img)\n",
    "        axes[i, 0].set_title(f\"Original\\n{filename}\")\n",
    "    else:\n",
    "        axes[i, 0].text(0.5, 0.5, \"Original Not Found\", ha='center')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Mask\n",
    "    if mask_img is not None:\n",
    "        axes[i, 1].imshow(mask_img, cmap='gray')\n",
    "        axes[i, 1].set_title(f\"Mask\")\n",
    "    else:\n",
    "        axes[i, 1].text(0.5, 0.5, \"Mask Not Found\", ha='center')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    # Masked\n",
    "    if masked_img is not None:\n",
    "        axes[i, 2].imshow(masked_img)\n",
    "        axes[i, 2].set_title(f\"Masked Result (Resized)\")\n",
    "    else:\n",
    "        axes[i, 2].text(0.5, 0.5, \"Result Not Found\", ha='center')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images to display\n",
    "num_img = 10\n",
    "# Starting index\n",
    "start_img= random.randint(0, len(train_images) - num_img)\n",
    "# Create subplots for displaying items\n",
    "fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
    "for i in range(start_img, start_img+num_img):\n",
    "    ax = axes[i%2, i%num_img//2]\n",
    "    ax.imshow(np.clip(train_images[i], 0, 1))  # Display clipped item images\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b56890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load the labels CSV\n",
    "labels_df = pd.read_csv(train_labels_path)\n",
    "# Create a dictionary mapping filename -> label for fast lookup\n",
    "labels_map = dict(zip(labels_df.iloc[:, 0], labels_df.iloc[:, 1]))\n",
    "\n",
    "# 2. Get the filenames corresponding to the images loaded in Cell 7\n",
    "# IMPORTANT: This must match the order used in 'load_images_from_folder' exactly.\n",
    "# We use the filenames list created in the previous cell.\n",
    "# filenames = os.listdir(MASKED_IMAGE_PATH)\n",
    "\n",
    "print(\"Aligning labels to loaded images...\")\n",
    "\n",
    "X_aligned = []\n",
    "y_aligned = []\n",
    "\n",
    "# 3. Iterate through the filenames to sync X (images) and y (labels)\n",
    "# We assume 'train_images' matches the order of 'filenames' because they rely on the same os.listdir call order\n",
    "# provided no files were added/deleted between Cell 7 and Cell 9.\n",
    "if len(filenames) != len(train_images):\n",
    "    raise ValueError(f\"Mismatch! Loaded images ({len(train_images)}) != Files found ({len(filenames)}).\")\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    if filename in labels_map:\n",
    "        # If the file exists in our CSV, keep the image and the label\n",
    "        X_aligned.append(train_images[i])\n",
    "        y_aligned.append(labels_map[filename])\n",
    "    else:\n",
    "        # If image is in folder but NOT in CSV, we must discard the image\n",
    "        print(f\"Skipping {filename}: Image found but no label in CSV.\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X_aligned)\n",
    "y = np.array(y_aligned)\n",
    "\n",
    "print(f\"Images aligned: {X.shape}\")\n",
    "print(f\"Labels aligned: {y.shape}\")\n",
    "\n",
    "# 4. Encode labels (String -> Integer)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print class mapping\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# 5. Train-Test Split (Stratified)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, Labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape based on the training data\n",
    "input_shape = (X_train.shape[3], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa29695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch datasets (without augmentation for now)\n",
    "train_ds = TensorDataset(\n",
    "    torch.from_numpy(X_train).permute(0, 3, 1, 2),\n",
    "    torch.from_numpy(y_train).squeeze().long()\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    torch.from_numpy(X_val).permute(0, 3, 1, 2),\n",
    "    torch.from_numpy(y_val).squeeze().long()\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.from_numpy(X_test).permute(0, 3, 1, 2),\n",
    "    torch.from_numpy(y_test).squeeze().long()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader with optimized settings.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): PyTorch Dataset object\n",
    "        batch_size (int): Number of samples per batch\n",
    "        shuffle (bool): Whether to shuffle data at each epoch\n",
    "        drop_last (bool): Whether to drop last incomplete batch\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: Configured DataLoader instance\n",
    "    \"\"\"\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with different settings for each phase\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from the training data loader\n",
    "for xb, yb in train_loader:\n",
    "    print(\"Features batch shape:\", xb.shape)\n",
    "    print(\"Labels batch shape:\", yb.shape)\n",
    "    break  # Stop after getting one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fce4",
   "metadata": {},
   "source": [
    "## **4. Training Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 50\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.2         # Dropout probability\n",
    "L1_LAMBDA = 0            # L1 penalty\n",
    "L2_LAMBDA = 0            # L2 penalty\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", EPOCHS)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Learning Rare:\", LEARNING_RATE)\n",
    "print(\"Dropout Rate:\", DROPOUT_RATE)\n",
    "print(\"L1 Penalty:\", L1_LAMBDA)\n",
    "print(\"L2 Penalty:\", L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a766f63",
   "metadata": {},
   "source": [
    "## **5. CNN Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b386427",
   "metadata": {},
   "source": [
    "## 5.1 Phikon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# Load Phikon (ViT-Base) - 86M parameters\n",
    "# This is much smaller than H-optimus-0 (1100M)\n",
    "model_name = 'vit_base_patch16_224' \n",
    "\n",
    "model = timm.create_model(\n",
    "    model_name, \n",
    "    pretrained=True, \n",
    "    num_classes=0  # Remove head for feature extraction\n",
    ")\n",
    "\n",
    "# Standard preprocessing for this model\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "print(f\"Loaded Phikon-Base. Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 2. Freeze EVERYTHING\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. Add a simple trainable classifier on top\n",
    "# Phikon-Base outputs 768 features (ViT-Base standard)\n",
    "class LightweightClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(768, num_classes) # 768 -> 2 (Disease/Normal)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x) # Shape: [Batch, 768]\n",
    "        return self.head(features)\n",
    "\n",
    "model_to_train = LightweightClassifier(model, num_classes=2)\n",
    "\n",
    "# Now only the 'head' parameters will update. \n",
    "# You can train this on a standard laptop CPU if needed (though GPU is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c86b0",
   "metadata": {},
   "source": [
    "## **7. Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b239851",
   "metadata": {},
   "source": [
    "### 7.1 Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea533d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = accuracy_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions)\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = accuracy_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions)\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        patience=0, evaluation_metric=\"val_acc\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n",
    "    \"\"\"Train the neural network model.\"\"\"\n",
    "\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': []\n",
    "    }\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_acc'].append(train_acc)\n",
    "        training_history['val_acc'].append(val_acc)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "            writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/Training', train_acc, epoch)\n",
    "            writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b037b",
   "metadata": {},
   "source": [
    "### 7.2  Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e616b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, input_size=input_shape)\n",
    "model_graph = draw_graph(model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=6, device=device)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Move the trainable model to the device (GPU/CPU)\n",
    "model_to_train = model_to_train.to(device)\n",
    "\n",
    "# 2. Define Optimizer\n",
    "# We only optimize parameters that require gradients (the head)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_to_train.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=L2_LAMBDA\n",
    ")\n",
    "\n",
    "# 3. Define Scaler for Mixed Precision Training\n",
    "# Enabled only if CUDA is available to prevent errors on CPU\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# 4. Define TensorBoard Writer\n",
    "writer = SummaryWriter(log_dir=f\"{logs_dir}/phikon_finetune\")\n",
    "\n",
    "# 5. Run Training\n",
    "model_to_train, history = fit(\n",
    "    model=model_to_train,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    patience=PATIENCE,\n",
    "    evaluation_metric=\"val_acc\",\n",
    "    mode=\"max\",)\n",
    "    # 1. Move the trainable model to the device (GPU/CPU)\n",
    "model_to_train = model_to_train.to(device)\n",
    "\n",
    "# 2. Define Optimizer\n",
    "# We only optimize parameters that require gradients (the head)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_to_train.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=L2_LAMBDA\n",
    ")\n",
    "\n",
    "# 3. Define Scaler for Mixed Precision Training\n",
    "# Enabled only if CUDA is available to prevent errors on CPU\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# 4. Define TensorBoard Writer\n",
    "writer = SummaryWriter(log_dir=f\"{logs_dir}/phikon_finetune\")\n",
    "\n",
    "# 5. Run Training\n",
    "model_to_train, history = fit(\n",
    "    model=model_to_train,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    patience=PATIENCE,\n",
    "    evaluation_metric=\"val_acc\",\n",
    "    mode=\"max\",)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
