{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3293c812",
   "metadata": {
    "id": "3293c812"
   },
   "source": [
    "## **1. Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631b342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4631b342",
    "outputId": "58fbc501-7799-471f-9a5a-e64689609281"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#cur_dir = \"/content/drive/MyDrive/CH2/Notebooks\"\n",
    "#%cd $cur_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfd385",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8bfd385",
    "outputId": "b4006194-5220-4fbf-9b47-0be0b5673790"
   },
   "outputs": [],
   "source": [
    "#%pip install torchview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee15c7",
   "metadata": {
    "id": "edee15c7"
   },
   "source": [
    "## **2. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c7403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "963c7403",
    "outputId": "b6727f0e-41f8-4a6b-dea0-f74d54e6a861"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tfs\n",
    "\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ee8",
   "metadata": {
    "id": "3b5b1ee8"
   },
   "source": [
    "## **3. Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d674a2d",
   "metadata": {
    "id": "7d674a2d"
   },
   "outputs": [],
   "source": [
    "USE_MASKED_PATCHES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3da911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df3da911",
    "outputId": "8f0365f9-913a-4b45-9039-d4b26ac59c53"
   },
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path\n",
    "\n",
    "if USE_MASKED_PATCHES:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"train_patches_masked\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"submission_patches_masked\")\n",
    "else:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\")\n",
    "\n",
    "MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\", \"train_patches\", \"masks\")\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "print(f\"Test data path: {test_data_path}\")\n",
    "print(f\"Patches output path: {PATCHES_OUT}\")\n",
    "print(f\"Submission patches output path: {SUBMISSION_PATCHES_OUT}\")\n",
    "  \n",
    "\n",
    "# ImageNet normalization statistics\n",
    "IMAGENET_MEAN = [float(x) for x in [0.485, 0.456, 0.406]]  # or convert your current values to float\n",
    "IMAGENET_STD  = [float(x) for x in [0.229, 0.224, 0.225]]\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple function to handle this\n",
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions(model, loader, device):\n",
    "    \"\"\"\n",
    "    Aggregates patch-level predictions to image-level.\n",
    "    Strategy: Average the Softmax probabilities of all patches in a bag,\n",
    "    then take the argmax. This handles 'noisy/benign' patches well.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Access the dataframe and transform stored in the dataset\n",
    "    dataset = loader.dataset\n",
    "    df = dataset.df\n",
    "    transform = dataset.transform\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Get unique sample IDs (original images)\n",
    "    sample_ids = df['sample_id'].unique()\n",
    "\n",
    "    print(f\"\\nAggregating predictions for {len(sample_ids)} unique images...\")\n",
    "\n",
    "    for sample_id in tqdm(sample_ids, leave=False):\n",
    "        # Get all patches belonging to this image\n",
    "        sample_patches = df[df['sample_id'] == sample_id]\n",
    "\n",
    "        # Ground Truth (all patches share the image label)\n",
    "        true_label = sample_patches.iloc[0]['label_encoded']\n",
    "        y_true.append(true_label)\n",
    "\n",
    "        # Load and process all patches for this image\n",
    "        patches = []\n",
    "        for img_path in sample_patches['path']:\n",
    "            try:\n",
    "                # Load image (ensure RGB)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                if transform:\n",
    "                    img = transform(img)\n",
    "                patches.append(img)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if not patches:\n",
    "            y_pred.append(true_label) # Fallback (should not happen)\n",
    "            continue\n",
    "\n",
    "        # Stack patches into a single batch: [Num_Patches, 3, 224, 224]\n",
    "        batch = torch.stack(patches).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # --- Aggregation: Mean Probability ---\n",
    "            # Averaging probabilities reduces the impact of outliers (benign patches)\n",
    "            avg_probs = torch.mean(probs, dim=0)\n",
    "            pred_label = torch.argmax(avg_probs).item()\n",
    "\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_with_predictions(model, loader, device, label_encoder, sample_id=None, aggregation_method='max_confidence'):\n",
    "    \"\"\"Plot all patches of a single sample and the aggregated image prediction.\"\"\"\n",
    "    import math\n",
    "    model.eval()\n",
    "    dataset = loader.dataset\n",
    "    df = dataset.df\n",
    "\n",
    "    # Pick a sample_id\n",
    "    if sample_id is None:\n",
    "        sample_id = np.random.choice(df['sample_id'].unique())\n",
    "    sample_patches = df[df['sample_id'] == sample_id].reset_index(drop=True)\n",
    "\n",
    "    # Load and transform all patches\n",
    "    images_tensors = []\n",
    "    display_imgs = []\n",
    "    for _, row in sample_patches.iterrows():\n",
    "        img = Image.open(row['path']).convert('RGB')\n",
    "        if dataset.transform:\n",
    "            tensor_img = dataset.transform(img)\n",
    "        else:\n",
    "            tensor_img = transforms.ToTensor()(img)\n",
    "        images_tensors.append(tensor_img)\n",
    "        # denormalize for display\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "        display_imgs.append(torch.clamp(tensor_img * std + mean, 0, 1))\n",
    "\n",
    "    batch = torch.stack(images_tensors).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "        probs = torch.softmax(logits, dim=1).cpu()\n",
    "\n",
    "    patch_preds = probs.argmax(dim=1).numpy()\n",
    "    patch_confs = probs.max(dim=1).values.numpy()\n",
    "\n",
    "    if aggregation_method == 'max_confidence':\n",
    "        # Average probabilities (Soft Voting)\n",
    "        image_probs = probs.mean(dim=0).numpy()\n",
    "        image_pred = image_probs.argmax()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    elif aggregation_method == 'majority_voting':\n",
    "        # Hard Voting\n",
    "        counts = np.bincount(patch_preds, minlength=len(label_encoder.classes_))\n",
    "        image_pred = counts.argmax()\n",
    "        # Normalize counts for visualization purposes\n",
    "        image_probs = counts / counts.sum()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n",
    "\n",
    "    image_pred_label = label_encoder.inverse_transform([image_pred])[0]\n",
    "    true_label = label_encoder.inverse_transform([sample_patches.iloc[0]['label_encoded']])[0]\n",
    "\n",
    "    cols = min(6, len(sample_patches))\n",
    "    rows = math.ceil(len(sample_patches) / cols)\n",
    "    fig = plt.figure(figsize=(3*cols + 4, 3*rows))\n",
    "    gs = fig.add_gridspec(rows, cols + 1, width_ratios=[1]*cols + [1.3])\n",
    "\n",
    "    # Patch grid\n",
    "    for idx, (img_disp, pred, conf) in enumerate(zip(display_imgs, patch_preds, patch_confs)):\n",
    "        ax = fig.add_subplot(gs[idx // cols, idx % cols])\n",
    "        ax.imshow(img_disp.permute(1,2,0))\n",
    "        lbl = label_encoder.inverse_transform([pred])[0]\n",
    "        ax.set_title(f\"{lbl}\\n{conf:.2%}\", fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Aggregated distribution\n",
    "    ax_bar = fig.add_subplot(gs[:, -1])\n",
    "    class_names = label_encoder.classes_\n",
    "    colors = ['green' if i == image_pred else 'steelblue' for i in range(len(class_names))]\n",
    "    ax_bar.barh(class_names, image_probs, color=colors)\n",
    "    ax_bar.set_xlabel('Probability' if aggregation_method == 'max_confidence' else 'Vote Share')\n",
    "    ax_bar.set_xlim([0,1])\n",
    "    ax_bar.set_title(f\"Sample: {sample_id}\\nTrue: {true_label} | Pred: {image_pred_label} ({image_conf:.2%})\\nMethod: {aggregation_method}\")\n",
    "    for i, prob in enumerate(image_probs):\n",
    "        ax_bar.text(prob + 0.02, i, f\"{prob:.3f}\", va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427b09e",
   "metadata": {
    "id": "e427b09e"
   },
   "source": [
    "## **4. Train/Val Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7455a",
   "metadata": {
    "id": "7cc7455a"
   },
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
    "    \"\"\"\n",
    "    # 1. Load the labels CSV\n",
    "    # Assuming CSV structure: [image_id, label] or similar\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "\n",
    "    # Standardize column names for easier merging\n",
    "    # We assume the first column is the ID and the second is the Label\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "\n",
    "    # Ensure IDs in CSV are strings (to match filenames)\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "\n",
    "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
    "    # because our parsed Bag IDs won't have them.\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # 2. List all patch files\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
    "\n",
    "    # 3. Parse filenames to get Bag IDs\n",
    "    data = []\n",
    "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
    "\n",
    "    for filename in patch_files:\n",
    "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
    "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
    "\n",
    "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
    "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
    "        try:\n",
    "            bag_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(f\"Skipping malformed filename: {filename}\")\n",
    "\n",
    "    # Create temporary patches DataFrame\n",
    "    df_patches = pd.DataFrame(data)\n",
    "\n",
    "    # 4. Merge patches with labels\n",
    "    # This assigns the correct Bag Label to every Patch in that Bag\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
    "\n",
    "    # 5. Clean up and Rename\n",
    "    # Keep only required columns\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "\n",
    "    # Rename label column to standard 'label' if it isn't already\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "\n",
    "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b8e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633b8e05",
    "outputId": "7d671b8b-3e81-446b-d478-67847824bf41"
   },
   "outputs": [],
   "source": [
    "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(patches_metadata_df.head().drop(columns=['path']))\n",
    "print(\"\\nPatches per Bag (Distribution):\")\n",
    "print(patches_metadata_df['sample_id'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6c6cd3",
    "outputId": "be243fc3-59b3-4318-ed4b-09eb2ce21d3d"
   },
   "outputs": [],
   "source": [
    "# Add Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "\n",
    "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"  {orig} -> {enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52314ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52314ae9",
    "outputId": "ce81e6f4-f3fe-4311-ebd2-11946f50a08e"
   },
   "outputs": [],
   "source": [
    "# Train/Val Split on Original Images (not patches)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train/Val Split on Original Images\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get unique sample IDs\n",
    "unique_samples = patches_metadata_df['sample_id'].unique()\n",
    "print(f\"\\nTotal unique samples (original images): {len(unique_samples)}\")\n",
    "\n",
    "# Split samples into train (80%) and val (20%)\n",
    "train_samples, val_samples = train_test_split(\n",
    "    unique_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples, 'label_encoded'].values\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples: {len(val_samples)}\")\n",
    "\n",
    "# Create train and val DataFrames by filtering patches\n",
    "df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_samples)].reset_index(drop=True)\n",
    "df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_samples)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain patches: {len(df_train)}\")\n",
    "print(f\"Val patches: {len(df_val)}\")\n",
    "print(f\"\\nTrain label distribution:\\n{df_train['label'].value_counts()}\")\n",
    "print(f\"\\nVal label distribution:\\n{df_val['label'].value_counts()}\")\n",
    "\n",
    "# Print percentage distribution\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Percentage Distribution\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTrain label percentage:\\n{df_train['label'].value_counts(normalize=True) * 100}\")\n",
    "print(f\"\\nVal label percentage:\\n{df_val['label'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b797aff",
   "metadata": {
    "id": "4b797aff"
   },
   "source": [
    "## **5. Transformations & Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94459d26",
   "metadata": {
    "id": "94459d26"
   },
   "outputs": [],
   "source": [
    "# Define augmentation for training with enhanced transformations\n",
    "train_augmentation = transforms.Compose([\n",
    "    # Geometric transformations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Small rotations to handle orientation variations\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),  # Reduced from 0.2 for more conservative shifts\n",
    "        scale=None,  # Add scale variation\n",
    "        shear=10  # Add shear transformation\n",
    "    ),\n",
    "\n",
    "    # Color/appearance transformations\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,  # Adjust brightness\n",
    "        contrast=0.2,    # Adjust contrast\n",
    "        saturation=0.2,  # Adjust saturation\n",
    "        hue=0.1          # Slight hue variation\n",
    "    ),\n",
    "    #transforms.RandomGrayscale(p=0.1),  # Occasionally convert to grayscale to improve robustness\n",
    "\n",
    "    # Occlusion simulation\n",
    "    #transforms.RandomErasing(\n",
    "    #    p=0.3,  # Reduced probability for more balanced augmentation\n",
    "    #    scale=(0.02, 0.15),  # Reduced max scale\n",
    "    #    ratio=(0.3, 3.3)  # Aspect ratio range\n",
    "    #),\n",
    "\n",
    "    # Optional: Add Gaussian blur for noise robustness\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cdd92",
   "metadata": {
    "id": "dc8cdd92"
   },
   "source": [
    "## **6. Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b69bba",
   "metadata": {
    "id": "29b69bba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class TissueDataset(Dataset):\n",
    "    def __init__(self, df, img_dir=None, masks_dir=None, augmentation=None, normalize_imagenet=True, target_size=(224, 224), label_col='label_encoded'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing metadata.\n",
    "            img_dir: Root directory for images.\n",
    "            masks_dir: (Backwards compat) Root directory.\n",
    "            augmentation: Augmentation pipeline.\n",
    "            normalize_imagenet: Boolean to apply ImageNet norm.\n",
    "            target_size: Tuple (H, W).\n",
    "            label_col: Column name containing the Integer labels.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        \n",
    "        # --- Handle Directory Logic ---\n",
    "        root_dir = img_dir if img_dir is not None else masks_dir\n",
    "        \n",
    "        # Look for masks in root/masks/\n",
    "        if root_dir:\n",
    "            self.masks_location = os.path.join(root_dir, 'masks')\n",
    "        else:\n",
    "            self.masks_location = None\n",
    "\n",
    "        self.do_augmentation = augmentation is not None \n",
    "        self.normalize_imagenet = normalize_imagenet\n",
    "        self.target_size = target_size\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # 1. Standard Normalization\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "        # 2. Image-Only Augmentations\n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.0, contrast=0.2, saturation=0.0, hue=0.1\n",
    "        )\n",
    "        \n",
    "        # 3. Random Erasing\n",
    "        self.random_erasing = transforms.RandomErasing(\n",
    "            p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- A. Load Image ---\n",
    "        img_path = row['path']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # --- B. Load Mask (Handle Naming Difference) ---\n",
    "        if self.masks_location:\n",
    "            # 1. Get image filename (e.g., 'img_1234_p5.png')\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            \n",
    "            # 2. Convert to mask filename (e.g., 'mask_1234_p5.png')\n",
    "            # This is the critical fix for your naming convention\n",
    "            mask_filename = img_filename.replace('img_', 'mask_')\n",
    "            \n",
    "            mask_path = os.path.join(self.masks_location, mask_filename)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                mask = Image.open(mask_path).convert(\"L\") \n",
    "            else:\n",
    "                # Debugging tip: Print this if your masks are all black/empty to verify path\n",
    "                # print(f\"Missing mask: {mask_path}\") \n",
    "                mask = Image.new('L', image.size, 0)\n",
    "        else:\n",
    "            mask = Image.new('L', image.size, 0)\n",
    "\n",
    "        # --- C. Resize ---\n",
    "        image = TF.resize(image, self.target_size)\n",
    "        mask = TF.resize(mask, self.target_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "        # --- D. SYNCHRONIZED AUGMENTATION ---\n",
    "        if self.do_augmentation:\n",
    "            if random.random() > 0.5: # Flip\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "            angle = transforms.RandomRotation.get_params(degrees=[-15, 15]) # Rotate\n",
    "            image = TF.rotate(image, angle)\n",
    "            mask = TF.rotate(mask, angle)\n",
    "\n",
    "            # Affine\n",
    "            ret_angle, translations, scale, shear = transforms.RandomAffine.get_params(\n",
    "                degrees=[0, 0], translate=(0.1, 0.1), scale_ranges=None, shears=[-10, 10], img_size=image.size\n",
    "            )\n",
    "            image = TF.affine(image, angle=ret_angle, translate=translations, scale=scale, shear=shear)\n",
    "            mask = TF.affine(mask, angle=ret_angle, translate=translations, scale=scale, shear=shear)\n",
    "\n",
    "            image = self.color_jitter(image) # Color Jitter (Image Only)\n",
    "\n",
    "        # --- E. Convert to Tensor ---\n",
    "        img_tensor = TF.to_tensor(image)\n",
    "        mask_tensor = TF.to_tensor(mask)\n",
    "\n",
    "        ## --- F. Random Erasing (Image Tensor Only) ---\n",
    "        #if self.do_augmentation:\n",
    "        #    img_tensor = self.random_erasing(img_tensor)\n",
    "\n",
    "        # --- G. Final Prep ---\n",
    "        mask_tensor = (mask_tensor > 0.5).float()\n",
    "        \n",
    "        if self.normalize_imagenet:\n",
    "            img_tensor = self.normalize(img_tensor)\n",
    "\n",
    "        # --- H. Return Label ---\n",
    "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
    "        \n",
    "        return img_tensor, label, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4cb5a",
   "metadata": {
    "id": "ffa4cb5a"
   },
   "source": [
    "## **7. Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a923ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL: \n",
    "    num_workers = 0\n",
    "    CACHE_IMAGES = False\n",
    "else:\n",
    "    num_workers = os.cpu_count()//2\n",
    "    CACHE_IMAGES = True\n",
    "\n",
    "# Instantiate Datasets\n",
    "train_dataset = TissueDataset(\n",
    "    df_train, \n",
    "    augmentation=train_augmentation, \n",
    "    normalize_imagenet=True, \n",
    "    img_dir=PATCHES_OUT\n",
    ")\n",
    "val_dataset = TissueDataset(\n",
    "    df_val, \n",
    "    augmentation=None, \n",
    "    normalize_imagenet=True,\n",
    "    img_dir=PATCHES_OUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19525f4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19525f4f",
    "outputId": "6eb2758d-db78-4ec9-9135-2b5effb6c297"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Batch Size: 32 or 64 is standard for ResNet18/50 on 1MP images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate Loaders with optimizations\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    " # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "# Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "print(f\"Train Loader: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader: {len(val_loader)} batches\")\n",
    "print(f\"Num workers: {train_loader.num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bfef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "4212bfef",
    "outputId": "89015ce8-f70c-419c-c798-0243efeb0983"
   },
   "outputs": [],
   "source": [
    "def show_batch(loader, count=4):\n",
    "    # 1. Robust Unpacking\n",
    "    # The loader might return 2 items (Image, Label) or 3 (Image, Label, Mask)\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch[0], batch[1]\n",
    "    masks = batch[2] if len(batch) > 2 else None\n",
    "\n",
    "    # Determine layout: 1 row if no masks, 2 rows if masks exist\n",
    "    nrows = 2 if masks is not None else 1\n",
    "    plt.figure(figsize=(15, 5 * nrows))\n",
    "\n",
    "    # Denormalize for visualization (ImageNet stats)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    for i in range(count):\n",
    "        # --- Plot Image ---\n",
    "        ax = plt.subplot(nrows, count, i + 1)\n",
    "        \n",
    "        img = images[i]\n",
    "        img = img * std + mean  # Un-normalize\n",
    "        img = torch.clamp(img, 0, 1)  # Clip to valid range\n",
    "        \n",
    "        plt.imshow(img.permute(1, 2, 0)) # CHW -> HWC\n",
    "        plt.title(f\"Label: {labels[i].item()}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # --- Plot Mask (if available) ---\n",
    "        if masks is not None:\n",
    "            ax = plt.subplot(nrows, count, i + 1 + count) # Second row\n",
    "            mask = masks[i]\n",
    "            # Mask is (1, H, W), squeeze to (H, W) for matplotlib\n",
    "            plt.imshow(mask.squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Augmented Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing Training Batch (Augmented):\")\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89d235",
   "metadata": {
    "id": "7c89d235"
   },
   "source": [
    "## **8. Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e381193",
   "metadata": {
    "id": "9e381193"
   },
   "outputs": [],
   "source": [
    "DROPOUT_RATE = 0.3\n",
    "HIDDEN_SIZE = 512\n",
    "L2_REG = 1e-4\n",
    "\n",
    "NUM_EPOCHS = 100  # Increased since we have early stopping\n",
    "PATIENCE = 5    # Stop if val_loss doesn't improve for 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d978b",
   "metadata": {
    "id": "4f1d978b"
   },
   "source": [
    "## **9. Model Definition (Transfer Learning - MobileNetV3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4ec0",
   "metadata": {
    "id": "5c6c4ec0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# IMPORTANT: this is RetCCL's ResNet implementation (copy ResNet.py from the RetCCL repo)\n",
    "import ResNet as RetCCLResNet\n",
    "\n",
    "\n",
    "def _clean_state_dict(sd: dict) -> dict:\n",
    "    \"\"\"Strip common prefixes (DataParallel, wrapper modules).\"\"\"\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        for p in (\"module.\", \"model.\", \"encoder.\", \"backbone.\"):\n",
    "            if k.startswith(p):\n",
    "                k = k[len(p):]\n",
    "        out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "class MaskedGAP(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Global Average Pooling.\n",
    "    feat: [B, C, Hf, Wf]\n",
    "    mask: [B, 1, H, W] or [B, H, W] (will be resized to Hf x Wf)\n",
    "    \"\"\"\n",
    "    def __init__(self, mode: str = \"bilinear\", eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, feat: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(1)  # [B,1,H,W]\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Resize mask to feature map resolution\n",
    "        mask = F.interpolate(mask, size=feat.shape[-2:], mode=self.mode,\n",
    "                             align_corners=False if self.mode in (\"bilinear\", \"bicubic\") else None)\n",
    "\n",
    "        # Weighted average over spatial dims\n",
    "        masked_feat = feat * mask\n",
    "        denom = mask.sum(dim=(2, 3), keepdim=True).clamp_min(self.eps)\n",
    "        pooled = masked_feat.sum(dim=(2, 3), keepdim=True) / denom  # [B,C,1,1]\n",
    "        return pooled.flatten(1)  # [B,C]\n",
    "\n",
    "\n",
    "class RetCCLResNet50_MGAP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        dropout_rate: float = 0.2,\n",
    "        freeze_backbone: bool = True,\n",
    "        ckpt_path: str = \"best_ckpt.pth\",\n",
    "        unfreeze_last_block: bool = True,\n",
    "        mask_pool_mode: str = \"bilinear\",  # use \"nearest\" if you want strictly-binary masks after resize\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Build RetCCL ResNet50\n",
    "        self.backbone = RetCCLResNet.resnet50(\n",
    "            num_classes=128, mlp=False, two_branch=False, normlinear=True\n",
    "        )\n",
    "\n",
    "        # 2) Load RetCCL pretrained weights (drop fc.*)\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        if isinstance(sd, dict) and \"state_dict\" in sd:\n",
    "            sd = sd[\"state_dict\"]\n",
    "        sd = _clean_state_dict(sd)\n",
    "        sd = {k: v for k, v in sd.items() if not k.startswith(\"fc.\")}\n",
    "        self.backbone.load_state_dict(sd, strict=False)\n",
    "\n",
    "        # 3) We will NOT use backbone.avgpool / backbone.fc; we do our own pooling + head\n",
    "        self.masked_gap = MaskedGAP(mode=mask_pool_mode)\n",
    "\n",
    "        # Infer feature dim robustly (ResNet50 is typically 2048)\n",
    "        in_features = 2048\n",
    "        if hasattr(self.backbone, \"layer4\"):\n",
    "            try:\n",
    "                # torchvision-like Bottleneck last conv is conv3\n",
    "                in_features = self.backbone.layer4[-1].conv3.out_channels\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "        # 4) Freeze backbone (optional) + optionally unfreeze last block\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            if unfreeze_last_block and hasattr(self.backbone, \"layer4\"):\n",
    "                for p in self.backbone.layer4.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        # always train head\n",
    "        for p in self.head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward through backbone until the last conv feature map.\n",
    "        Assumes torchvision-like ResNet blocks (common for RetCCL).\n",
    "        \"\"\"\n",
    "        b = self.backbone\n",
    "        x = b.conv1(x)\n",
    "        x = b.bn1(x)\n",
    "        x = b.relu(x)\n",
    "        x = b.maxpool(x)\n",
    "        x = b.layer1(x)\n",
    "        x = b.layer2(x)\n",
    "        x = b.layer3(x)\n",
    "        x = b.layer4(x)\n",
    "        return x  # [B,C,Hf,Wf]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        feat = self.forward_features(x)\n",
    "\n",
    "        if mask is None:\n",
    "            pooled = feat.mean(dim=(2, 3))   # standard GAP\n",
    "        else:\n",
    "            pooled = self.masked_gap(feat, mask)\n",
    "\n",
    "        return self.head(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ba3db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "439ba3db",
    "outputId": "e91a18e2-eb93-4cef-cf29-7c221c88ccf7"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Ensure device is defined (usually from previous cells, but safe to redefine if standalone)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming label_encoder is defined in your notebook scope\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = RetCCLResNet50_MGAP(num_classes, DROPOUT_RATE, \n",
    "                       freeze_backbone=True  , \n",
    "                       ckpt_path=os.path.join(\"models\",\"best_ckpt.pth\"),\n",
    "                       unfreeze_last_block=False)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model initialized (RetCCLResNet50_MGAP) with {num_classes} output classes.\")\n",
    "summary(model, input_size=(3, TARGET_SIZE[0], TARGET_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "model_graph = draw_graph(\n",
    "    model, \n",
    "    input_size=[(1, 3, TARGET_SIZE[0], TARGET_SIZE[1]),(1, 1, TARGET_SIZE[0], TARGET_SIZE[1])],\n",
    "    device=device.type\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb49a6",
   "metadata": {
    "id": "fcfb49a6"
   },
   "source": [
    "## **10. Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Shape [C].\n",
    "            gamma (float): Focusing parameter. Higher value = more focus on hard examples.\n",
    "                           Default is 2.0 (standard from the paper).\n",
    "            reduction (str): 'mean', 'sum', or 'none'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: [Batch, C] (Logits)\n",
    "        # targets: [Batch] (Class Indices)\n",
    "        \n",
    "        # 1. Standard Cross Entropy Loss (element-wise, no reduction yet)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # 2. Get the probability of the true class (pt)\n",
    "        # pt = exp(-ce_loss) because ce_loss = -log(pt)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # 3. Calculate Focal Component: (1 - pt)^gamma\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        # 4. Apply Class Weights (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            # Gather the alpha value corresponding to the target class for each sample\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            \n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        # 5. Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c87c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b270c87c",
    "outputId": "3e5283ef-c4a8-4fef-b0b3-0399a7ee7e65"
   },
   "outputs": [],
   "source": [
    "# 1. Get Counts (from your snippet)\n",
    "class_counts = df_train['label_encoded'].value_counts().sort_index().values\n",
    "total_samples = sum(class_counts)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "# 2. Define Manual Tuning Factors (The \"weight\" knob)\n",
    "# 1.0 = Default (Pure Inverse Frequency)\n",
    "# > 1.0 = Force model to focus MORE on this class (e.g., critical error)\n",
    "# < 1.0 = Force model to focus LESS on this class (e.g., noisy label)\n",
    "# Ensure this list length matches n_classes (4 in your case)\n",
    "tuning_factors = torch.tensor([1.0, 1.0, 1.0, 0.6], dtype=torch.float32)\n",
    "\n",
    "# 3. Calculate Base Weights (Standard Inverse Frequency)\n",
    "# Formula: N / (C * freq)\n",
    "base_weights = torch.tensor(\n",
    "    [total_samples / (n_classes * c) for c in class_counts],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 4. Apply Tuning\n",
    "# Final Weight = Inverse_Freq_Weight * Manual_Tuning_Factor\n",
    "final_weights = base_weights * tuning_factors\n",
    "\n",
    "# 5. Move to device\n",
    "final_weights = final_weights.to(device)\n",
    "\n",
    "print(f\"Base Weights:  {base_weights}\")\n",
    "print(f\"Tuning Factors:{tuning_factors}\")\n",
    "\n",
    "\n",
    "# Update Loss Function\n",
    "criterion = nn.CrossEntropyLoss(weight=final_weights, label_smoothing=0.1)\n",
    "print(f\"Applied Weights: {criterion.weight}\")\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RAdam(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=1e-3,\n",
    "    # You can also include other Adam parameters like betas, eps, weight_decay\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=L2_REG\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aa519",
   "metadata": {
    "id": "9c7aa519"
   },
   "source": [
    "## **11. Function: Training & Validation Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abb343",
   "metadata": {
    "id": "37abb343"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion_cls, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, labels, masks in loop:\n",
    "        images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- FIX 1: Pass BOTH Image and Mask ---\n",
    "        # The model \"fuses\" them to produce a single classification result\n",
    "        logits = model(images, masks) \n",
    "        \n",
    "        # --- FIX 2: Classification Loss Only ---\n",
    "        # We do not calculate segmentation loss because the mask is an INPUT here, not an output.\n",
    "        loss = criterion_cls(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate(model, loader, criterion_cls, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, masks in loader:\n",
    "            images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "            \n",
    "            # Pass both inputs\n",
    "            logits = model(images, masks)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion_cls(logits, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440fc05",
   "metadata": {
    "id": "e440fc05"
   },
   "source": [
    "## **12. Training Loop: Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6689e",
   "metadata": {
    "id": "41d6689e"
   },
   "source": [
    "### 12.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897de7e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "897de7e8",
    "outputId": "b75e0d4b-e0b2-46f9-87da-27ef1f064240"
   },
   "outputs": [],
   "source": [
    "best_val_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_tl_epoch = 0\n",
    "model_saved = False\n",
    "\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "print(f\"Starting Training with RetCCLResNet50_MGAP (Patience: {PATIENCE})...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Update Scheduler (based on Loss) EXPERIMENT: No scheduler on TL\n",
    "    #scheduler.step(val_loss)\n",
    "\n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "\n",
    "    # --- Checkpointing (Save Best Model based on F1) ---\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0  # Reset counter\n",
    "        best_tl_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'models/best_model_RetCCLResNet50_MGAP_tl.pt')\n",
    "        model_saved = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "    if model_saved:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE} Best:{best_val_f1:.4f} \")\n",
    "    else:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"   >>> Early Stopping Triggered! Training stopped.\")\n",
    "        break\n",
    "    \n",
    "SUB_MODEL = 'models/best_model_RetCCLResNet50_MGAP_tl.pt'\n",
    "print(f\"Submodel saved to {SUB_MODEL} at epoch {best_tl_epoch} with Val F1: {best_val_f1:.4f} for now. Will update if better model found in fine tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dbdd6",
   "metadata": {
    "id": "437dbdd6"
   },
   "source": [
    "### 12.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "a6263355",
    "outputId": "e4b0b334-f800-48f0-ee2d-303d639ad12b"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2068748",
   "metadata": {
    "id": "c2068748"
   },
   "source": [
    "### 12.4 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88a0a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "cd88a0a7",
    "outputId": "4e04e6e1-67f1-45aa-e1ae-84c28ea6bf6b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Calculate and Plot Confusion Matrix\n",
    "print(\"Generating Confusion Matrix on Original Images...\")\n",
    "y_true_img, y_pred_img = get_image_predictions(model, val_loader, device)\n",
    "\n",
    "# Compute Matrix\n",
    "cm = confusion_matrix(y_true_img, y_pred_img)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Aggregated per Image)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b568a48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "9b568a48",
    "outputId": "3f9a323d-c874-4a44-efa4-d2b6d501dbde"
   },
   "outputs": [],
   "source": [
    "# Visualize a random validation sample\n",
    "print(\"Plotting random validation sample with prediction distribution:\")\n",
    "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad76976",
   "metadata": {
    "id": "2ad76976"
   },
   "source": [
    "## **13. Training Loop: Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01cc5a",
   "metadata": {
    "id": "af01cc5a"
   },
   "source": [
    "### 13.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the NEW model instance\n",
    "#    (Using the ResNet18 class we defined)\n",
    "ft_model = RetCCLResNet50_MGAP(num_classes, DROPOUT_RATE, \n",
    "                          freeze_backbone=True,\n",
    "                        ckpt_path=os.path.join(\"models\",\"best_model_RetCCLResNet50_MGAP_tl.pt\"),\n",
    "                          unfreeze_last_block=True).to(device)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "ft_optimizer = torch.optim.RAdam(\n",
    "    [p for p in ft_model.parameters() if p.requires_grad],\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=L2_REG\n",
    ")\n",
    "\n",
    "# 5. New Scheduler\n",
    "ft_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    ft_optimizer, mode='min', factor=0.1, patience=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ed2bdb",
    "outputId": "bba7f2ce-d9c2-4249-d9a1-fe1d04603a8f"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting Fine-Tuning (ResNet18)\")\n",
    "\n",
    "# Reset history\n",
    "ft_history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1_ft = 0.0\n",
    "patience_counter_ft = 0\n",
    "best_ft_epoch = 0\n",
    "model_saved = False\n",
    "ft_better_than_tl = False\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(ft_model, train_loader, criterion, ft_optimizer, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(ft_model, val_loader, criterion, device)\n",
    "\n",
    "    # Update Scheduler\n",
    "    ft_scheduler.step(val_loss)\n",
    "\n",
    "    ft_history['train_loss'].append(train_loss)\n",
    "    ft_history['train_f1'].append(train_f1)\n",
    "    ft_history['val_loss'].append(val_loss)\n",
    "    ft_history['val_f1'].append(val_f1)\n",
    "\n",
    "    # --- Checkpointing ---\n",
    "    if val_f1 > best_val_f1_ft:\n",
    "        best_val_f1_ft = val_f1\n",
    "        best_ft_epoch = epoch + 1\n",
    "        patience_counter_ft = 0 \n",
    "        # UPDATED FILENAME: Changed from mobilenetv3 to resnet18\n",
    "        torch.save(ft_model.state_dict(), 'models/best_model_RetCCLResNet50_MGAP_ft.pt')\n",
    "        model_saved = True\n",
    "        # Assuming best_val_f1 exists from previous TL phase\n",
    "        if best_val_f1_ft > best_val_f1:\n",
    "            ft_better_than_tl = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter_ft += 1\n",
    "\n",
    "    # Logging\n",
    "    status_mark = \"\" if model_saved else \"\"\n",
    "    print(f\"FT Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE} {status_mark}\")\n",
    "\n",
    "    if patience_counter_ft >= PATIENCE:\n",
    "        print(\"Early Stopping Triggered! Best FT Epoch: {} with Val F1: {:.4f}\".format(best_ft_epoch, best_val_f1_ft))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ft_better_than_tl:\n",
    "    print(\"Fine-Tuning improved over Transfer Learning. Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1_ft, best_ft_epoch))\n",
    "    SUB_MODEL = 'models/best_model_RetCCLResNet50_MGAP_ft.pt'\n",
    "else:\n",
    "    print(\"Fine-Tuning did not improve over Transfer Learning. Best Validation F1 Score remains: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))\n",
    "    SUB_MODEL = 'models/best_model_RetCCLResNet50_MGAP_tl.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb7a76",
   "metadata": {
    "id": "b2bb7a76"
   },
   "source": [
    "### 13.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc006f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "f8dc006f",
    "outputId": "3c0c6a86-695a-4a21-b45c-a5b9bd0153f3"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ft_history['train_loss'], label='Train Loss')\n",
    "plt.plot(ft_history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ft_history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(ft_history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Fine-Tuned Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_ft_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c2dac",
   "metadata": {
    "id": "808c2dac"
   },
   "source": [
    "### 13.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230087f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "230087f0",
    "outputId": "246d19d8-ea87-4708-de4a-5931cbc85a5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get predictions for the Transfer Learning model (from original `model`)\n",
    "print(\"Generating Confusion Matrix for Transfer Learning Model...\")\n",
    "y_true_tl, y_pred_tl = get_image_predictions(model, val_loader, device)\n",
    "cm_tl = confusion_matrix(y_true_tl, y_pred_tl)\n",
    "f1_tl = f1_score(y_true_tl, y_pred_tl, average='macro')\n",
    "# 2. Get predictions for the Fine-Tuning model (from `ft_model`)\n",
    "print(\"Generating Confusion Matrix for Fine-Tuning Model...\")\n",
    "y_true_ft, y_pred_ft = get_image_predictions(ft_model, val_loader, device)\n",
    "cm_ft = confusion_matrix(y_true_ft, y_pred_ft)\n",
    "f1_ft = f1_score(y_true_ft, y_pred_ft, average='macro')\n",
    "# 3. Plotting side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot Transfer Learning Confusion Matrix\n",
    "sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_title(f'Confusion Matrix (Transfer Learning)\\nF1 Score: {f1_tl:.2f}')\n",
    "\n",
    "# Plot Fine-Tuning Confusion Matrix\n",
    "sns.heatmap(cm_ft, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_title(f'Confusion Matrix (Fine-Tuning)\\nF1 Score: {f1_ft:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710db55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "a710db55",
    "outputId": "88b89cb5-7868-4e30-e496-1da01ee65c19"
   },
   "outputs": [],
   "source": [
    "# Visualize a random validation sample\n",
    "print(\"Transfer Learning:\")\n",
    "sample_id_plot = np.random.choice(val_loader.dataset.df['sample_id'].unique())\n",
    "plot_sample_with_predictions(model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)\n",
    "print(\"Fine Tuning:\")\n",
    "plot_sample_with_predictions(ft_model, val_loader, device, label_encoder, aggregation_method='max_confidence', sample_id=sample_id_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923f870",
   "metadata": {},
   "source": [
    "## **14. Class Activation Maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c870f",
   "metadata": {},
   "source": [
    "### **14.1 Cam Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Grad-CAM Helper Class ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Hooks\n",
    "        # We hook into the target layer to intercept forward and backward passes\n",
    "        self.handle_fwd = self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.handle_bwd = self.target_layer.register_full_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        # Tuple of gradients; we want the first one corresponding to the output\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output)\n",
    "            \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        score = output[0, class_idx]\n",
    "        score.backward()\n",
    "        \n",
    "        # Generate CAM\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        \n",
    "        # b=batch, k=channels, u=height, v=width\n",
    "        b, k, u, v = gradients.size()\n",
    "        \n",
    "        # Global Average Pooling of gradients\n",
    "        alpha = gradients.view(b, k, -1).mean(2)\n",
    "        weights = alpha.view(b, k, 1, 1)\n",
    "        \n",
    "        # Linear combination of activations weighted by alpha\n",
    "        cam = (weights * activations).sum(1, keepdim=True)\n",
    "        cam = F.relu(cam) # Apply ReLU to focus on positive contributions\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam.view(1, -1)\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-7)\n",
    "        cam = cam.view(1, 1, u, v)\n",
    "        \n",
    "        return cam.detach().cpu().numpy()[0, 0], output\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.handle_fwd.remove()\n",
    "        self.handle_bwd.remove()\n",
    "\n",
    "# --- 2. Mask Overlay Function ---\n",
    "def get_mask_overlay(cam_mask, patch_path, masks_dir, img_fallback=None, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Overlays a Class Activation Map (CAM) onto the ground truth mask.\n",
    "    Returns: (overlay_image, mask_found_boolean)\n",
    "    \"\"\"\n",
    "    bg_img = None\n",
    "    mask_found = False\n",
    "\n",
    "    if masks_dir:\n",
    "        # Derive mask filename: img_xxxx.png -> mask_xxxx.png\n",
    "        filename = os.path.basename(patch_path)\n",
    "        mask_filename = filename.replace('img_', 'mask_')\n",
    "        mask_path = os.path.join(masks_dir, mask_filename)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            try:\n",
    "                mask_pil = Image.open(mask_path).convert('L') \n",
    "                mask_pil = mask_pil.resize((224, 224))\n",
    "                mask_np = np.array(mask_pil)\n",
    "                # Normalize and convert to RGB\n",
    "                mask_display = mask_np.astype(np.float32) / 255.0 \n",
    "                bg_img = np.stack([mask_display]*3, axis=-1)\n",
    "                mask_found = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading mask {mask_filename}: {e}\")\n",
    "\n",
    "    # Fallback Logic\n",
    "    if bg_img is None:\n",
    "        if img_fallback is not None:\n",
    "            bg_img = img_fallback\n",
    "        else:\n",
    "            bg_img = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "\n",
    "    # Process Heatmap\n",
    "    heatmap = cv2.resize(cam_mask, (224, 224))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_colored = np.float32(heatmap_colored) / 255\n",
    "\n",
    "    # Overlay\n",
    "    overlay = (alpha * heatmap_colored) + ((1 - alpha) * bg_img)\n",
    "    overlay = overlay / np.max(overlay) \n",
    "    \n",
    "    return overlay, mask_found\n",
    "\n",
    "# --- 3. Main Visualization Function ---\n",
    "def visualize_sample_analysis(model, df_metadata, sample_id, label_encoder, device, masks_dir=None):\n",
    "    \"\"\"\n",
    "    Visualizes analysis for a sample. \n",
    "    Row 1: [Original], [CAM on Img], [CAM on Mask (HIDDEN if missing)], [Stats]\n",
    "    Row 2: Other Classes CAMs (Fallback to Image if Mask missing)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Setup\n",
    "    sample_rows = df_metadata[df_metadata['sample_id'] == sample_id]\n",
    "    if len(sample_rows) == 0:\n",
    "        print(f\"Sample {sample_id} not found.\")\n",
    "        return\n",
    "\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                             std=[1/0.229, 1/0.224, 1/0.225])\n",
    "    ])\n",
    "\n",
    "    # Inference\n",
    "    patch_probs = []\n",
    "    patch_images = []\n",
    "    \n",
    "    # --- CRITICAL UPDATE FOR RESNET18 ---\n",
    "    # ResNet structure: model.backbone.layer4 is the last block of layers\n",
    "    # We select the last basic block [-1] from layer4\n",
    "    target_layer = model.backbone.layer4[-1]\n",
    "    \n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    print(f\"Processing sample {sample_id}...\")\n",
    "    \n",
    "    for _, row in sample_rows.iterrows():\n",
    "        img_pil = Image.open(row['path']).convert('RGB')\n",
    "        img_tensor = transform_pipeline(img_pil).unsqueeze(0).to(device)\n",
    "        patch_images.append({'tensor': img_tensor, 'path': row['path']})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            patch_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    patch_probs = np.vstack(patch_probs)\n",
    "    avg_probs = np.mean(patch_probs, axis=0) \n",
    "    \n",
    "    classes = label_encoder.classes_\n",
    "    pred_class_idx = np.argmax(avg_probs)\n",
    "    pred_label = classes[pred_class_idx]\n",
    "    true_label = sample_rows.iloc[0]['label']\n",
    "    \n",
    "    # Best patch\n",
    "    best_patch_idx = np.argmax(patch_probs[:, pred_class_idx])\n",
    "    best_patch_data = patch_images[best_patch_idx]\n",
    "    img_tensor_active = best_patch_data['tensor'].clone().detach().requires_grad_(True)\n",
    "    patch_path = best_patch_data['path']\n",
    "\n",
    "    # 1. Background Image\n",
    "    img_display = inv_normalize(best_patch_data['tensor'][0]).cpu().detach().numpy()\n",
    "    img_display = np.transpose(img_display, (1, 2, 0))\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "    # 2. Predicted Class CAMs\n",
    "    pred_cam_mask, _ = grad_cam(img_tensor_active, pred_class_idx)\n",
    "    \n",
    "    # A: Overlay on Image\n",
    "    heatmap = cv2.resize(pred_cam_mask, (224, 224))\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_colored = np.float32(heatmap_colored) / 255\n",
    "    cam_on_image = (0.5 * heatmap_colored) + (0.5 * img_display)\n",
    "    cam_on_image = cam_on_image / np.max(cam_on_image)\n",
    "    \n",
    "    # B: Overlay on Mask (Row 1, Col 3)\n",
    "    cam_on_mask_main, mask_found_main = get_mask_overlay(\n",
    "        pred_cam_mask, patch_path, masks_dir, img_fallback=img_display\n",
    "    )\n",
    "\n",
    "    # 3. Other Classes CAMs (Row 2)\n",
    "    cam_others_data = {}\n",
    "    for i in range(len(classes)):\n",
    "        if i == pred_class_idx: continue\n",
    "        mask, _ = grad_cam(img_tensor_active, i)\n",
    "        \n",
    "        # We allow fallback here so we can still see activations for other classes\n",
    "        overlay, is_mask = get_mask_overlay(\n",
    "            mask, patch_path, masks_dir, img_fallback=img_display\n",
    "        )\n",
    "        cam_others_data[i] = (overlay, is_mask)\n",
    "        \n",
    "    grad_cam.remove_hooks() \n",
    "\n",
    "    # --- Plotting ---\n",
    "    other_indices = [i for i in range(len(classes)) if i != pred_class_idx]\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 10))\n",
    "    gs = gridspec.GridSpec(2, max(4, len(other_indices)), height_ratios=[1.2, 0.8])\n",
    "\n",
    "    # Row 1\n",
    "    # 1. Original\n",
    "    ax0 = plt.subplot(gs[0, 0])\n",
    "    ax0.imshow(img_display)\n",
    "    ax0.set_title(f\"Most Representative Patch\\nTrue: {true_label}\", fontsize=12, fontweight='bold')\n",
    "    ax0.axis('off')\n",
    "\n",
    "    # 2. Predicted CAM on Image\n",
    "    ax1 = plt.subplot(gs[0, 1])\n",
    "    ax1.imshow(cam_on_image)\n",
    "    ax1.set_title(f\"Focus on Image: {pred_label}\\n(Score: {avg_probs[pred_class_idx]:.2f})\", fontsize=12, fontweight='bold', color='darkblue')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 3. Predicted CAM on Mask (Conditional)\n",
    "    ax2 = plt.subplot(gs[0, 2])\n",
    "    if mask_found_main:\n",
    "        ax2.imshow(cam_on_mask_main)\n",
    "        ax2.set_title(f\"Focus on Mask: {pred_label}\\n(Ground Truth Overlay)\", fontsize=12, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "    else:\n",
    "        # Deactivate subplot if mask is missing (avoid redundancy with ax1)\n",
    "        ax2.set_visible(False)\n",
    "\n",
    "    # 4. Stats\n",
    "    ax3 = plt.subplot(gs[0, 3])\n",
    "    colors = ['#d3d3d3'] * len(classes)\n",
    "    colors[pred_class_idx] = '#4CAF50' if pred_label == true_label else '#F44336'\n",
    "    \n",
    "    bars = ax3.bar(classes, avg_probs, color=colors, alpha=0.85, edgecolor='black')\n",
    "    ax3.set_title(f\"Bag Prediction Probabilities\\nSample ID: {sample_id}\", fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    ax3.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                 f'{height:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Row 2: Other Classes on MASK (with fallback)\n",
    "    for idx, class_idx in enumerate(other_indices):\n",
    "        ax = plt.subplot(gs[1, idx])\n",
    "        overlay, is_mask = cam_others_data[class_idx]\n",
    "        \n",
    "        ax.imshow(overlay)\n",
    "        class_name = classes[class_idx]\n",
    "        bg_type = \"Mask\" if is_mask else \"Img\"\n",
    "        ax.set_title(f\"Activ. on {bg_type}: {class_name}\\n(Score: {avg_probs[class_idx]:.2f})\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1efec",
   "metadata": {},
   "source": [
    "### **14.2 CAM Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample ID from your validation set\n",
    "if SUB_MODEL is None:\n",
    "    SUB_MODEL = 'models/best_model_resnet18_tl.pt'  # Default to TL model if none specified\n",
    "PATCH_MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\",\"masks\")\n",
    "\n",
    "cam_model = RetCCLResNet50_MGAP(num_classes, \n",
    "                           DROPOUT_RATE, \n",
    "                           ckpt_path=SUB_MODEL,\n",
    "                           freeze_backbone=True).to(device)\n",
    "cam_model.load_state_dict(torch.load(SUB_MODEL), strict=True)\n",
    "cam_idx = random.randint(0, len(df_val['sample_id'].unique()) - 1)\n",
    "sample_id_to_test = df_val['sample_id'].iloc[cam_idx] \n",
    "\n",
    "visualize_sample_analysis(cam_model, patches_metadata_df, sample_id_to_test, label_encoder, device)\n",
    "visualize_sample_analysis(cam_model, patches_metadata_df, sample_id_to_test, label_encoder, device, masks_dir=PATCH_MASKS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06322077",
   "metadata": {
    "id": "06322077"
   },
   "source": [
    "## **14. Submission Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9edaa",
   "metadata": {
    "id": "05e9edaa"
   },
   "source": [
    "### 14.1 Create Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942dad1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0942dad1",
    "outputId": "21f19774-17f8-4bb4-b5cd-d96f459d47ef"
   },
   "outputs": [],
   "source": [
    "sub_model = RetCCLResNet50_MGAP(num_classes, DROPOUT_RATE,ckpt_path=SUB_MODEL, freeze_backbone=True).to(device)\n",
    "\n",
    "sub_model.load_state_dict(torch.load(SUB_MODEL), strict=True)\n",
    "\n",
    "print(f\"Submodel {SUB_MODEL} loaded for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f4093",
   "metadata": {
    "id": "8a4f4093"
   },
   "source": [
    "### 14.2 Define Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd99e8",
   "metadata": {
    "id": "80cd99e8"
   },
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    # No augmentation for validation, just resizing and normalization\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36adb9",
   "metadata": {
    "id": "db36adb9"
   },
   "source": [
    "### 14.3 Function: Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21a0e8",
   "metadata": {
    "id": "9c21a0e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def generate_submission(model, submission_folder, method='max_confidence', output_csv=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Generates a submission file by predicting on patches and aggregating results.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        submission_folder: Path to folder containing test patches.\n",
    "        method: 'majority_voting' or 'max_confidence'.\n",
    "        output_csv: Filename for the output CSV.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Get list of test patches\n",
    "    patch_files = sorted([f for f in os.listdir(submission_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    print(f\"Found {len(patch_files)} patches in {submission_folder}\")\n",
    "    print(f\"Aggregation Method: {method}\")\n",
    "\n",
    "    # Store predictions per image\n",
    "    # Structure: { 'img_0001': {'votes': [], 'probs': []}, ... }\n",
    "    image_predictions = {}\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(patch_files):\n",
    "            filepath = os.path.join(submission_folder, filename)\n",
    "\n",
    "            try:\n",
    "                # Extract Sample ID (e.g., \"img_0015_p12.png\" -> \"img_0015\")\n",
    "                # Adjust split logic if your naming convention is different\n",
    "                sample_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "                # Initialize dictionary for this sample if new\n",
    "                if sample_id not in image_predictions:\n",
    "                    image_predictions[sample_id] = {'probs': []}\n",
    "\n",
    "                # Load and Transform\n",
    "                image = Image.open(filepath).convert('RGB')\n",
    "                input_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                # Predict\n",
    "                outputs = model(input_tensor)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "                # Store probabilities for this patch\n",
    "                image_predictions[sample_id]['probs'].append(probs.cpu().numpy()[0])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    # 3. Aggregate Results\n",
    "    final_results = []\n",
    "\n",
    "    print(f\"Aggregating results for {len(image_predictions)} unique samples...\")\n",
    "\n",
    "    for sample_id, data in image_predictions.items():\n",
    "        all_probs = np.array(data['probs']) # Shape: [Num_Patches, Num_Classes]\n",
    "\n",
    "        # Ensure sample_index has .png extension as requested\n",
    "        sample_index_name = f\"{sample_id}.png\"\n",
    "\n",
    "        if len(all_probs) == 0:\n",
    "            final_results.append({'sample_index': sample_index_name, 'label': \"Luminal A\"}) # Default fallback\n",
    "            continue\n",
    "\n",
    "        if method == 'majority_voting':\n",
    "            # Get class prediction for each patch\n",
    "            patch_preds = np.argmax(all_probs, axis=1)\n",
    "            # Find most frequent class\n",
    "            counts = np.bincount(patch_preds)\n",
    "            final_class_idx = np.argmax(counts)\n",
    "\n",
    "        elif method == 'max_confidence':\n",
    "            # Option A: Average probabilities (Soft Voting) - usually best/safest\n",
    "            avg_probs = np.mean(all_probs, axis=0)\n",
    "            final_class_idx = np.argmax(avg_probs)\n",
    "\n",
    "            # Option B: Strict Max Confidence (Uncomment if you prefer this)\n",
    "            # max_probs = np.max(all_probs, axis=0)\n",
    "            # final_class_idx = np.argmax(max_probs)\n",
    "\n",
    "        # Decode Label\n",
    "        pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
    "        final_results.append({'sample_index': sample_index_name, 'label': pred_label})\n",
    "\n",
    "    # 4. Create Pandas DataFrame and Save\n",
    "    df_submission = pd.DataFrame(final_results)\n",
    "\n",
    "    # Ensure correct column order\n",
    "    df_submission = df_submission[['sample_index', 'label']]\n",
    "\n",
    "    # Sort by sample_index for neatness\n",
    "    df_submission.sort_values('sample_index', inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_submission.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Submission saved to {output_csv}\")\n",
    "    print(df_submission.head())\n",
    "\n",
    "    return df_submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078d160",
   "metadata": {
    "id": "7078d160"
   },
   "source": [
    "### 14.3 Create the Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e05e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "187e05e0",
    "outputId": "f8fb865f-86ae-4157-8cbc-a93efa39be5f"
   },
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# Replace 'path/to/SUBMISSION_PATCHES' with the actual path\n",
    "# If your folder is just named SUBMISSION_PATCHES in current dir:\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "date_time_str = now.strftime(\"%d_%b-%H_%M\")\n",
    "\n",
    "sub_dir = os.path.join(os.path.pardir, \"submission_csvs\")\n",
    "OUTPUT_NAME = os.path.join(sub_dir, f\"submission_ft--{date_time_str}.csv\")\n",
    "\n",
    "\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "# Check if folder exists\n",
    "if os.path.exists(SUBMISSION_PATCHES_OUT):\n",
    "    # Method 1: Max Confidence / Average Probability (Recommended)\n",
    "    df_sub_max_conf = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='max_confidence', output_csv=OUTPUT_NAME)\n",
    "    print(f\"Submission CSV saved to: {OUTPUT_NAME}\")\n",
    "\n",
    "    # Method 2: Majority Voting (Optional, uncomment to run)\n",
    "    # df_sub_majority_voting = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='majority_voting', output_csv=\"submission_voting.csv\")\n",
    "else:\n",
    "    print(f\"Directory '{SUBMISSION_PATCHES_OUT}' not found. Please create it or set the correct path.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
