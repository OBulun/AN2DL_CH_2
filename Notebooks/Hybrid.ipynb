{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0778df6b",
   "metadata": {},
   "source": [
    "# Part 1: Environment, Config, and Data Pipeline\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619816bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive (Colab Only)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IS_COLAB = True\n",
    "    print(\"Colab detected. Drive mounted.\")\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "    print(\"Local environment detected.\")\n",
    "\n",
    "# Install dependencies\n",
    "%pip install transformers torchview torchsummary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d107b",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1024fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import shutil\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# --- Torch & Vision ---\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Foundation Model Support ---\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Plotting Config\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b869b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Adjust these paths to match your specific folder structure\n",
    "if 'IS_COLAB' in globals() and IS_COLAB:\n",
    "    # Assuming standard Drive structure. ADJUST THIS PATH IF NEEDED.\n",
    "    datasets_path = \"/content/drive/MyDrive/AN2DL_CH_2/an2dl2526c2\"\n",
    "    print(f\"Using Colab Path: {datasets_path}\")\n",
    "else:\n",
    "    datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\") \n",
    "    print(f\"Using Local Path: {datasets_path}\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "\n",
    "CSV_PATH = train_labels_path\n",
    "\n",
    "# Output Directories\n",
    "PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\", \"train_patches\")\n",
    "MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\", \"train_patches\")\n",
    "\n",
    "# ImageNet Normalization (Used by Phikon)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "TARGET_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32 # Can increase to 64/128 with AMP on A100\n",
    "\n",
    "print(f\"Patches Directory: {PATCHES_OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe86947",
   "metadata": {},
   "source": [
    "## 3. Metadata Generation & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70766372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Scans the patch directory and links patches to their WSI labels.\n",
    "    \"\"\"\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "    \n",
    "    # Standardize IDs\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "    \n",
    "    # Scan patches\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png') and \"mask\" not in f]\n",
    "    \n",
    "    data = []\n",
    "    print(f\"Parsing {len(patch_files)} patches...\")\n",
    "    \n",
    "    for filename in patch_files:\n",
    "        # Naming convention: img_XXXX_pY.png\n",
    "        try:\n",
    "            if '_p' in filename:\n",
    "                bag_id = filename.rsplit('_p', 1)[0]\n",
    "            else:\n",
    "                bag_id = os.path.splitext(filename)[0]\n",
    "            \n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    df_patches = pd.DataFrame(data)\n",
    "    \n",
    "    # Merge\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col)\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create Metadata\n",
    "if os.path.exists(PATCHES_OUT):\n",
    "    patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "    \n",
    "    # Label Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    # Stratified Split (Image Level, not Patch Level)\n",
    "    unique_samples = patches_metadata_df['sample_id'].unique()\n",
    "    train_ids, val_ids = train_test_split(\n",
    "        unique_samples, test_size=0.2, random_state=SEED, \n",
    "        stratify=patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples]['label']\n",
    "    )\n",
    "    \n",
    "    df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_ids)].reset_index(drop=True)\n",
    "    df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_ids)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Train Patches: {len(df_train)} | Val Patches: {len(df_val)}\")\n",
    "else:\n",
    "    print(\"WARNING: Patches directory not found. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3ac57",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Class (The \"God Mode\" Pipeline)\n",
    "This dataset handles the **Two-Stream** architecture requirements:\n",
    "1.  **Synchronized Augmentations:** Ensures geometric transforms (flips, rotations) happen identically on both the RGB Image and the Mask.\n",
    "2.  **Dual Output:** Returns `(image, mask, label)` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f88076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TissueDataset(Dataset):\n",
    "    def __init__(self, df, img_dir=None, masks_dir=None, augmentation=None, normalize_imagenet=True, target_size=(224, 224), label_col='label_encoded'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing metadata.\n",
    "            img_dir: Root directory for images.\n",
    "            masks_dir: Directory where masks are stored.\n",
    "            augmentation: Boolean (True/False) to enable synchronized augmentation.\n",
    "            normalize_imagenet: Apply ImageNet mean/std (Required for Phikon).\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.masks_location = masks_dir\n",
    "        self.do_augmentation = augmentation\n",
    "        self.normalize_imagenet = normalize_imagenet\n",
    "        self.target_size = target_size\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Normalization for the RGB Stream (Phikon)\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        \n",
    "        # Color Jitter (Applied ONLY to Image, NOT Mask)\n",
    "        self.color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- A. Load Image ---\n",
    "        img_path = row['path']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # --- B. Load Mask ---\n",
    "        if self.masks_location:\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            mask_filename = img_filename.replace('img_', 'mask_') # Logic: img_123 -> mask_123\n",
    "            mask_path = os.path.join(self.masks_location, mask_filename)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "            else:\n",
    "                # Fallback: Black mask\n",
    "                mask = Image.new('L', image.size, 0)\n",
    "        else:\n",
    "            mask = Image.new('L', image.size, 0)\n",
    "\n",
    "        # --- C. Resize ---\n",
    "        image = TF.resize(image, self.target_size)\n",
    "        mask = TF.resize(mask, self.target_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "        # --- D. SYNCHRONIZED AUGMENTATION ---\n",
    "        # Critical for \"God Mode\": The Mask must guide the Image, so they must align perfectly.\n",
    "        if self.do_augmentation:\n",
    "            # 1. Random Horizontal Flip\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "            \n",
    "            # 2. Random Vertical Flip\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "                mask = TF.vflip(mask)\n",
    "            \n",
    "            # 3. Random Rotation (Assumes rotation invariant tissue)\n",
    "            angle = transforms.RandomRotation.get_params(degrees=[-15, 15])\n",
    "            image = TF.rotate(image, angle)\n",
    "            mask = TF.rotate(mask, angle)\n",
    "            \n",
    "            # 4. Color Jitter (Image Only)\n",
    "            image = self.color_jitter(image)\n",
    "\n",
    "        # --- E. Convert to Tensor ---\n",
    "        img_tensor = TF.to_tensor(image)\n",
    "        mask_tensor = TF.to_tensor(mask)\n",
    "\n",
    "        # --- F. Stream-Specific Processing ---\n",
    "        \n",
    "        # Stream 1 (Phikon): Apply ImageNet Normalization\n",
    "        if self.normalize_imagenet:\n",
    "            img_tensor = self.normalize(img_tensor)\n",
    "            \n",
    "        # Stream 2 (Mask): Keep as float (0.0 to 1.0), do NOT normalize with ImageNet stats.\n",
    "        # Ensure it is (1, H, W)\n",
    "        \n",
    "        # --- G. Label ---\n",
    "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
    "\n",
    "        return img_tensor, label, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c1d6b",
   "metadata": {},
   "source": [
    "## 5. Data Loaders & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c612a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Datasets\n",
    "train_dataset = TissueDataset(df_train, masks_dir=MASKS_DIR, augmentation=True, normalize_imagenet=True)\n",
    "val_dataset = TissueDataset(df_val, masks_dir=MASKS_DIR, augmentation=False, normalize_imagenet=True)\n",
    "\n",
    "# Setup Loaders\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train Batches: {len(train_loader)}\")\n",
    "print(f\"Val Batches: {len(val_loader)}\")\n",
    "\n",
    "def show_batch(loader, count=4):\n",
    "    batch = next(iter(loader))\n",
    "    images, labels, masks = batch[0], batch[1], batch[2]\n",
    "    \n",
    "    # De-normalize for visualization\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for i in range(count):\n",
    "        # Plot Image\n",
    "        ax = plt.subplot(2, count, i + 1)\n",
    "        img = images[i] * std + mean\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        plt.imshow(img.permute(1, 2, 0))\n",
    "        plt.title(f\"Label: {labels[i].item()}\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot Mask\n",
    "        ax = plt.subplot(2, count, i + 1 + count)\n",
    "        plt.imshow(masks[i].squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Mask\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cee25",
   "metadata": {},
   "source": [
    "# Part 2: The \"God Mode\" Hybrid Architecture\n",
    "\n",
    "We now construct the Dual-Stream model.\n",
    "*   **Stream 1:** `owkin/phikon` (ViT) for RGB images.\n",
    "*   **Stream 2:** Custom CNN for Binary Masks.\n",
    "*   **Fusion:** Concatenation + MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Pillar 2: The Guide.\n",
    "    A lightweight CNN to extract geometric features from the binary mask.\n",
    "    Input: (B, 1, 224, 224)\n",
    "    Output: (B, 128)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskEncoder, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 112x112\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 56x56\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)) # Global Average Pooling -> 1x1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten: (B, 128)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GodModeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Summit: Phikon + Geometry Fusion.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4, dropout_rate=0.3, freeze_backbone=True):\n",
    "        super(GodModeClassifier, self).__init__()\n",
    "        \n",
    "        # --- Pillar 1: Phikon (Foundation Model) ---\n",
    "        print(\"Loading Phikon (this may take a moment).... \")\n",
    "        # We use owkin/phikon. It maps to a ViT architecture.\n",
    "        self.phikon = AutoModel.from_pretrained(\"owkin/phikon\")\n",
    "        \n",
    "        # Determine Phikon output dimension dynamically (usually 768 for Base, 1024 for Large)\n",
    "        self.phikon_dim = self.phikon.config.hidden_size\n",
    "        print(f\"Phikon Loaded. Embedding Dimension: {self.phikon_dim}\")\n",
    "        \n",
    "        # --- Pillar 2: Mask Encoder ---\n",
    "        self.mask_encoder = MaskEncoder()\n",
    "        self.mask_dim = 128\n",
    "        \n",
    "        # --- The Summit: Fusion Head ---\n",
    "        fusion_dim = self.phikon_dim + self.mask_dim\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for custom heads\n",
    "        self._init_weights(self.mask_encoder)\n",
    "        self._init_weights(self.classifier)\n",
    "        \n",
    "        # Freeze Logic\n",
    "        if freeze_backbone:\n",
    "            self.freeze_phikon()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def freeze_phikon(self):\n",
    "        print(\"Freezing Phikon Backbone...\")\n",
    "        for param in self.phikon.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_phikon_layers(self, n_layers=2):\n",
    "        \"\"\"\n",
    "        Unfreezes the last n_layers of the ViT encoder for fine-tuning.\n",
    "        \"\"\"\n",
    "        print(f\"Unfreezing last {n_layers} layers of Phikon...\")\n",
    "        # Phikon (ViT) structure: phikon.encoder.layer is a ModuleList\n",
    "        # We unlock the last N blocks and the LayerNorms\n",
    "        \n",
    "        # 1. Unfreeze the final LayerNorm (pooler)\n",
    "        for param in self.phikon.pooler.parameters():\n",
    "             param.requires_grad = True\n",
    "        if hasattr(self.phikon, 'layernorm'):\n",
    "             for param in self.phikon.layernorm.parameters():\n",
    "                 param.requires_grad = True\n",
    "\n",
    "        # 2. Unfreeze last N Transformer Blocks\n",
    "        encoder_layers = self.phikon.encoder.layer\n",
    "        for layer in encoder_layers[-n_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        # 1. Pillar 1: Phikon\n",
    "        # Phikon expects (B, 3, 224, 224). Returns object with last_hidden_state.\n",
    "        # We utilize the CLS token (index 0)\n",
    "        phikon_out = self.phikon(pixel_values=img)\n",
    "        # Extract CLS token: (B, Hidden_Dim)\n",
    "        img_features = phikon_out.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 2. Pillar 2: Mask\n",
    "        mask_features = self.mask_encoder(mask)\n",
    "        \n",
    "        # 3. Fusion\n",
    "        combined = torch.cat((img_features, mask_features), dim=1)\n",
    "        \n",
    "        # 4. Classify\n",
    "        logits = self.classifier(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate Model ---\n",
    "# Assuming label_encoder exists from Part 1\n",
    "NUM_CLASSES = len(label_encoder.classes_)\n",
    "\n",
    "model = GodModeClassifier(num_classes=NUM_CLASSES, freeze_backbone=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Stats Helper (from ResNet notebook) ---\n",
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "\n",
    "print_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8079e",
   "metadata": {},
   "source": [
    "## 6. Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# Create dummy inputs on CPU for visualization to save GPU memory\n",
    "model_cpu = GodModeClassifier(num_classes=NUM_CLASSES, freeze_backbone=True).to(\"cpu\")\n",
    "dummy_img = torch.randn(1, 3, 224, 224)\n",
    "dummy_mask = torch.randn(1, 1, 224, 224)\n",
    "\n",
    "model_graph = draw_graph(\n",
    "    model_cpu,\n",
    "    input_data=(dummy_img, dummy_mask),\n",
    "    device='cpu',\n",
    "    expand_nested=True,\n",
    "    depth=2 # Limit depth to avoid massive ViT graph explosion\n",
    ")\n",
    "\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240b2fc",
   "metadata": {},
   "source": [
    "# Part 3: Training Phase 1 (Warmup)\n",
    "\n",
    "## 7. Loss & Optimizer Configuration\n",
    "\n",
    "We use **Focal Loss** to focus on hard-to-classify examples, which is crucial for Histopathology where \"Luminal A\" might dominate the dataset.\n",
    "\n",
    "**Spec:** Gamma=2.0, RAdam Optimizer (LR=1e-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Class Weights (Inverse Frequency)\n",
    "class_counts = df_train['label_encoded'].value_counts().sort_index().values\n",
    "total_samples = sum(class_counts)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "weight_tensor = torch.tensor(\n",
    "    [total_samples / (n_classes * c) for c in class_counts],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "print(f\"Class Weights: {weight_tensor.cpu().numpy()}\")\n",
    "\n",
    "# 2. Define Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha=weight_tensor, gamma=2.0)\n",
    "print(\"Criterion: Focal Loss (Gamma=2.0) with Class Weights enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89724e9",
   "metadata": {},
   "source": [
    "## 8. Training & Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Initialize Scaler for AMP\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for images, labels, masks in loop:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Forward Pass (AMP Enabled) ---\n",
    "        with autocast():\n",
    "            logits = model(images, masks)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # --- Backward Pass (AMP Enabled) ---\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Metrics\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, masks in loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Validate with AMP (optional but good practice for speed)\n",
    "            with autocast():\n",
    "                logits = model(images, masks)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phase 1 Execution: Warmup\n",
    "**Objective:** Train the Fusion Head and the Mask Encoder. The Phikon Backbone is **FROZEN**.\n",
    "This prevents the massive gradients from the uninitialized head from wrecking the pre-trained Phikon weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e56e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Phase 1\n",
    "NUM_EPOCHS_WARMUP = 10\n",
    "LR_WARMUP = 1e-4\n",
    "\n",
    "# Optimizer: RAdam as requested\n",
    "# Note: Phikon params are frozen, so they won't be in model.parameters() where requires_grad=True\n",
    "optimizer = torch.optim.RAdam(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LR_WARMUP,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1 = 0.0\n",
    "# Ensure persistence on Drive\n",
    "MODELS_DIR = os.path.join(datasets_path, \"models\") if 'IS_COLAB' in globals() and IS_COLAB else \"models\"\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "model_path_warmup = os.path.join(MODELS_DIR, \"best_godmode_warmup.pt\")\n",
    "\n",
    "\n",
    "print(\"--- Starting Phase 1: Warmup (Phikon Frozen) ---\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_WARMUP):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # History\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Save Best\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), model_path_warmup)\n",
    "        save_msg = \"[Saved]\"\n",
    "    else:\n",
    "        save_msg = \"\"\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_WARMUP} | Train Loss: {train_loss:.4f} F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} F1: {val_f1:.4f} {save_msg}\")\n",
    "\n",
    "print(f\"Warmup Complete. Best Val F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Warmup History\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Warmup Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_f1'], label='Train F1')\n",
    "plt.plot(history['val_f1'], label='Val F1')\n",
    "plt.title('Warmup F1 Score (Macro)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training Phase 2 (Fine-Tuning)\n",
    "\n",
    "## 10. Unfreezing & Config\n",
    "\n",
    "Now that the Fusion Head is stable, we unlock the **Foundation Model**. \n",
    "We unfreeze the last **2 Transformer Blocks** of Phikon to adapt the high-level texture features to our specific histology dataset.\n",
    "\n",
    "*   **Mixed Precision (AMP):** Enabled (Critical for A100/V100 memory management).\n",
    "*   **LR:** Reduced to `1e-5` to preserve pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Best Warmup Weights\n",
    "print(\"Loading best model from Warmup Phase...\")\n",
    "model.load_state_dict(torch.load(model_path_warmup), strict=True)\n",
    "\n",
    "# 2. Unfreeze Phikon Layers\n",
    "model.unfreeze_phikon_layers(n_layers=2)\n",
    "model.mask_encoder.requires_grad_(True) # Ensure mask encoder is also training\n",
    "\n",
    "# 3. Verify Trainable Parameters\n",
    "print_model_stats(model)\n",
    "\n",
    "# 4. Configuration for Fine-Tuning\n",
    "NUM_EPOCHS_FT = 30\n",
    "LR_FT = 1e-5\n",
    "PATIENCE = 8\n",
    "\n",
    "# Optimizer: Re-initialize for new parameters\n",
    "optimizer_ft = torch.optim.RAdam(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LR_FT,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "model_path_ft = \"models/best_godmode_finetuned.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_amp(model, loader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    loop = tqdm(loader, desc=\"Fine-Tuning\", leave=False)\n",
    "\n",
    "    for images, labels, masks in loop:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Mixed Precision Forward --- \n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(images, masks)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # --- Mixed Precision Backward ---\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Metrics (Disable autocast for simple tensor ops to avoid overhead)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Phase 2: Fine-Tuning (Phikon Partial Unfreeze) ---\")\n",
    "\n",
    "# Reset history for FT phase visualization\n",
    "ft_history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1_ft = best_val_f1 # Carry over best from warmup\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FT):\n",
    "    # Train with AMP\n",
    "    train_loss, train_f1 = train_one_epoch_amp(model, train_loader, criterion, optimizer_ft, device, scaler)\n",
    "    \n",
    "    # Validate (No scaler needed for inference, just standard)\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Scheduler Step\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Update History\n",
    "    ft_history['train_loss'].append(train_loss)\n",
    "    ft_history['train_f1'].append(train_f1)\n",
    "    ft_history['val_loss'].append(val_loss)\n",
    "    ft_history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Checkpointing & Early Stopping\n",
    "    if val_f1 > best_val_f1_ft:\n",
    "        best_val_f1_ft = val_f1\n",
    "        torch.save(model.state_dict(), model_path_ft)\n",
    "        patience_counter = 0\n",
    "        save_msg = \"[Saved Best FT]\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        save_msg = \"\"\n",
    "        \n",
    "    print(f\"FT Epoch {epoch+1}/{NUM_EPOCHS_FT} | Train Loss: {train_loss:.4f} F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} F1: {val_f1:.4f} {save_msg}\")\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"Early Stopping Triggered.\")\n",
    "        break\n",
    "\n",
    "print(f\"Fine-Tuning Complete. Best Val F1: {best_val_f1_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Combine histories for visualization\n",
    "full_train_loss = history['train_loss'] + ft_history['train_loss']\n",
    "full_val_loss = history['val_loss'] + ft_history['val_loss']\n",
    "full_train_f1 = history['train_f1'] + ft_history['train_f1']\n",
    "full_val_f1 = history['val_f1'] + ft_history['val_f1']\n",
    "\n",
    "# Vertical line indicating phase switch\n",
    "phase_switch = len(history['train_loss']) - 1\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(full_train_loss, label='Train Loss')\n",
    "plt.plot(full_val_loss, label='Val Loss')\n",
    "plt.axvline(x=phase_switch, color='r', linestyle='--', label='Start Fine-Tuning')\n",
    "plt.title('Total Loss History')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(full_train_f1, label='Train F1')\n",
    "plt.plot(full_val_f1, label='Val F1')\n",
    "plt.axvline(x=phase_switch, color='r', linestyle='--', label='Start Fine-Tuning')\n",
    "plt.title('Total F1 Score History')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Evaluation and Submission\n",
    "\n",
    "## 12. Evaluation (Confusion Matrix)\n",
    "We evaluate the model on the Validation set to see per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Best Model (Fine-Tuned)\n",
    "final_model = GodModeClassifier(num_classes=NUM_CLASSES, freeze_backbone=False).to(device)\n",
    "final_model.load_state_dict(torch.load(model_path_ft), strict=True)\n",
    "final_model.eval()\n",
    "print(\"Final Model Loaded.\")\n",
    "\n",
    "# 2. Get Predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"Generating predictions for Confusion Matrix...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels, masks in tqdm(val_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        logits = final_model(images, masks)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "# 3. Plot\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Patch Level)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Submission Generation\n",
    "\n",
    "**Aggregation Strategy:** `Max Mean Confidence`.\n",
    "1.  We collect all patch probabilities for a Bag (WSI).\n",
    "2.  We calculate the **Mean** probability for each class across all patches.\n",
    "3.  The class with the highest Mean probability is the WSI label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Output configs\n",
    "TEST_PATCHES_DIR = os.path.join(datasets_path, \"preprocessing_results\", \"test_patches\")\n",
    "TEST_MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\", \"test_patches\") # Assuming same struct\n",
    "SUBMISSION_DIR = os.path.join(os.path.pardir, \"submission_csvs\")\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "def generate_submission(model, submission_folder, masks_folder=None):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Scan Test Patches\n",
    "    if not os.path.exists(submission_folder):\n",
    "        print(f\"Test folder not found: {submission_folder}\")\n",
    "        return\n",
    "        \n",
    "    patch_files = sorted([f for f in os.listdir(submission_folder) if f.lower().endswith('.png') and \"mask\" not in f])\n",
    "    print(f\"Found {len(patch_files)} test patches.\")\n",
    "    \n",
    "    # 2. Setup Transforms (Standard Phikon normalization)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    \n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # 3. Inference Loop\n",
    "    image_predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(patch_files):\n",
    "            filepath = os.path.join(submission_folder, filename)\n",
    "            \n",
    "            # Extract Sample ID\n",
    "            if '_p' in filename:\n",
    "                sample_id = filename.rsplit('_p', 1)[0]\n",
    "            else:\n",
    "                sample_id = os.path.splitext(filename)[0]\n",
    "                \n",
    "            if sample_id not in image_predictions:\n",
    "                image_predictions[sample_id] = []\n",
    "            \n",
    "            # A. Load Image\n",
    "            try:\n",
    "                image = Image.open(filepath).convert('RGB')\n",
    "                img_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "                \n",
    "                # B. Load Mask (Robust Fallback)\n",
    "                mask_tensor = None\n",
    "                if masks_folder:\n",
    "                    mask_name = filename.replace('img_', 'mask_')\n",
    "                    mask_path = os.path.join(masks_folder, mask_name)\n",
    "                    if os.path.exists(mask_path):\n",
    "                        mask = Image.open(mask_path).convert('L')\n",
    "                        mask_tensor = mask_transform(mask).unsqueeze(0).to(device)\n",
    "                \n",
    "                if mask_tensor is None:\n",
    "                    # Black mask fallback\n",
    "                    mask_tensor = torch.zeros((1, 1, TARGET_SIZE[0], TARGET_SIZE[1])).to(device)\n",
    "                \n",
    "                # C. Predict\n",
    "                logits = model(img_tensor, mask_tensor)\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "                image_predictions[sample_id].append(probs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # 4. Aggregation (Max Mean Confidence)\n",
    "    final_results = []\n",
    "    print(\"Aggregating results...\")\n",
    "    \n",
    "    for sample_id, probs_list in image_predictions.items():\n",
    "        if len(probs_list) == 0:\n",
    "            # Fallback for empty bags (rare)\n",
    "            pred_label = label_encoder.classes_[0]\n",
    "        else:\n",
    "            probs_array = np.array(probs_list)\n",
    "            # Mean probability across all patches for each class\n",
    "            avg_probs = np.mean(probs_array, axis=0)\n",
    "            final_class_idx = np.argmax(avg_probs)\n",
    "            pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
    "            \n",
    "        final_results.append({\n",
    "            'sample_index': f\"{sample_id}.png\", # Format requirement from sample_submission\n",
    "            'label': pred_label\n",
    "        })\n",
    "        \n",
    "    # 5. Save\n",
    "    df_submission = pd.DataFrame(final_results)\n",
    "    df_submission = df_submission.sort_values('sample_index')\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%d_%b-%H_%M\")\n",
    "    out_path = os.path.join(SUBMISSION_DIR, f\"submission_godmode_{timestamp}.csv\")\n",
    "    \n",
    "    df_submission.to_csv(out_path, index=False)\n",
    "    print(f\"Submission saved to: {out_path}\")\n",
    "    return df_submission\n",
    "\n",
    "# Run Generation (Ensure TEST_PATCHES_DIR is correct)\n",
    "if os.path.exists(TEST_PATCHES_DIR):\n",
    "    generate_submission(final_model, TEST_PATCHES_DIR, TEST_MASKS_DIR)\n",
    "else:\n",
    "    print(\"Test patches directory not found. Skipping submission generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Explainability (God Mode CAM)\n",
    "\n",
    "## 14. ViT-Compatible Class Activation Maps\n",
    "\n",
    "We need a custom CAM implementation because our model has two distinct pillars with different architectures:\n",
    "1.  **Phikon (ViT):** Requires reshaping sequence tokens `(B, 197, 768)` -> `(B, 14, 14, 768)` to get a spatial map.\n",
    "2.  **Mask Encoder (CNN):** Uses standard spatial feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "class GodModeCAM:\n",
    "    def __init__(self, model, target_layer_phikon, target_layer_mask):\n",
    "        self.model = model\n",
    "        self.gradients = {\"phikon\": None, \"mask\": None}\n",
    "        self.activations = {\"phikon\": None, \"mask\": None}\n",
    "        \n",
    "        # Register Hooks\n",
    "        # Phikon Hook\n",
    "        target_layer_phikon.register_forward_hook(self.save_activation_phikon)\n",
    "        target_layer_phikon.register_full_backward_hook(self.save_gradient_phikon)\n",
    "        \n",
    "        # Mask Hook\n",
    "        target_layer_mask.register_forward_hook(self.save_activation_mask)\n",
    "        target_layer_mask.register_full_backward_hook(self.save_gradient_mask)\n",
    "\n",
    "    def save_activation_phikon(self, module, input, output):\n",
    "        # ViT Output is usually a tuple, index 0 is hidden state\n",
    "        if isinstance(output, tuple):\n",
    "            self.activations[\"phikon\"] = output[0]\n",
    "        else:\n",
    "            self.activations[\"phikon\"] = output\n",
    "\n",
    "    def save_gradient_phikon(self, module, grad_input, grad_output):\n",
    "        self.gradients[\"phikon\"] = grad_output[0]\n",
    "\n",
    "    def save_activation_mask(self, module, input, output):\n",
    "        self.activations[\"mask\"] = output\n",
    "\n",
    "    def save_gradient_mask(self, module, grad_input, grad_output):\n",
    "        self.gradients[\"mask\"] = grad_output[0]\n",
    "\n",
    "    def generate_cam(self, img_tensor, mask_tensor, class_idx=None):\n",
    "        # 1. Forward Pass\n",
    "        self.model.zero_grad()\n",
    "        logits = self.model(img_tensor, mask_tensor)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(logits)\n",
    "            \n",
    "        # 2. Backward Pass\n",
    "        score = logits[0, class_idx]\n",
    "        score.backward()\n",
    "        \n",
    "        # --- Process Phikon CAM (ViT Reshaping) ---\n",
    "        grads_phi = self.gradients[\"phikon\"]\n",
    "        acts_phi = self.activations[\"phikon\"]\n",
    "        \n",
    "        # Remove CLS token (index 0), keep spatial tokens (1-196)\n",
    "        # Shape: (B, 197, 768) -> (B, 196, 768)\n",
    "        grads_phi = grads_phi[:, 1:, :]\n",
    "        acts_phi = acts_phi[:, 1:, :]\n",
    "        \n",
    "        # Weighted combination\n",
    "        weights_phi = torch.mean(grads_phi, dim=1, keepdim=True)\n",
    "        cam_phi = torch.sum(weights_phi * acts_phi, dim=2)\n",
    "        \n",
    "        # Reshape to square (14x14 for ViT-Base/Large)\n",
    "        side = int(math.sqrt(cam_phi.shape[1])) # Should be 14\n",
    "        cam_phi = cam_phi.view(1, 1, side, side)\n",
    "        \n",
    "        # Upsample to 224x224\n",
    "        cam_phi = F.interpolate(cam_phi, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        cam_phi = F.relu(cam_phi)\n",
    "\n",
    "        # --- Process Mask CAM (Standard CNN) ---\n",
    "        grads_mask = self.gradients[\"mask\"]\n",
    "        acts_mask = self.activations[\"mask\"]\n",
    "        \n",
    "        weights_mask = torch.mean(grads_mask, dim=(2, 3), keepdim=True)\n",
    "        cam_mask = torch.sum(weights_mask * acts_mask, dim=1, keepdim=True)\n",
    "        cam_mask = F.interpolate(cam_mask, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        cam_mask = F.relu(cam_mask)\n",
    "\n",
    "        return cam_phi.detach().cpu().numpy()[0, 0], cam_mask.detach().cpu().numpy()[0, 0], logits, class_idx\n",
    "\n",
    "# Setup CAM Hooks\n",
    "# 1. Target last layer of Phikon Encoder (The LayerNorm before pooler or last block output)\n",
    "# Structure: phikon.encoder.layer[-1].output\n",
    "target_phikon = final_model.phikon.encoder.layer[-1].output\n",
    "\n",
    "# 2. Target last conv layer of Mask Encoder\n",
    "# Structure: mask_encoder.features[6] (The Conv2d(64, 128))\n",
    "target_mask = final_model.mask_encoder.features[6]\n",
    "\n",
    "god_cam = GodModeCAM(final_model, target_phikon, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_god_mode_analysis(dataset, model, cam_engine, count=3):\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), count, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        # Load Data\n",
    "        img_tensor, label, mask_tensor = dataset[idx]\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).to(device)\n",
    "        true_label = label_encoder.inverse_transform([label.item()])[0]\n",
    "        \n",
    "        # Generate CAMs\n",
    "        cam_phi, cam_mask, logits, pred_idx = cam_engine.generate_cam(img_tensor, mask_tensor)\n",
    "        pred_label = label_encoder.inverse_transform([pred_idx.item()])[0]\n",
    "        \n",
    "        # Prepare for Plotting\n",
    "        # Denormalize Image\n",
    "        mean = np.array(IMAGENET_MEAN).reshape(1, 1, 3)\n",
    "        std = np.array(IMAGENET_STD).reshape(1, 1, 3)\n",
    "        img_np = img_tensor.cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "        img_np = np.clip(img_np * std + mean, 0, 1)\n",
    "        \n",
    "        # Normalize CAMs (0-1)\n",
    "        cam_phi = (cam_phi - cam_phi.min()) / (cam_phi.max() + 1e-7)\n",
    "        cam_mask = (cam_mask - cam_mask.min()) / (cam_mask.max() + 1e-7)\n",
    "        \n",
    "        # Create Heatmaps\n",
    "        heatmap_phi = cv2.applyColorMap(np.uint8(255 * cam_phi), cv2.COLORMAP_JET)\n",
    "        heatmap_phi = cv2.cvtColor(heatmap_phi, cv2.COLOR_BGR2RGB)\n",
    "        overlay_phi = np.uint8(255 * img_np) * 0.5 + heatmap_phi * 0.5\n",
    "        overlay_phi = overlay_phi / 255.0\n",
    "        \n",
    "        # Plot\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        gs = gridspec.GridSpec(1, 5, width_ratios=[1, 1, 1, 1, 0.2])\n",
    "        \n",
    "        # 1. Original Image\n",
    "        ax0 = plt.subplot(gs[0])\n",
    "        ax0.imshow(img_np)\n",
    "        ax0.set_title(f\"Original\\nTrue: {true_label}\")\n",
    "        ax0.axis('off')\n",
    "        \n",
    "        # 2. Mask Input\n",
    "        ax1 = plt.subplot(gs[1])\n",
    "        ax1.imshow(mask_tensor.cpu().squeeze(), cmap='gray')\n",
    "        ax1.set_title(\"Input Mask\")\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # 3. Phikon Focus (Where the Foundation Model looked)\n",
    "        ax2 = plt.subplot(gs[2])\n",
    "        ax2.imshow(overlay_phi)\n",
    "        ax2.set_title(f\"Phikon Attention\\n(Texture Focus)\")\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # 4. Mask Focus (Where the CNN looked)\n",
    "        ax3 = plt.subplot(gs[3])\n",
    "        ax3.imshow(cam_mask, cmap='jet')\n",
    "        ax3.set_title(f\"Geometry Attention\\n(Shape Focus)\")\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # 5. Prediction Bar\n",
    "        ax4 = plt.subplot(gs[4])\n",
    "        probs = torch.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "        colors = ['gray'] * len(probs)\n",
    "        colors[pred_idx] = 'green' if pred_idx == label else 'red'\n",
    "        ax4.barh(label_encoder.classes_, probs, color=colors)\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_title(f\"Pred: {pred_label}\")\n",
    "        ax4.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Visualizing God Mode Inference Analysis on Validation Set...\")\n",
    "visualize_god_mode_analysis(val_dataset, final_model, god_cam, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Notebook\n",
    "**Summary:**\n",
    "1.  **Foundation:** Uses `owkin/phikon` (ViT-Large) for SOTA texture extraction.\n",
    "2.  **Guidance:** Uses a parallel CNN to process binary masks explicitly.\n",
    "3.  **Training:** Two-stage process (Warmup -> Fine-Tuning with AMP).\n",
    "4.  **Explainability:** Custom ViT-Reshape CAM to visualize Foundation Model attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 (Optional): Test Time Augmentation (TTA)\n",
    "\n",
    "To squeeze the final drops of performance out of the model, we use TTA.\n",
    "Instead of predicting on just the image, we predict on:\n",
    "1.  Original\n",
    "2.  Horizontal Flip\n",
    "3.  Vertical Flip\n",
    "\n",
    "We average the probabilities before making the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tta(model, img_tensor, mask_tensor):\n",
    "    \"\"\"\n",
    "    Predicts using Original, H-Flip, and V-Flip.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # List of augmentations (Original is identity)\n",
    "    # Note: mask must be augmented identically to image!\n",
    "    augments = [\n",
    "        lambda x, m: (x, m),                                      # Original\n",
    "        lambda x, m: (TF.hflip(x), TF.hflip(m)),                  # H-Flip\n",
    "        lambda x, m: (TF.vflip(x), TF.vflip(m)),                  # V-Flip\n",
    "    ]\n",
    "    \n",
    "    probs_sum = None\n",
    "    \n",
    "    for aug_func in augments:\n",
    "        img_aug, mask_aug = aug_func(img_tensor, mask_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(img_aug, mask_aug)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            \n",
    "        if probs_sum is None:\n",
    "            probs_sum = probs\n",
    "        else:\n",
    "            probs_sum += probs\n",
    "            \n",
    "    # Average\n",
    "    return (probs_sum / len(augments)).cpu().numpy()[0]\n",
    "\n",
    "def generate_submission_tta(model, submission_folder, masks_folder=None):\n",
    "    print(\"--- Generating Submission with TTA (3x Slower, Better Accuracy) ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    patch_files = sorted([f for f in os.listdir(submission_folder) if f.lower().endswith('.png') and \"mask\" not in f])\n",
    "    \n",
    "    # Transforms\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    \n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    image_predictions = {}\n",
    "    \n",
    "    for filename in tqdm(patch_files):\n",
    "        filepath = os.path.join(submission_folder, filename)\n",
    "        \n",
    "        if '_p' in filename:\n",
    "            sample_id = filename.rsplit('_p', 1)[0]\n",
    "        else:\n",
    "            sample_id = os.path.splitext(filename)[0]\n",
    "            \n",
    "        if sample_id not in image_predictions:\n",
    "            image_predictions[sample_id] = []\n",
    "        \n",
    "        try:\n",
    "            # Load & Prep\n",
    "            image = Image.open(filepath).convert('RGB')\n",
    "            img_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            mask_tensor = None\n",
    "            if masks_folder:\n",
    "                mask_name = filename.replace('img_', 'mask_')\n",
    "                mask_path = os.path.join(masks_folder, mask_name)\n",
    "                if os.path.exists(mask_path):\n",
    "                    mask = Image.open(mask_path).convert('L')\n",
    "                    mask_tensor = mask_transform(mask).unsqueeze(0).to(device)\n",
    "            \n",
    "            if mask_tensor is None:\n",
    "                mask_tensor = torch.zeros((1, 1, TARGET_SIZE[0], TARGET_SIZE[1])).to(device)\n",
    "            \n",
    "            # Predict with TTA\n",
    "            probs = predict_with_tta(model, img_tensor, mask_tensor)\n",
    "            image_predictions[sample_id].append(probs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Aggregate\n",
    "    final_results = []\n",
    "    for sample_id, probs_list in image_predictions.items():\n",
    "        if len(probs_list) == 0:\n",
    "            pred_label = label_encoder.classes_[0]\n",
    "        else:\n",
    "            probs_array = np.array(probs_list)\n",
    "            avg_probs = np.mean(probs_array, axis=0)\n",
    "            final_class_idx = np.argmax(avg_probs)\n",
    "            pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
    "            \n",
    "        final_results.append({'sample_index': f\"{sample_id}.png\", 'label': pred_label})\n",
    "        \n",
    "    df_submission = pd.DataFrame(final_results)\n",
    "    df_submission = df_submission.sort_values('sample_index')\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%d_%b-%H_%M\")\n",
    "    out_path = os.path.join(SUBMISSION_DIR, f\"submission_godmode_TTA_{timestamp}.csv\")\n",
    "    \n",
    "    df_submission.to_csv(out_path, index=False)\n",
    "    print(f\"TTA Submission saved to: {out_path}\")\n",
    "\n",
    "# Uncomment to run TTA Submission\n",
    "# if os.path.exists(TEST_PATCHES_DIR):\n",
    "#     generate_submission_tta(final_model, TEST_PATCHES_DIR, TEST_MASKS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
