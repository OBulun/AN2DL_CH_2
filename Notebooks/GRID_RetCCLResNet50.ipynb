{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3293c812",
   "metadata": {
    "id": "3293c812"
   },
   "source": [
    "## **1. Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee15c7",
   "metadata": {
    "id": "edee15c7"
   },
   "source": [
    "## **2. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c7403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "963c7403",
    "outputId": "b6727f0e-41f8-4a6b-dea0-f74d54e6a861"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from optuna.samplers import TPESampler\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import other libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ee8",
   "metadata": {
    "id": "3b5b1ee8"
   },
   "source": [
    "## **3. Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d674a2d",
   "metadata": {
    "id": "7d674a2d"
   },
   "outputs": [],
   "source": [
    "USE_MASKED_PATCHES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3da911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df3da911",
    "outputId": "8f0365f9-913a-4b45-9039-d4b26ac59c53"
   },
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path\n",
    "\n",
    "if USE_MASKED_PATCHES:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"train_patches_masked\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"submission_patches_masked\")\n",
    "else:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\")\n",
    "\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "print(f\"Test data path: {test_data_path}\")\n",
    "print(f\"Patches output path: {PATCHES_OUT}\")\n",
    "print(f\"Submission patches output path: {SUBMISSION_PATCHES_OUT}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427b09e",
   "metadata": {
    "id": "e427b09e"
   },
   "source": [
    "## **4. Train/Val Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7455a",
   "metadata": {
    "id": "7cc7455a"
   },
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
    "    \"\"\"\n",
    "    # 1. Load the labels CSV\n",
    "    # Assuming CSV structure: [image_id, label] or similar\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "\n",
    "    # Standardize column names for easier merging\n",
    "    # We assume the first column is the ID and the second is the Label\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "\n",
    "    # Ensure IDs in CSV are strings (to match filenames)\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "\n",
    "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
    "    # because our parsed Bag IDs won't have them.\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # 2. List all patch files\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
    "\n",
    "    # 3. Parse filenames to get Bag IDs\n",
    "    data = []\n",
    "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
    "\n",
    "    for filename in patch_files:\n",
    "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
    "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
    "\n",
    "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
    "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
    "        try:\n",
    "            bag_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(f\"Skipping malformed filename: {filename}\")\n",
    "\n",
    "    # Create temporary patches DataFrame\n",
    "    df_patches = pd.DataFrame(data)\n",
    "\n",
    "    # 4. Merge patches with labels\n",
    "    # This assigns the correct Bag Label to every Patch in that Bag\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
    "\n",
    "    # 5. Clean up and Rename\n",
    "    # Keep only required columns\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "\n",
    "    # Rename label column to standard 'label' if it isn't already\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "\n",
    "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b8e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633b8e05",
    "outputId": "7d671b8b-3e81-446b-d478-67847824bf41"
   },
   "outputs": [],
   "source": [
    "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(patches_metadata_df.head().drop(columns=['path']))\n",
    "print(\"\\nPatches per Bag (Distribution):\")\n",
    "print(patches_metadata_df['sample_id'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6c6cd3",
    "outputId": "be243fc3-59b3-4318-ed4b-09eb2ce21d3d"
   },
   "outputs": [],
   "source": [
    "# Add Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "\n",
    "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"  {orig} -> {enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52314ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52314ae9",
    "outputId": "ce81e6f4-f3fe-4311-ebd2-11946f50a08e"
   },
   "outputs": [],
   "source": [
    "# Train/Val Split on Original Images (not patches)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train/Val Split on Original Images\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get unique sample IDs\n",
    "unique_samples = patches_metadata_df['sample_id'].unique()\n",
    "print(f\"\\nTotal unique samples (original images): {len(unique_samples)}\")\n",
    "\n",
    "# Split samples into train (80%) and val (20%)\n",
    "train_samples, val_samples = train_test_split(\n",
    "    unique_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples, 'label_encoded'].values\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples: {len(val_samples)}\")\n",
    "\n",
    "# Create train and val DataFrames by filtering patches\n",
    "df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_samples)].reset_index(drop=True)\n",
    "df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_samples)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain patches: {len(df_train)}\")\n",
    "print(f\"Val patches: {len(df_val)}\")\n",
    "print(f\"\\nTrain label distribution:\\n{df_train['label'].value_counts()}\")\n",
    "print(f\"\\nVal label distribution:\\n{df_val['label'].value_counts()}\")\n",
    "\n",
    "# Print percentage distribution\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Percentage Distribution\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTrain label percentage:\\n{df_train['label'].value_counts(normalize=True) * 100}\")\n",
    "print(f\"\\nVal label percentage:\\n{df_val['label'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b797aff",
   "metadata": {
    "id": "4b797aff"
   },
   "source": [
    "## **5. Transformations & Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94459d26",
   "metadata": {
    "id": "94459d26"
   },
   "outputs": [],
   "source": [
    "# Define augmentation for training with enhanced transformations\n",
    "train_augmentation = transforms.Compose([\n",
    "    # Geometric transformations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Small rotations to handle orientation variations\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),  # Reduced from 0.2 for more conservative shifts\n",
    "        scale=None,  # Add scale variation\n",
    "        shear=10  # Add shear transformation\n",
    "    ),\n",
    "\n",
    "    # Color/appearance transformations\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,  # Adjust brightness\n",
    "        contrast=0.2,    # Adjust contrast\n",
    "        saturation=0.2,  # Adjust saturation\n",
    "        hue=0.1          # Slight hue variation\n",
    "    ),\n",
    "    #transforms.RandomGrayscale(p=0.1),  # Occasionally convert to grayscale to improve robustness\n",
    "\n",
    "    # Occlusion simulation\n",
    "    #transforms.RandomErasing(\n",
    "    #    p=0.3,  # Reduced probability for more balanced augmentation\n",
    "    #    scale=(0.02, 0.15),  # Reduced max scale\n",
    "    #    ratio=(0.3, 3.3)  # Aspect ratio range\n",
    "    #),\n",
    "\n",
    "    # Optional: Add Gaussian blur for noise robustness\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cdd92",
   "metadata": {
    "id": "dc8cdd92"
   },
   "source": [
    "## **6. Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b69bba",
   "metadata": {
    "id": "29b69bba"
   },
   "outputs": [],
   "source": [
    "# ImageNet normalization statistics\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class TissueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, augmentation=None, normalize_imagenet=False, cache_images=True):\n",
    "        self.augmentation = augmentation\n",
    "        self.normalize_imagenet = normalize_imagenet\n",
    "        self.df = df\n",
    "        \n",
    "        # CRITICAL OPTIMIZATION: Pre-convert to lists\n",
    "        self.paths = df['path'].tolist()\n",
    "        self.labels = df['label_encoded'].tolist()\n",
    "        \n",
    "        # Define transforms\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.Resize(TARGET_SIZE),\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "        \n",
    "        if normalize_imagenet:\n",
    "            self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        else:\n",
    "            self.normalize = None\n",
    "        \n",
    "        # For external use\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(TARGET_SIZE),\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) if normalize_imagenet else transforms.Identity()\n",
    "        ])\n",
    "        \n",
    "        # --- IMAGE CACHING (Pre-load all images) ---\n",
    "        self.image_cache = {}\n",
    "        if cache_images:\n",
    "            self._preload_images()\n",
    "    \n",
    "    def _preload_images(self):\n",
    "        \"\"\"Pre-load all images into memory for O(1) access during training.\"\"\"\n",
    "        total_images = len(self.paths)\n",
    "        \n",
    "        with tqdm(total=total_images, desc=\"Pre-loading images\", unit=\"img\") as pbar:\n",
    "            for img_path in self.paths:\n",
    "                if img_path not in self.image_cache:\n",
    "                    try:\n",
    "                        image = Image.open(img_path).convert(\"RGB\")\n",
    "                        image = self.to_tensor(image)\n",
    "                        self.image_cache[img_path] = image\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading image {img_path}: {e}\")\n",
    "                        self.image_cache[img_path] = None\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        print(f\"Successfully cached {len(self.image_cache)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image (from cache or disk)\n",
    "        if img_path in self.image_cache:\n",
    "            image = self.image_cache[img_path]\n",
    "            if image is None:\n",
    "                # Fallback for corrupted cached images\n",
    "                image = torch.ones((3, TARGET_SIZE[0], TARGET_SIZE[1]), dtype=torch.float32) * 0.5\n",
    "        else:\n",
    "            # Fallback: Load on-the-fly if not cached\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                image = self.to_tensor(img)\n",
    "            except:\n",
    "                image = torch.ones((3, TARGET_SIZE[0], TARGET_SIZE[1]), dtype=torch.float32) * 0.5\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augmentation:\n",
    "            image = self.augmentation(image)\n",
    "        \n",
    "        # Apply normalization\n",
    "        if self.normalize:\n",
    "            image = self.normalize(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4cb5a",
   "metadata": {
    "id": "ffa4cb5a"
   },
   "source": [
    "## **7. Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a923ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL: \n",
    "    num_workers = 0\n",
    "    CACHE_IMAGES = False\n",
    "else:\n",
    "    num_workers = os.cpu_count()//2\n",
    "    CACHE_IMAGES = True\n",
    "\n",
    "# Instantiate Datasets\n",
    "train_dataset = TissueDataset(\n",
    "    df_train, \n",
    "    augmentation=train_augmentation, \n",
    "    normalize_imagenet=True,\n",
    "    cache_images=CACHE_IMAGES  # Enable image pre-loading\n",
    ")\n",
    "val_dataset = TissueDataset(\n",
    "    df_val, \n",
    "    augmentation=None, \n",
    "    normalize_imagenet=True,\n",
    "    cache_images=CACHE_IMAGES  # Enable image pre-loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d978b",
   "metadata": {
    "id": "4f1d978b"
   },
   "source": [
    "## **9. Model Definition (Transfer Learning - MobileNetV3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4ec0",
   "metadata": {
    "id": "5c6c4ec0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# IMPORTANT: this is RetCCL's ResNet implementation (copy ResNet.py from the RetCCL repo)\n",
    "import ResNet as RetCCLResNet\n",
    "\n",
    "\n",
    "def _clean_state_dict(sd: dict) -> dict:\n",
    "    \"\"\"Strip common prefixes (DataParallel, wrapper modules).\"\"\"\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        for p in (\"module.\", \"model.\", \"encoder.\", \"backbone.\"):\n",
    "            if k.startswith(p):\n",
    "                k = k[len(p):]\n",
    "        out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "class RetCCLResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for your ResNet18 class, but using RetCCL (CNN) ResNet50 backbone.\n",
    "\n",
    "    Args match your original:\n",
    "      - num_classes\n",
    "      - dropout_rate\n",
    "      - freeze_backbone\n",
    "\n",
    "    Extra:\n",
    "      - ckpt_path: path to RetCCL checkpoint (e.g., best_ckpt.pth)\n",
    "      - unfreeze_last_block: often helps on small datasets\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        dropout_rate: float = 0.2,\n",
    "        freeze_backbone: bool = True,\n",
    "        ckpt_path: str = \"best_ckpt.pth\",\n",
    "        unfreeze_last_block: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Build RetCCL ResNet50 (their script uses num_classes=128 for the pretext head)\n",
    "        self.backbone = RetCCLResNet.resnet50(\n",
    "            num_classes=128, mlp=False, two_branch=False, normlinear=True\n",
    "        )\n",
    "\n",
    "        # 2) Load RetCCL pretrained weights\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        if isinstance(sd, dict) and \"state_dict\" in sd:\n",
    "            sd = sd[\"state_dict\"]\n",
    "        sd = _clean_state_dict(sd)\n",
    "\n",
    "        # Drop any fc keys from the checkpoint (we replace the head anyway)\n",
    "        sd = {k: v for k, v in sd.items() if not k.startswith(\"fc.\")}\n",
    "\n",
    "        msg = self.backbone.load_state_dict(sd, strict=False)\n",
    "        # Uncomment for debugging:\n",
    "        # print(\"Missing keys:\", msg.missing_keys)\n",
    "        # print(\"Unexpected keys:\", msg.unexpected_keys)\n",
    "\n",
    "        # 3) Replace fc with your custom multi-layer head\n",
    "        # RetCCL fc may not expose .in_features (e.g., NormLinear), so use weight shape\n",
    "        if hasattr(self.backbone.fc, \"in_features\"):\n",
    "            in_features = self.backbone.fc.in_features\n",
    "        elif hasattr(self.backbone.fc, \"weight\"):\n",
    "            in_features = self.backbone.fc.weight.shape[1]\n",
    "        else:\n",
    "            # ResNet50 default is 2048 if all else fails\n",
    "            in_features = 2048\n",
    "\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "        # 4) Freeze backbone (optional) + optionally unfreeze last block\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # always train the new head\n",
    "            for p in self.backbone.fc.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # often beneficial for small pathology datasets\n",
    "            if unfreeze_last_block and hasattr(self.backbone, \"layer4\"):\n",
    "                for p in self.backbone.layer4.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee73770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import ResNet as RetCCLResNet # Ensure this matches your import\n",
    "\n",
    "class RetCCLResNet50_Flexible(RetCCLResNet50):\n",
    "    \"\"\"\n",
    "    Extends your original class to support dynamic MLP heads for Grid Search.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        dropout_rate: float = 0.2,\n",
    "        freeze_backbone: bool = True,\n",
    "        ckpt_path: str = \"best_ckpt.pth\",\n",
    "        unfreeze_last_block: bool = True,\n",
    "        # New parameters for head search\n",
    "        head_type: str = 'original', # 'linear', 'mlp_1_layer', 'original'\n",
    "        head_hidden_dim: int = 1024\n",
    "    ):\n",
    "        # Initialize the base model (loads weights, sets up backbone)\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            freeze_backbone=freeze_backbone,\n",
    "            ckpt_path=ckpt_path,\n",
    "            unfreeze_last_block=unfreeze_last_block\n",
    "        )\n",
    "\n",
    "        # Detect input features (ResNet50 usually 2048)\n",
    "        # We look at the first layer of the head created by the parent class to find input size\n",
    "        if isinstance(self.backbone.fc, nn.Sequential):\n",
    "            in_features = self.backbone.fc[0].in_features\n",
    "        elif isinstance(self.backbone.fc, nn.Linear):\n",
    "            in_features = self.backbone.fc.in_features\n",
    "        else:\n",
    "            in_features = 2048 \n",
    "\n",
    "        # --- Re-define the Head based on 'head_type' ---\n",
    "        \n",
    "        # Variation 1: Simple Linear (Logistic Regression equivalent)\n",
    "        if head_type == 'linear':\n",
    "            self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "        # Variation 2: 1 Hidden Layer (Standard MLP)\n",
    "        elif head_type == 'mlp_1_layer':\n",
    "            self.backbone.fc = nn.Sequential(\n",
    "                nn.Linear(in_features, head_hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.Linear(head_hidden_dim, num_classes),\n",
    "            )\n",
    "            \n",
    "        # Variation 3: Your Original Complex Head (2 Hidden Layers + Hardswish)\n",
    "        elif head_type == 'original':\n",
    "            self.backbone.fc = nn.Sequential(\n",
    "                nn.Linear(in_features, 1024),\n",
    "                nn.Hardswish(inplace=True),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.Linear(1024, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.Linear(256, num_classes),\n",
    "            )\n",
    "            \n",
    "        # Ensure the new head is trainable\n",
    "        for p in self.backbone.fc.parameters():\n",
    "            p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb49a6",
   "metadata": {
    "id": "fcfb49a6"
   },
   "source": [
    "## **10. Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Shape [C].\n",
    "            gamma (float): Focusing parameter. Higher value = more focus on hard examples.\n",
    "                           Default is 2.0 (standard from the paper).\n",
    "            reduction (str): 'mean', 'sum', or 'none'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: [Batch, C] (Logits)\n",
    "        # targets: [Batch] (Class Indices)\n",
    "        \n",
    "        # 1. Standard Cross Entropy Loss (element-wise, no reduction yet)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # 2. Get the probability of the true class (pt)\n",
    "        # pt = exp(-ce_loss) because ce_loss = -log(pt)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # 3. Calculate Focal Component: (1 - pt)^gamma\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        # 4. Apply Class Weights (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            # Gather the alpha value corresponding to the target class for each sample\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            \n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        # 5. Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aa519",
   "metadata": {
    "id": "9c7aa519"
   },
   "source": [
    "## **11. Function: Training & Validation Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abb343",
   "metadata": {
    "id": "37abb343"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Lists to store all predictions and labels for F1 calculation\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    loop = tqdm(loader, leave=False)\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics accumulation\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Move to CPU and convert to numpy for sklearn metrics\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    # Calculate F1 Score (Macro for imbalanced data)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adc741",
   "metadata": {},
   "source": [
    "## **OPTUNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de10848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit combinations to avoid redundant runs in Grid Search\n",
    "LOSS_CONFIGS = {\n",
    "    # --- Focal Loss Variations ---\n",
    "    'focal_g2_weighted':   {'type': 'Focal', 'gamma': 2.0, 'use_weights': True,  'smoothing': 0.0},\n",
    "    'focal_g5_weighted':   {'type': 'Focal', 'gamma': 5.0, 'use_weights': True,  'smoothing': 0.0},\n",
    "    'focal_g2_no_weight':  {'type': 'Focal', 'gamma': 2.0, 'use_weights': False, 'smoothing': 0.0},\n",
    "    \n",
    "    # --- CrossEntropy Variations ---\n",
    "    'ce_plain':            {'type': 'CE',    'gamma': None, 'use_weights': False, 'smoothing': 0.0},\n",
    "    'ce_weighted':         {'type': 'CE',    'gamma': None, 'use_weights': True,  'smoothing': 0.0},\n",
    "    'ce_smooth_0.1':       {'type': 'CE',    'gamma': None, 'use_weights': False, 'smoothing': 0.1},\n",
    "    'ce_weighted_smooth':  {'type': 'CE',    'gamma': None, 'use_weights': True,  'smoothing': 0.1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f02161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- 1. Clean Hyperparameters (TPE supports ranges) ---\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'SGD', 'RAdam'])\n",
    "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # Corrected Dropout (removed the duplicate)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.5) \n",
    "    \n",
    "    # Head Params\n",
    "    head_type = trial.suggest_categorical('head_type', ['linear', 'mlp_1_layer', 'original'])\n",
    "    head_hidden_dim = trial.suggest_categorical('head_hidden_dim', [512, 1024])\n",
    "\n",
    "    # --- 2. Smart Weight Logic ---\n",
    "    # We select the Loss Config first\n",
    "    loss_config_name = trial.suggest_categorical('loss_config', list(LOSS_CONFIGS.keys()))\n",
    "    current_loss_params = LOSS_CONFIGS[loss_config_name]\n",
    "    \n",
    "    # Only suggest weights if the loss config actually USES them\n",
    "    # This prevents TPE from wasting time tuning w1..w4 for unweighted losses\n",
    "    if current_loss_params['use_weights']:\n",
    "        w1 = trial.suggest_float('w1', 0.5, 1.0)\n",
    "        w2 = trial.suggest_float('w2', 0.5, 1.0)\n",
    "        w3 = trial.suggest_float('w3', 0.5, 1.0)\n",
    "        w4 = trial.suggest_float('w4', 1.0, 1.5)\n",
    "        final_weights = torch.tensor([w1, w2, w3, w4], dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        final_weights = None\n",
    "\n",
    "    # --- 3. Data Loaders ---\n",
    "    opt_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    opt_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # --- 4. Model Setup ---\n",
    "    model = RetCCLResNet50_Flexible(\n",
    "        num_classes=4, \n",
    "        dropout_rate=dropout_rate,\n",
    "        freeze_backbone=True, \n",
    "        ckpt_path=os.path.join(\"models\", \"best_ckpt.pth\"), \n",
    "        unfreeze_last_block=False,\n",
    "        head_type=head_type,\n",
    "        head_hidden_dim=head_hidden_dim\n",
    "    ).to(device)\n",
    "\n",
    "    # --- 5. Loss Setup ---\n",
    "    if current_loss_params['type'] == 'Focal':\n",
    "        criterion = FocalLoss(alpha=final_weights, gamma=current_loss_params['gamma'])\n",
    "    elif current_loss_params['type'] == 'CE':\n",
    "        criterion = nn.CrossEntropyLoss(weight=final_weights, label_smoothing=current_loss_params['smoothing'])\n",
    "    \n",
    "    # --- 6. Optimizer ---\n",
    "    if optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=l2_reg)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9, weight_decay=l2_reg)\n",
    "    else:  # RAdam\n",
    "        optimizer = optim.RAdam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=l2_reg, eps=1e-8, betas=(0.9, 0.999))\n",
    "\n",
    "    # --- 7. Training Loop ---\n",
    "    SEARCH_EPOCHS = 10 \n",
    "    best_f1_in_trial = 0.0\n",
    "\n",
    "    for epoch in range(SEARCH_EPOCHS):\n",
    "        _, _ = train_one_epoch(model, opt_train_loader, criterion, optimizer, device)\n",
    "        _, val_f1 = validate(model, opt_val_loader, criterion, device)\n",
    "\n",
    "        if val_f1 > best_f1_in_trial:\n",
    "            best_f1_in_trial = val_f1\n",
    "        \n",
    "        trial.report(val_f1, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return best_f1_in_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de81c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Run the Expanded Study ---\n",
    "sampler = TPESampler(seed=42)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"retccl_head_search\", \n",
    "    direction=\"maximize\", \n",
    "    sampler=sampler, \n",
    "    pruner=pruner\n",
    ")\n",
    "N_TRIALS = 100\n",
    "print(f\"Starting TPE Search with {N_TRIALS} trials...\") # Fixed print statement\n",
    "\n",
    "# Ensure n_trials is sufficient for TPE to converge (100 is a good start)\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=2) \n",
    "\n",
    "print(\"Best Params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f73ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Study Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best F1 Score: {study.best_value:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
