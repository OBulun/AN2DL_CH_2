{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥ The Grumpy Doctogres: EfficientNet-B3 + AutoAugment (Windows Fix)\n",
    "\n",
    "## ðŸ› ï¸ Windows Stability Fix Applied\n",
    "- **`num_workers=0`**: Windows cannot handle multi-process data loading easily inside Jupyter Notebooks. We have disabled worker processes to prevent the `DataLoader worker exited unexpectedly` crash. \n",
    "- **`AutoAugment`**: Maintained as the primary augmentation strategy.\n",
    "- **`fit()` function**: Restored the exact robust training loop from your lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# 1. IMPORTS & SETUP\n",
    "# --------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import time\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# 2. DATA PREP & AUTO AUGMENTATION\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# PATHS\n",
    "DATASET_ROOT = os.path.join(os.path.pardir, \"an2dl2526c2\") \n",
    "LABELS_CSV = os.path.join(DATASET_ROOT, \"train_labels.csv\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATASET_ROOT, \"train_masked_noshreks\")\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE = 16 # RTX 3070 8GB VRAM Safe limit for B3\n",
    "LEARNING_RATE = 1e-4 \n",
    "EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "IMG_SIZE = 300 # EfficientNet B3 Resolution\n",
    "\n",
    "# Load Data Logic\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "labels_map = dict(zip(labels_df.iloc[:, 0], labels_df.iloc[:, 1]))\n",
    "valid_filenames = sorted([f for f in os.listdir(CLEAN_DATA_DIR) if f in labels_map])\n",
    "y_labels_raw = [labels_map[f] for f in valid_filenames]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_labels_raw)\n",
    "CLASSES = label_encoder.classes_\n",
    "\n",
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(valid_filenames, y_encoded, test_size=0.2, stratify=y_encoded, random_state=SEED)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n",
    "\n",
    "# Class Weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Dataset Class\n",
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, root_dir, transform=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.filenames)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.root_dir, self.filenames[idx])).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "# --- âš¡ AUTO AUGMENTATION âš¡ ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Loaders - num_workers=0 is CRITICAL for Windows stability in Notebooks\n",
    "train_loader = DataLoader(HistologyDataset(X_train, y_train, CLEAN_DATA_DIR, train_transform), \n",
    "                          batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(HistologyDataset(X_val, y_val, CLEAN_DATA_DIR, val_transform), \n",
    "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(HistologyDataset(X_test, y_test, CLEAN_DATA_DIR, val_transform), \n",
    "                         batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# 3. TRAINING FUNCTIONS (EXACTLY AS REQUESTED)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Iterate through training batches\n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device, patience=10, model_name=\"model\"):\n",
    "    training_history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_path = f\"{model_name}_best.pth\"\n",
    "    \n",
    "    print(f\"Training {model_name} for {epochs} epochs on {device}...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update History\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['train_acc'].append(train_acc)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Step Scheduler\n",
    "        if 'scheduler' in globals():\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Time: {elapsed:.0f}s\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        \n",
    "        # Early Stopping & Saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  --> New Best Model Saved! (Acc: {val_acc:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  --> Patience: {patience_counter}/{patience}\")\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "            \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_8796\\1369748484.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training efficientnet_b3_autoaugment for 100 epochs on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Time: 26s\n",
      "  Train Loss: 2.3041 | Train Acc: 0.2507\n",
      "  Val Loss:   2.0922 | Val Acc:   0.2540\n",
      "  --> New Best Model Saved! (Acc: 0.2540)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 | Time: 12s\n",
      "  Train Loss: 1.7391 | Train Acc: 0.3053\n",
      "  Val Loss:   1.9479 | Val Acc:   0.3333\n",
      "  --> New Best Model Saved! (Acc: 0.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 | Time: 12s\n",
      "  Train Loss: 1.5530 | Train Acc: 0.3330\n",
      "  Val Loss:   1.9740 | Val Acc:   0.1825\n",
      "  --> Patience: 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 | Time: 13s\n",
      "  Train Loss: 1.4320 | Train Acc: 0.3726\n",
      "  Val Loss:   1.6981 | Val Acc:   0.2381\n",
      "  --> Patience: 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 | Time: 13s\n",
      "  Train Loss: 1.3014 | Train Acc: 0.4172\n",
      "  Val Loss:   1.6824 | Val Acc:   0.2302\n",
      "  --> Patience: 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 | Time: 12s\n",
      "  Train Loss: 1.2900 | Train Acc: 0.4182\n",
      "  Val Loss:   1.6478 | Val Acc:   0.3254\n",
      "  --> Patience: 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 | Time: 13s\n",
      "  Train Loss: 1.1877 | Train Acc: 0.4718\n",
      "  Val Loss:   1.7688 | Val Acc:   0.3333\n",
      "  --> Patience: 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 | Time: 13s\n",
      "  Train Loss: 1.0964 | Train Acc: 0.5510\n",
      "  Val Loss:   1.7356 | Val Acc:   0.2381\n",
      "  --> Patience: 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 | Time: 13s\n",
      "  Train Loss: 1.0684 | Train Acc: 0.5342\n",
      "  Val Loss:   1.6577 | Val Acc:   0.3175\n",
      "  --> Patience: 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Time: 12s\n",
      "  Train Loss: 0.9539 | Train Acc: 0.6085\n",
      "  Val Loss:   1.6805 | Val Acc:   0.3016\n",
      "  --> Patience: 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 | Time: 14s\n",
      "  Train Loss: 0.9414 | Train Acc: 0.6026\n",
      "  Val Loss:   1.7095 | Val Acc:   0.3175\n",
      "  --> Patience: 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 | Time: 13s\n",
      "  Train Loss: 0.8744 | Train Acc: 0.6392\n",
      "  Val Loss:   1.7551 | Val Acc:   0.3333\n",
      "  --> Patience: 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 | Time: 13s\n",
      "  Train Loss: 0.7439 | Train Acc: 0.6759\n",
      "  Val Loss:   1.7118 | Val Acc:   0.3651\n",
      "  --> New Best Model Saved! (Acc: 0.3651)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 | Time: 14s\n",
      "  Train Loss: 0.6767 | Train Acc: 0.7433\n",
      "  Val Loss:   1.7417 | Val Acc:   0.3254\n",
      "  --> Patience: 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 | Time: 14s\n",
      "  Train Loss: 0.6915 | Train Acc: 0.7314\n",
      "  Val Loss:   1.8131 | Val Acc:   0.3175\n",
      "  --> Patience: 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 | Time: 13s\n",
      "  Train Loss: 0.6440 | Train Acc: 0.7552\n",
      "  Val Loss:   1.7739 | Val Acc:   0.3254\n",
      "  --> Patience: 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 | Time: 14s\n",
      "  Train Loss: 0.6662 | Train Acc: 0.7512\n",
      "  Val Loss:   1.7461 | Val Acc:   0.3095\n",
      "  --> Patience: 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 | Time: 13s\n",
      "  Train Loss: 0.6275 | Train Acc: 0.7493\n",
      "  Val Loss:   1.8252 | Val Acc:   0.3254\n",
      "  --> Patience: 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 | Time: 15s\n",
      "  Train Loss: 0.6044 | Train Acc: 0.7701\n",
      "  Val Loss:   1.7762 | Val Acc:   0.3413\n",
      "  --> Patience: 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 | Time: 14s\n",
      "  Train Loss: 0.6119 | Train Acc: 0.7651\n",
      "  Val Loss:   1.7143 | Val Acc:   0.3095\n",
      "  --> Patience: 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 | Time: 13s\n",
      "  Train Loss: 0.6256 | Train Acc: 0.7750\n",
      "  Val Loss:   1.7902 | Val Acc:   0.3413\n",
      "  --> Patience: 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 | Time: 15s\n",
      "  Train Loss: 0.5821 | Train Acc: 0.7800\n",
      "  Val Loss:   1.7195 | Val Acc:   0.3571\n",
      "  --> Patience: 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 | Time: 15s\n",
      "  Train Loss: 0.6068 | Train Acc: 0.7582\n",
      "  Val Loss:   1.8818 | Val Acc:   0.3016\n",
      "  --> Patience: 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 | Time: 15s\n",
      "  Train Loss: 0.6207 | Train Acc: 0.7592\n",
      "  Val Loss:   1.7765 | Val Acc:   0.3413\n",
      "  --> Patience: 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 | Time: 14s\n",
      "  Train Loss: 0.5911 | Train Acc: 0.7820\n",
      "  Val Loss:   1.7352 | Val Acc:   0.3175\n",
      "  --> Patience: 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 | Time: 15s\n",
      "  Train Loss: 0.6169 | Train Acc: 0.7562\n",
      "  Val Loss:   1.7686 | Val Acc:   0.3095\n",
      "  --> Patience: 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 | Time: 15s\n",
      "  Train Loss: 0.6219 | Train Acc: 0.7572\n",
      "  Val Loss:   1.8253 | Val Acc:   0.3095\n",
      "  --> Patience: 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 | Time: 15s\n",
      "  Train Loss: 0.6358 | Train Acc: 0.7532\n",
      "  Val Loss:   1.8640 | Val Acc:   0.2698\n",
      "  --> Patience: 15/15\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# 4. MODEL INIT & EXECUTION\n",
    "# --------------------------------------------------------\n",
    "\n",
    "MODEL_NAME = 'efficientnet_b3'\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=len(CLASSES))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Initialize Scaler for Mixed Precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# RUN THE FIT FUNCTION\n",
    "history = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    patience=PATIENCE,\n",
    "    model_name=\"efficientnet_b3_autoaugment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_8796\\1941345601.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"efficientnet_b3_autoaugment_best.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.3071\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        HER2(+)       0.38      0.24      0.30        33\n",
      "      Luminal A       0.24      0.44      0.31        34\n",
      "      Luminal B       0.50      0.22      0.31        45\n",
      "Triple negative       0.26      0.40      0.32        15\n",
      "\n",
      "       accuracy                           0.31       127\n",
      "      macro avg       0.34      0.33      0.31       127\n",
      "   weighted avg       0.37      0.31      0.31       127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# 5. FINAL EVALUATION\n",
    "# --------------------------------------------------------\n",
    "model.load_state_dict(torch.load(\"efficientnet_b3_autoaugment_best.pth\"))\n",
    "\n",
    "test_loss, test_acc = validate_one_epoch(model, test_loader, criterion, device)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_targets, all_preds, target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data found at: ..\\an2dl2526c2\\test_masked_noshreks\n",
      "Found 954 test images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_8796\\844993284.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"efficientnet_b3_autoaugment_best.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Inference on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:05<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… submission.csv generated successfully!\n",
      "       filename            label\n",
      "0  img_0000.png          HER2(+)\n",
      "1  img_0001.png  Triple negative\n",
      "2  img_0002.png        Luminal A\n",
      "3  img_0003.png        Luminal A\n",
      "4  img_0004.png          HER2(+)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# GENERATE KAGGLE SUBMISSION\n",
    "# --------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# 1. SETUP PATHS\n",
    "# Adjust this if your test images are in a different folder name\n",
    "TEST_DATA_DIR = os.path.join(DATASET_ROOT, \"test_masked_noshreks\") \n",
    "\n",
    "# Check if path exists\n",
    "if not os.path.exists(TEST_DATA_DIR):\n",
    "    print(f\"âš ï¸ WARNING: Could not find '{TEST_DATA_DIR}'.\")\n",
    "    print(\"Please check where you extracted 'test_data.zip' and update TEST_DATA_DIR.\")\n",
    "else:\n",
    "    print(f\"Test Data found at: {TEST_DATA_DIR}\")\n",
    "\n",
    "# 2. DEFINE SUBMISSION DATASET\n",
    "# (Does not require labels, returns image + filename)\n",
    "class SubmissionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        # Get all valid image files\n",
    "        self.filenames = sorted([\n",
    "            f for f in os.listdir(root_dir) \n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        img_path = os.path.join(self.root_dir, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, fname\n",
    "\n",
    "# 3. PREPARE LOADER\n",
    "# Use the same validation transform (Resize + Normalize only)\n",
    "submission_dataset = SubmissionDataset(TEST_DATA_DIR, transform=val_transform)\n",
    "# num_workers=0 for Windows stability\n",
    "submission_loader = DataLoader(submission_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Found {len(submission_dataset)} test images.\")\n",
    "\n",
    "# 4. LOAD BEST MODEL\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=len(CLASSES))\n",
    "model.load_state_dict(torch.load(f\"efficientnet_b3_autoaugment_best.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 5. INFERENCE LOOP\n",
    "all_filenames = []\n",
    "all_predictions = []\n",
    "\n",
    "print(\"Running Inference on Test Set...\")\n",
    "with torch.no_grad():\n",
    "    for images, filenames in tqdm(submission_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get predicted class index\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # Store results\n",
    "        all_filenames.extend(filenames)\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# 6. DECODE LABELS & SAVE CSV\n",
    "# Convert numerical indices (0, 1, 2...) back to strings ('Luminal A', 'HER2+', etc.)\n",
    "predicted_labels = label_encoder.inverse_transform(all_predictions)\n",
    "\n",
    "# Create DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': all_filenames,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… submission.csv generated successfully!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
