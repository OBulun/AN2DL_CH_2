{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥ The Grumpy Doctogres: Advanced Fine-Tuning Pipeline\n",
    "\n",
    "## Key Features implemented:\n",
    "1.  **AutoAugment**: \"The Storm\" designed by the machine (as per Prof's advice).\n",
    "2.  **Custom Architecture**: We don't just use `timm` directly. We wrap it in a custom `nn.Module` with a sophisticated classification head (Linear -> BN -> ReLU -> Dropout -> Linear).\n",
    "3.  **Two-Stage Training**: \n",
    "    *   **Phase 1**: Frozen Backbone (Train only the head).\n",
    "    *   **Phase 2**: Unfrozen Backbone (Fine-tune the whole network with low LR).\n",
    "4.  **Class Balancing**: Weighted Loss to handle the imbalance between 'Luminal' and 'Triple Negative'.\n",
    "5.  **Mixed Precision**: For faster training on RTX cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- REPRODUCIBILITY ---\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['HER2(+)' 'Luminal A' 'Luminal B' 'Triple negative']\n",
      "Train: 1009 | Val: 126 | Test: 127\n",
      "Class Weights: [0.98151751 0.93081181 0.70856742 2.018     ]\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATASET_ROOT = os.path.join(os.path.pardir, \"an2dl2526c2\") # Adjust path if needed\n",
    "LABELS_CSV = os.path.join(DATASET_ROOT, \"train_labels.csv\")\n",
    "CLEAN_DATA_DIR = os.path.join(DATASET_ROOT, \"train_masked_noshreks\")\n",
    "TEST_DATA_DIR = os.path.join(DATASET_ROOT, \"test_masked_noshreks\")\n",
    "\n",
    "IMG_SIZE = 300 # EfficientNet B3 works well with 300x300\n",
    "BATCH_SIZE = 16 # Adjust based on VRAM (16 for 8GB, 32 for 16GB)\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "# Create dictionary mapping filename -> label\n",
    "labels_map = dict(zip(labels_df.iloc[:, 0], labels_df.iloc[:, 1]))\n",
    "\n",
    "# Filter filenames ensuring they exist in the clean folder\n",
    "valid_filenames = sorted([f for f in os.listdir(CLEAN_DATA_DIR) if f in labels_map])\n",
    "y_labels_raw = [labels_map[f] for f in valid_filenames]\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_labels_raw)\n",
    "CLASSES = label_encoder.classes_\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "# --- STRATIFIED SPLIT ---\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    valid_filenames, y_encoded, test_size=0.2, stratify=y_encoded, random_state=SEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# --- COMPUTE CLASS WEIGHTS ---\n",
    "# This handles the imbalance (e.g., fewer Triple Negative samples)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset & AutoAugment\n",
    "Here we implement the Professor's suggestion: **AutoAugment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, root_dir, transform=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.filenames[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# --- âš¡ AUTO AUGMENTATION âš¡ ---\n",
    "# The \"Machine chooses the storm\" approach\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    # AutoAugment: Learned augmentation policy from ImageNet\n",
    "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- DATALOADERS ---\n",
    "# num_workers=0 is safer on Windows to avoid \"broken pipe\" errors\n",
    "train_dataset = HistologyDataset(X_train, y_train, CLEAN_DATA_DIR, train_transform)\n",
    "val_dataset = HistologyDataset(X_val, y_val, CLEAN_DATA_DIR, val_transform)\n",
    "test_dataset = HistologyDataset(X_test, y_test, CLEAN_DATA_DIR, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Model Architecture\n",
    "We use EfficientNet B3 as a backbone, but we define a custom **Classification Head**. We also add a method to freeze/unfreeze the backbone for 2-stage training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefinedEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(RefinedEfficientNet, self).__init__()\n",
    "        \n",
    "        # 1. Load Backbone (EfficientNet B3)\n",
    "        # num_classes=0 removes the default classifier\n",
    "        self.backbone = timm.create_model('efficientnet_b3', pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        # Get the input dimension of the classification head (usually 1536 for B3)\n",
    "        in_features = self.backbone.num_features\n",
    "        \n",
    "        # 2. Custom Head\n",
    "        # More complex than a single layer: [Dense -> BN -> ReLU -> Drop -> Dense]\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),  # Dropout to reduce overfitting\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.head(features)\n",
    "        return output\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        # Freezes the feature extractor, allowing only the head to learn\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Backbone frozen.\")\n",
    "            \n",
    "    def unfreeze_backbone(self):\n",
    "        # Unfreezes everything for fine-tuning\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Backbone unfrozen.\")\n",
    "\n",
    "model = RefinedEfficientNet(num_classes=NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Engine\n",
    "Standard loop with Mixed Precision (`scaler`) and Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Mixed Precision Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def run_training(model, epochs, patience, save_name, lr):\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    # Scheduler: Drop LR if validation loss stalls\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_name)\n",
    "            print(f\"--> Saved Best Model ({best_val_acc:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "            \n",
    "    # Load best model before returning\n",
    "    model.load_state_dict(torch.load(save_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 1: Warmup (Head Only)\n",
    "We freeze the backbone so gradients don't destroy pre-trained weights. We use a higher LR (1e-3) to get the head into shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_38196\\1767922644.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1: WARMUP ==\n",
      "Backbone frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_38196\\1767922644.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.6175 Acc: 0.2440 | Val Loss: 1.4526 Acc: 0.3492\n",
      "--> Saved Best Model (0.3492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 1.4819 Acc: 0.2619 | Val Loss: 1.4738 Acc: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 1.4353 Acc: 0.3175 | Val Loss: 1.4383 Acc: 0.2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 1.4231 Acc: 0.2996 | Val Loss: 1.4607 Acc: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 1.3835 Acc: 0.3333 | Val Loss: 1.4233 Acc: 0.3016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 1.3803 Acc: 0.3433 | Val Loss: 1.4546 Acc: 0.2937\n",
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_38196\\1767922644.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_name))\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PHASE 1: WARMUP ==\")\n",
    "model.freeze_backbone()\n",
    "# Short training (10 epochs) just for the head\n",
    "model = run_training(model, epochs=10, patience=5, save_name=\"warmup_checkpoint.pth\", lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 2: Fine-Tuning (Full Network)\n",
    "We unfreeze the backbone and train everything with a **low Learning Rate** (1e-4) to gently adapt the feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PHASE 2: FINE-TUNING ==\n",
      "Backbone unfrozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_38196\\1767922644.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Training:   0%|          | 0/63 [00:00<?, ?it/s]C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_38196\\1767922644.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 1.4536 Acc: 0.3026 | Val Loss: 1.4252 Acc: 0.3333\n",
      "--> Saved Best Model (0.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Train Loss: 1.4032 Acc: 0.3046 | Val Loss: 1.4307 Acc: 0.3571\n",
      "--> Saved Best Model (0.3571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Train Loss: 1.3307 Acc: 0.3274 | Val Loss: 1.4151 Acc: 0.3016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Train Loss: 1.2871 Acc: 0.4058 | Val Loss: 1.4500 Acc: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Train Loss: 1.1870 Acc: 0.4514 | Val Loss: 1.5152 Acc: 0.3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Train Loss: 1.1369 Acc: 0.4950 | Val Loss: 1.5665 Acc: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Train Loss: 1.0780 Acc: 0.5179 | Val Loss: 1.5668 Acc: 0.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Train Loss: 0.9812 Acc: 0.5605 | Val Loss: 1.5826 Acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Train Loss: 0.9113 Acc: 0.6111 | Val Loss: 1.4944 Acc: 0.3651\n",
      "--> Saved Best Model (0.3651)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Train Loss: 0.8321 Acc: 0.6558 | Val Loss: 1.6312 Acc: 0.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train Loss: 0.7821 Acc: 0.6786 | Val Loss: 1.5627 Acc: 0.3571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Train Loss: 0.7642 Acc: 0.7024 | Val Loss: 1.5783 Acc: 0.3810\n",
      "--> Saved Best Model (0.3810)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Train Loss: 0.6857 Acc: 0.7341 | Val Loss: 1.5722 Acc: 0.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Train Loss: 0.6556 Acc: 0.7550 | Val Loss: 1.5394 Acc: 0.3571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Train Loss: 0.6645 Acc: 0.7312 | Val Loss: 1.6269 Acc: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Train Loss: 0.6033 Acc: 0.7579 | Val Loss: 1.6670 Acc: 0.3571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Train Loss: 0.5678 Acc: 0.7897 | Val Loss: 1.5982 Acc: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Train Loss: 0.6046 Acc: 0.7738 | Val Loss: 1.6434 Acc: 0.3413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Train Loss: 0.5434 Acc: 0.7887 | Val Loss: 1.6256 Acc: 0.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 - Train Loss: 0.5610 Acc: 0.7986 | Val Loss: 1.6051 Acc: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Train Loss: 0.5349 Acc: 0.7996 | Val Loss: 1.5542 Acc: 0.3810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 - Train Loss: 0.5141 Acc: 0.8115 | Val Loss: 1.5648 Acc: 0.3651\n",
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Negm\\AppData\\Local\\Temp\\ipykernel_38196\\1767922644.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_name))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PHASE 2: FINE-TUNING ==\")\n",
    "model.unfreeze_backbone()\n",
    "# Longer training with lower LR\n",
    "model = run_training(model, epochs=50, patience=10, save_name=\"final_best_model.pth\", lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation on Test Split (Labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Internal Test Set):\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        HER2(+)       0.31      0.27      0.29        33\n",
      "      Luminal A       0.19      0.24      0.21        34\n",
      "      Luminal B       0.43      0.42      0.43        45\n",
      "Triple negative       0.42      0.33      0.37        15\n",
      "\n",
      "       accuracy                           0.32       127\n",
      "      macro avg       0.34      0.32      0.32       127\n",
      "   weighted avg       0.33      0.32      0.33       127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report (Internal Test Set):\")\n",
    "print(classification_report(all_targets, all_preds, target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Submission (Unlabeled Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:07<00:00,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Submission.csv generated!\n",
      "   sample_index      label\n",
      "0  img_0000.png  Luminal A\n",
      "1  img_0001.png  Luminal A\n",
      "2  img_0002.png  Luminal A\n",
      "3  img_0003.png    HER2(+)\n",
      "4  img_0004.png  Luminal A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SubmissionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.filenames = sorted([f for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        img_path = os.path.join(self.root_dir, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, fname\n",
    "\n",
    "if os.path.exists(TEST_DATA_DIR):\n",
    "    sub_dataset = SubmissionDataset(TEST_DATA_DIR, transform=val_transform)\n",
    "    sub_loader = DataLoader(sub_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    sub_preds = []\n",
    "    sub_files = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(sub_loader, desc=\"Inferring\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            sub_preds.extend(preds.cpu().numpy())\n",
    "            sub_files.extend(filenames)\n",
    "\n",
    "    # Convert indices back to strings\n",
    "    pred_labels = label_encoder.inverse_transform(sub_preds)\n",
    "\n",
    "    df_sub = pd.DataFrame({'sample_index': sub_files, 'label': pred_labels})\n",
    "    df_sub.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"\\nâœ… Submission.csv generated!\")\n",
    "    print(df_sub.head())\n",
    "else:\n",
    "    print(\"Test data folder not found. Skipping submission generation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
