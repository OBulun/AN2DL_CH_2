{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3293c812",
   "metadata": {
    "id": "3293c812"
   },
   "source": [
    "## **1. Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631b342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4631b342",
    "outputId": "894aecb7-7cec-4087-ebc6-23e79bd3cc59"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#cur_dir = \"/content/drive/MyDrive/CH2/Notebooks\"\n",
    "#%cd $cur_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfd385",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8bfd385",
    "outputId": "530a04d1-2a59-40bb-ec1b-e48362319dcc"
   },
   "outputs": [],
   "source": [
    "#%pip install torchview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee15c7",
   "metadata": {
    "id": "edee15c7"
   },
   "source": [
    "## **2. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c7403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "963c7403",
    "outputId": "1de1d7e2-30d6-48ad-9c91-01532e5c5a20"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED =42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "#from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms as tfs\n",
    "import math\n",
    "from transformers import AutoModel\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ee8",
   "metadata": {
    "id": "3b5b1ee8"
   },
   "source": [
    "## **3. Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MASKED_PATCHES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3da911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df3da911",
    "outputId": "0a589898-8a24-46f0-e2bd-fa26b93c1eec"
   },
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.path.pardir, \"an2dl2526c2\")\n",
    "\n",
    "train_data_path = os.path.join(datasets_path, \"train_data\")\n",
    "train_labels_path = os.path.join(datasets_path, \"train_labels.csv\")\n",
    "test_data_path = os.path.join(datasets_path, \"test_data\")\n",
    "\n",
    "CSV_PATH = train_labels_path                # Path to the CSV file with labels\n",
    "SOURCE_FOLDER = train_data_path\n",
    "\n",
    "if USE_MASKED_PATCHES:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"train_patches_masked\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results_masked\",\"submission_patches_masked\")\n",
    "else:\n",
    "  PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches\")\n",
    "  SUBMISSION_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\")\n",
    "\n",
    "\n",
    "    \n",
    "SUBMISSION_PATCH_MASKS = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches\",\"masks\")\n",
    "BLURRED_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"train_patches_blurred\")\n",
    "SUBMISSION_BLURRED_PATCHES_OUT = os.path.join(datasets_path, \"preprocessing_results\",\"submission_patches_blurred\")\n",
    "print(f\"Dataset path: {datasets_path}\")\n",
    "print(f\"Train data path: {train_data_path}\")\n",
    "print(f\"Train labels path: {train_labels_path}\")\n",
    "print(f\"Test data path: {test_data_path}\")\n",
    "print(f\"Patches output path: {PATCHES_OUT}\")\n",
    "print(f\"Submission patches output path: {SUBMISSION_PATCHES_OUT}\")\n",
    "print(f\"Submission patch masks path: {SUBMISSION_PATCH_MASKS}\")\n",
    "\n",
    "\n",
    "# Define where your Ground Truth binary masks are stored\n",
    "# Assuming filenames match: img_001.png -> mask_001.png (or similar)\n",
    "MASKS_DIR = os.path.join(datasets_path, \"preprocessing_results\", \"train_patches\", \"masks\")\n",
    "\n",
    "# ImageNet normalization statistics\n",
    "IMAGENET_MEAN = [float(x) for x in [0.485, 0.456, 0.406]]  # or convert your current values to float\n",
    "IMAGENET_STD  = [float(x) for x in [0.229, 0.224, 0.225]]\n",
    "\n",
    "\n",
    "\n",
    "TARGET_SIZE = (224, 224)                    # Target size for the resized images and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cd886",
   "metadata": {},
   "source": [
    "### 3.2 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80345722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def plot_sample_with_predictions(model, loader, device, label_encoder, sample_id=None, aggregation_method='max_confidence'):\n",
    "    \"\"\"\n",
    "    Plot all patches of a single sample and the aggregated image prediction.\n",
    "    (Updated for Paired Image / Two-Stream Models)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Robust Dataset Access\n",
    "    dataset = loader.dataset\n",
    "    while hasattr(dataset, 'dataset'): # Unwrap Subsets\n",
    "        dataset = dataset.dataset\n",
    "    \n",
    "    df = dataset.df\n",
    "    \n",
    "    # <--- CHANGED: Retrieve target_dir\n",
    "    target_dir = getattr(dataset, 'target_dir', None)\n",
    "\n",
    "    # 2. Pick a sample_id\n",
    "    if sample_id is None:\n",
    "        sample_id = np.random.choice(df['sample_id'].unique())\n",
    "    \n",
    "    # Get all patches for this sample\n",
    "    sample_patches = df[df['sample_id'] == sample_id].reset_index(drop=True)\n",
    "    print(f\"Visualizing Sample ID: {sample_id} ({len(sample_patches)} patches)\")\n",
    "\n",
    "    # 3. Define Transforms\n",
    "    # <--- CHANGED: Use the same transform (Resize+Norm) for BOTH images\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 4. Load Data\n",
    "    images_tensors = []\n",
    "    targets_tensors = [] # <--- Renamed from masks_tensors\n",
    "    display_imgs = []\n",
    "    \n",
    "    # Stats for denormalization (Display only)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "    for _, row in sample_patches.iterrows():\n",
    "        try:\n",
    "            # --- A. Load Input Image ---\n",
    "            img_path = row['path']\n",
    "            # Handle if full path or relative\n",
    "            if hasattr(dataset, 'img_dir') and dataset.img_dir:\n",
    "                 img_path = os.path.join(dataset.img_dir, img_path)\n",
    "\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            tensor_img = inference_transform(img)\n",
    "            images_tensors.append(tensor_img)\n",
    "            \n",
    "            # Create Display Image (Denormalize -> Clamp -> Numpy)\n",
    "            disp_img = torch.clamp(tensor_img * std + mean, 0, 1)\n",
    "            display_imgs.append(disp_img)\n",
    "            \n",
    "            # --- B. Load Target Image ---\n",
    "            # <--- CHANGED: Load second image based on filename match\n",
    "            if target_dir:\n",
    "                filename = os.path.basename(img_path)\n",
    "                target_path = os.path.join(target_dir, filename)\n",
    "                \n",
    "                if os.path.exists(target_path):\n",
    "                    target = Image.open(target_path).convert('RGB')\n",
    "                else:\n",
    "                    target = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            else:\n",
    "                target = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            \n",
    "            # <--- CHANGED: Normalize the target too\n",
    "            tensor_target = inference_transform(target)\n",
    "            targets_tensors.append(tensor_target)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {row['path']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not images_tensors:\n",
    "        print(\"No valid images found for this sample.\")\n",
    "        return\n",
    "\n",
    "    # 5. Batch Inference\n",
    "    batch_imgs = torch.stack(images_tensors).to(device)\n",
    "    batch_targets = torch.stack(targets_tensors).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- FIX: Pass BOTH image batches ---\n",
    "        logits = model(batch_imgs, batch_targets) \n",
    "        probs = torch.softmax(logits, dim=1).cpu()\n",
    "\n",
    "    # 6. Process Predictions\n",
    "    patch_preds = probs.argmax(dim=1).numpy()\n",
    "    patch_confs = probs.max(dim=1).values.numpy()\n",
    "\n",
    "    # --- Aggregation Logic ---\n",
    "    if aggregation_method == 'max_confidence':\n",
    "        image_probs = probs.mean(dim=0).numpy()\n",
    "        image_pred = image_probs.argmax()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    elif aggregation_method == 'majority_voting':\n",
    "        counts = np.bincount(patch_preds, minlength=len(label_encoder.classes_))\n",
    "        image_pred = counts.argmax()\n",
    "        image_probs = counts / counts.sum()\n",
    "        image_conf = image_probs[image_pred]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n",
    "\n",
    "    # Labels\n",
    "    image_pred_label = label_encoder.inverse_transform([image_pred])[0]\n",
    "    true_label_idx = sample_patches.iloc[0]['label_encoded']\n",
    "    true_label = label_encoder.inverse_transform([true_label_idx])[0]\n",
    "\n",
    "    # 7. Plotting\n",
    "    cols = min(6, len(sample_patches))\n",
    "    rows = math.ceil(len(sample_patches) / cols)\n",
    "    \n",
    "    fig = plt.figure(figsize=(3*cols + 4, 3*rows))\n",
    "    gs = fig.add_gridspec(rows, cols + 1, width_ratios=[1]*cols + [1.3])\n",
    "\n",
    "    # Patch grid\n",
    "    for idx, (img_disp, pred, conf) in enumerate(zip(display_imgs, patch_preds, patch_confs)):\n",
    "        ax = fig.add_subplot(gs[idx // cols, idx % cols])\n",
    "        \n",
    "        # Note: We display the INPUT image. \n",
    "        # If you want to see the target, you'd need a more complex plot (e.g., side-by-side).\n",
    "        ax.imshow(img_disp.permute(1,2,0))\n",
    "        \n",
    "        lbl = label_encoder.inverse_transform([pred])[0]\n",
    "        color = 'green' if pred == image_pred else 'red'\n",
    "        \n",
    "        # Title shows Patch Prediction\n",
    "        ax.set_title(f\"{lbl}\\n{conf:.2%}\", fontsize=9, color='black', backgroundcolor='white')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Aggregated Bar Chart\n",
    "    ax_bar = fig.add_subplot(gs[:, -1])\n",
    "    class_names = label_encoder.classes_\n",
    "    colors = ['green' if i == image_pred else 'lightgray' for i in range(len(class_names))]\n",
    "    \n",
    "    y_pos = np.arange(len(class_names))\n",
    "    ax_bar.barh(y_pos, image_probs, color=colors)\n",
    "    ax_bar.set_yticks(y_pos)\n",
    "    ax_bar.set_yticklabels(class_names)\n",
    "    ax_bar.invert_yaxis()\n",
    "    \n",
    "    ax_bar.set_xlabel('Probability' if aggregation_method == 'max_confidence' else 'Vote Share')\n",
    "    ax_bar.set_xlim([0,1])\n",
    "    \n",
    "    ax_bar.set_title(f\"Sample: {sample_id}\\nTrue: {true_label}\\nPred: {image_pred_label} ({image_conf:.2%})\\n{aggregation_method}\")\n",
    "    \n",
    "    for i, prob in enumerate(image_probs):\n",
    "        ax_bar.text(prob + 0.02, i, f\"{prob:.3f}\", va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ded56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_image_predictions(model, loader, device, method='max_mean_confidence'):\n",
    "    \"\"\"\n",
    "    Aggregates patch-level predictions to image-level.\n",
    "    Fixed for single-input models (Phikon).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    valid_methods = {\"max_mean_confidence\", \"max_confidence\"}\n",
    "    if method not in valid_methods:\n",
    "        raise ValueError(f\"Unknown method '{method}'. Choose from {valid_methods}.\")\n",
    "\n",
    "    # 1. Robustly retrieve the underlying dataset\n",
    "    dataset = loader.dataset\n",
    "    while hasattr(dataset, 'dataset'):\n",
    "        dataset = dataset.dataset\n",
    "        \n",
    "    df = dataset.df\n",
    "    target_dir = getattr(dataset, 'target_dir', None)\n",
    "\n",
    "    # 2. Define Transforms\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    sample_ids = df['sample_id'].unique()\n",
    "\n",
    "    for sample_id in tqdm(sample_ids, leave=False):\n",
    "        sample_patches = df[df['sample_id'] == sample_id]\n",
    "\n",
    "        # Ground Truth\n",
    "        true_label = sample_patches.iloc[0]['label_encoded']\n",
    "        y_true.append(true_label)\n",
    "\n",
    "        patches_img = []\n",
    "\n",
    "        for img_path in sample_patches['path']:\n",
    "            try:\n",
    "                # Load Input Image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = inference_transform(img)\n",
    "                patches_img.append(img)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not patches_img:\n",
    "            y_pred.append(true_label)  # Fallback if all patches fail\n",
    "            continue\n",
    "\n",
    "        # Stack batch\n",
    "        batch_imgs = torch.stack(patches_img).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # ✓ Pass only the primary image\n",
    "            logits = model(batch_imgs)\n",
    "            \n",
    "            # Softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            if method == \"max_mean_confidence\":\n",
    "                # Average pooling of probabilities\n",
    "                avg_probs = torch.mean(probs, dim=0)\n",
    "                pred_label = torch.argmax(avg_probs).item()\n",
    "            elif method == \"max_confidence\":\n",
    "                # Max confidence pooling\n",
    "                pred_label = torch.argmax(probs.max(dim=0).values).item()\n",
    "\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(patches_dir, labels_csv_path):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping patch filenames to their Bag IDs and Labels.\n",
    "    \"\"\"\n",
    "    # 1. Load the labels CSV\n",
    "    # Assuming CSV structure: [image_id, label] or similar\n",
    "    df_labels = pd.read_csv(labels_csv_path)\n",
    "\n",
    "    # Standardize column names for easier merging\n",
    "    # We assume the first column is the ID and the second is the Label\n",
    "    id_col = df_labels.columns[0]\n",
    "    label_col = df_labels.columns[1]\n",
    "\n",
    "    # Ensure IDs in CSV are strings (to match filenames)\n",
    "    df_labels[id_col] = df_labels[id_col].astype(str)\n",
    "\n",
    "    # If the CSV IDs contain extensions (e.g., 'img_001.png'), remove them\n",
    "    # because our parsed Bag IDs won't have them.\n",
    "    df_labels[id_col] = df_labels[id_col].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # 2. List all patch files\n",
    "    patch_files = [f for f in os.listdir(patches_dir) if f.endswith('.png')]\n",
    "\n",
    "    # 3. Parse filenames to get Bag IDs\n",
    "    data = []\n",
    "    print(f\"Found {len(patch_files)} patches. Parsing metadata...\")\n",
    "\n",
    "    for filename in patch_files:\n",
    "        # Expected format from your preprocessing: {base_name}_p{i}.png\n",
    "        # Example: \"img_0015_p12.png\" -> Bag ID should be \"img_0015\"\n",
    "\n",
    "        # Split from the right on '_p' to separate Bag ID from Patch Index\n",
    "        # \"img_0015_p12.png\" -> [\"img_0015\", \"12.png\"]\n",
    "        try:\n",
    "            bag_id = filename.rsplit('_p', 1)[0]\n",
    "\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sample_id': bag_id,\n",
    "                'path': os.path.join(patches_dir, filename)\n",
    "            })\n",
    "        except IndexError:\n",
    "            print(f\"Skipping malformed filename: {filename}\")\n",
    "\n",
    "    # Create temporary patches DataFrame\n",
    "    df_patches = pd.DataFrame(data)\n",
    "\n",
    "    # 4. Merge patches with labels\n",
    "    # This assigns the correct Bag Label to every Patch in that Bag\n",
    "    df = pd.merge(df_patches, df_labels, left_on='sample_id', right_on=id_col, how='inner')\n",
    "\n",
    "    # 5. Clean up and Rename\n",
    "    # Keep only required columns\n",
    "    df = df[['filename', label_col, 'sample_id', 'path']]\n",
    "\n",
    "    # Rename label column to standard 'label' if it isn't already\n",
    "    df = df.rename(columns={label_col: 'label'})\n",
    "\n",
    "    print(f\"Successfully created DataFrame with {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple function to handle this\n",
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427b09e",
   "metadata": {
    "id": "e427b09e"
   },
   "source": [
    "## **4. Train/Val Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b8e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633b8e05",
    "outputId": "cc264859-b72c-4997-972c-a608a57654d7"
   },
   "outputs": [],
   "source": [
    "patches_metadata_df = create_metadata_dataframe(PATCHES_OUT, CSV_PATH)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(patches_metadata_df.head().drop(columns=['path']))\n",
    "print(\"\\nPatches per Bag (Distribution):\")\n",
    "print(patches_metadata_df['sample_id'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6c6cd3",
    "outputId": "03cc0924-40cd-4064-b22b-c260a0a32895"
   },
   "outputs": [],
   "source": [
    "# Add Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "patches_metadata_df['label_encoded'] = label_encoder.fit_transform(patches_metadata_df['label'])\n",
    "\n",
    "print(f\"\\nOriginal Labels: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for orig, enc in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"  {orig} -> {enc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52314ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52314ae9",
    "outputId": "16d78934-db36-4c7f-c218-0a55baa806a6"
   },
   "outputs": [],
   "source": [
    "# Train/Val/Test Split on Original Images (not patches)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Train/Val/Test Split on Original Images\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Get unique sample IDs and their corresponding labels for stratification\n",
    "unique_samples = patches_metadata_df['sample_id'].unique()\n",
    "sample_labels = patches_metadata_df.drop_duplicates('sample_id').set_index('sample_id').loc[unique_samples, 'label_encoded']\n",
    "\n",
    "print(f\"\\nTotal unique samples (original images): {len(unique_samples)}\")\n",
    "\n",
    "# 2. First Split: Train (80%) vs Temp (20%)\n",
    "train_samples, temp_samples, y_train, y_temp = train_test_split(\n",
    "    unique_samples,\n",
    "    sample_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=sample_labels\n",
    ")\n",
    "\n",
    "# 3. Second Split: Split Temp (20%) into Val (10%) and Test (10%)\n",
    "# We split the temp set by 0.5 to get equal halves\n",
    "val_samples, test_samples = train_test_split(\n",
    "    temp_samples,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples:   {len(val_samples)}\")\n",
    "print(f\"Test samples:  {len(test_samples)}\")\n",
    "\n",
    "# 4. Create DataFrames by filtering patches based on the ID lists\n",
    "df_train = patches_metadata_df[patches_metadata_df['sample_id'].isin(train_samples)].reset_index(drop=True)\n",
    "df_val = patches_metadata_df[patches_metadata_df['sample_id'].isin(val_samples)].reset_index(drop=True)\n",
    "df_test = patches_metadata_df[patches_metadata_df['sample_id'].isin(test_samples)].reset_index(drop=True)\n",
    "\n",
    "# 5. Print Statistics\n",
    "print(f\"\\nTrain patches: {len(df_train)}\")\n",
    "print(f\"Val patches:   {len(df_val)}\")\n",
    "print(f\"Test patches:  {len(df_test)}\")\n",
    "\n",
    "print(f\"\\nTrain label distribution:\\n{df_train['label'].value_counts()}\")\n",
    "print(f\"\\nVal label distribution:\\n{df_val['label'].value_counts()}\")\n",
    "print(f\"\\nTest label distribution:\\n{df_test['label'].value_counts()}\")\n",
    "\n",
    "# Print percentage distribution\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Percentage Distribution\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTrain label percentage:\\n{df_train['label'].value_counts(normalize=True) * 100}\")\n",
    "print(f\"\\nVal label percentage:\\n{df_val['label'].value_counts(normalize=True) * 100}\")\n",
    "print(f\"\\nTest label percentage:\\n{df_test['label'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b797aff",
   "metadata": {
    "id": "4b797aff"
   },
   "source": [
    "## **5. Transformations & Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94459d26",
   "metadata": {
    "id": "94459d26"
   },
   "outputs": [],
   "source": [
    "# Define augmentation for training with enhanced transformations\n",
    "train_augmentation = transforms.Compose([\n",
    "    # Geometric transformations\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Small rotations to handle orientation variations\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),  # Reduced from 0.2 for more conservative shifts\n",
    "        scale=None,  # Add scale variation\n",
    "        shear=10  # Add shear transformation\n",
    "    ),\n",
    "\n",
    "    # Color/appearance transformations\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.05,  # Adjust brightness\n",
    "        contrast=0.2,    # Adjust contrast\n",
    "        saturation=0.2,  # Adjust saturation\n",
    "        hue=0.05       # Slight hue variation\n",
    "    ),\n",
    "    #transforms.RandomGrayscale(p=0.1),  # Occasionally convert to grayscale to improve robustness\n",
    "\n",
    "    # Occlusion simulation\n",
    "    #transforms.RandomErasing(\n",
    "    #    p=0.3,  # Reduced probability for more balanced augmentation\n",
    "    #    scale=(0.02, 0.15),  # Reduced max scale\n",
    "    #    ratio=(0.3, 3.3)  # Aspect ratio range\n",
    "    #),\n",
    "\n",
    "    # Optional: Add Gaussian blur for noise robustness\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])\n",
    "\n",
    "#train_augmentation = transforms.Compose([\n",
    "#    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),\n",
    "#    transforms.RandomHorizontalFlip(p=0.5)\n",
    "#])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cdd92",
   "metadata": {
    "id": "dc8cdd92"
   },
   "source": [
    "## **6. Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b69bba",
   "metadata": {
    "id": "29b69bba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class PairedTissueDataset(Dataset):\n",
    "    def __init__(self, df, img_dir=None, target_dir=None, transform=None, target_size=(224, 224), label_col='label_encoded'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing the first image paths.\n",
    "            img_dir: (Optional) Root dir to prepend to df paths.\n",
    "            target_dir: Root directory where the SECOND images are located.\n",
    "            transform: A torchvision.transforms object (e.g. Compose) to apply to both images.\n",
    "            target_size: Tuple (height, width) to resize images to BEFORE augmentation.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Standard Normalization (Applied individually after the synced transform)\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # --- A. Load Input Image ---\n",
    "        img_path = row['path']\n",
    "        if self.img_dir:\n",
    "            img_path = os.path.join(self.img_dir, img_path)\n",
    "            \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # --- B. Load Target Image ---\n",
    "        filename = os.path.basename(img_path)\n",
    "        \n",
    "        if self.target_dir:\n",
    "            target_path = os.path.join(self.target_dir, filename)\n",
    "            if os.path.exists(target_path):\n",
    "                target_image = Image.open(target_path).convert(\"RGB\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Target image not found: {target_path}\")\n",
    "        else:\n",
    "            raise ValueError(\"target_dir must be provided.\")\n",
    "\n",
    "        # --- C. Resize (Deterministic) ---\n",
    "        image = TF.resize(image, self.target_size)\n",
    "        target_image = TF.resize(target_image, self.target_size)\n",
    "\n",
    "        # --- D. SYNCHRONIZED AUGMENTATION (The \"State Replay\" Trick) ---\n",
    "        if self.transform:\n",
    "            # 1. Save the current random states\n",
    "            # We save both Python's random and PyTorch's random states \n",
    "            # to ensure complete synchronization across different transform types.\n",
    "            state_random = random.getstate()\n",
    "            state_torch = torch.get_rng_state()\n",
    "\n",
    "            # 2. Apply to Input Image\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # 3. Restore the random states exactly as they were\n",
    "            random.setstate(state_random)\n",
    "            torch.set_rng_state(state_torch)\n",
    "\n",
    "            # 4. Apply to Target Image\n",
    "            # Because the RNG state is reset, the transform will make the \n",
    "            # EXACT same random choices (same crop, same flip, same autoaugment policy)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        # --- E. To Tensor ---\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = TF.to_tensor(image)\n",
    "        if not isinstance(target_image, torch.Tensor):\n",
    "            target_image = TF.to_tensor(target_image)\n",
    "\n",
    "        # --- F. Normalization ---\n",
    "        # We normalize independently because this is a deterministic pixel operation\n",
    "        image = self.normalize(image)\n",
    "        target_image = self.normalize(target_image)\n",
    "\n",
    "        # --- G. Return ---\n",
    "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
    "        \n",
    "        return image, label, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19525f4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19525f4f",
    "outputId": "9ca869ec-7514-49ef-f05c-8b2f1b904c29"
   },
   "outputs": [],
   "source": [
    "# Instantiate Datasets\n",
    "train_dataset = PairedTissueDataset(df_train, transform=train_augmentation,  target_dir=BLURRED_PATCHES_OUT   )\n",
    "val_dataset = PairedTissueDataset(df_val, transform=None,  target_dir=BLURRED_PATCHES_OUT   )\n",
    "test_dataset = PairedTissueDataset(df_test, transform=None,  target_dir=BLURRED_PATCHES_OUT )\n",
    "\n",
    "# Batch Size: 32 or 64 is standard for ResNet18/50 on 1MP images\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "cpu_cores = os.cpu_count() or 2\n",
    "num_workers = cpu_cores//2\n",
    "# Instantiate Loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,          # Shuffle patches to break batch correlations\n",
    "    num_workers=num_workers,         # Adjust based on your CPU\n",
    "    pin_memory=True        # Faster data transfer to CUDA\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,         # No need to shuffle validation\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,         # No need to shuffle test\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train Loader: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader: {len(val_loader)} batches\")\n",
    "print(f\"Num workers: {train_loader.num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bfef",
   "metadata": {
    "id": "4212bfef"
   },
   "outputs": [],
   "source": [
    "def show_batch(loader, count=4):\n",
    "    # 1. Robust Unpacking\n",
    "    # The loader might return 2 items (Image, Label) or 3 (Image, Label, Mask)\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch[0], batch[1]\n",
    "    masks = batch[2] if len(batch) > 2 else None\n",
    "\n",
    "    # Determine layout: 1 row if no masks, 2 rows if masks exist\n",
    "    nrows = 2 if masks is not None else 1\n",
    "    plt.figure(figsize=(15, 5 * nrows))\n",
    "\n",
    "    # Denormalize for visualization (ImageNet stats)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    for i in range(count):\n",
    "        # --- Plot Image ---\n",
    "        ax = plt.subplot(nrows, count, i + 1)\n",
    "        \n",
    "        img = images[i]\n",
    "        img = img * std + mean  # Un-normalize\n",
    "        img = torch.clamp(img, 0, 1)  # Clip to valid range\n",
    "        \n",
    "        plt.imshow(img.permute(1, 2, 0)) # CHW -> HWC\n",
    "        plt.title(f\"Label: {labels[i].item()}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # --- Plot Mask (if available) ---\n",
    "        if masks is not None:\n",
    "            ax = plt.subplot(nrows, count, i + 1 + count) # Second row\n",
    "            mask = masks[i]\n",
    "            mask = mask * std + mean  # Un-normalize\n",
    "            mask = torch.clamp(mask, 0, 1)  # Clip to valid range\n",
    "            \n",
    "            plt.imshow(mask.permute(1, 2, 0)) # CHW -> HWC\n",
    "            plt.title(\"Blurred Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2RPTp88c0Arh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "2RPTp88c0Arh",
    "outputId": "029a90f6-1946-40e2-9644-62d0a86935dc"
   },
   "outputs": [],
   "source": [
    "print(\"\\nVisualizing Training Batch (Augmented):\")\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89d235",
   "metadata": {
    "id": "7c89d235"
   },
   "source": [
    "## **8. Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e381193",
   "metadata": {
    "id": "9e381193"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_TL = 5  \n",
    "NUM_EPOCHS_FT = 200\n",
    "\n",
    "PATIENCE = 6 \n",
    "\n",
    "L2_REG = 1e-4\n",
    "LR_TL = 1e-3\n",
    "LR_FT = 1e-4\n",
    "DROPOUT_RATE = 0.4\n",
    "\n",
    "# --- Configuration ---\n",
    "# How many blocks to unfreeze from the end of each backbone?\n",
    "# 0 = Frozen, 1 = Last Block, 2 = Last 2 Blocks, etc.\n",
    "UNFREEZE_LAYERS_IMG = 2  # For ResNet50 (The Color Stream)\n",
    "UNFREEZE_LAYERS_MASK = 2  # For ResNet18 (The Geometry Stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d978b",
   "metadata": {
    "id": "4f1d978b"
   },
   "source": [
    "## **9. Model Definition (Transfer Learning - PhikonV2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4ec0",
   "metadata": {
    "id": "5c6c4ec0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Re-define AttentionPooling (needed for PhikonClassifier)\n",
    "class PhikonClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, model_name=\"owkin/phikon\", dropout_rate=0.3, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f\"Loading Phikon backbone: {model_name}...\")\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Dynamically get embedding_dim (usually 768 for ViT-Base)\n",
    "        self.embedding_dim = self.backbone.config.hidden_size\n",
    "        print(f\"Detected backbone embedding_dim: {self.embedding_dim}\")\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # MLP Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 3, 224, 224)\n",
    "        outputs = self.backbone(pixel_values=x)\n",
    "        \n",
    "        # Extract the [CLS] token (index 0)\n",
    "        # last_hidden_state shape: (Batch, Seq_Len, Embed_Dim) -> (Batch, Embed_Dim)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return self.classifier(cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ba3db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "439ba3db",
    "outputId": "86f2b482-63fc-476a-8f31-e47b799b23c8"
   },
   "outputs": [],
   "source": [
    "# Assuming label_encoder is defined in your notebook scope\n",
    "num_classes = len(label_encoder.classes_) # label_encoder and num_classes are from previous cells\n",
    "\n",
    "model = PhikonClassifier(\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"Model re-initialized (Phikon ViT-G) with {num_classes} output classes and embedding_dim: {model.embedding_dim}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f632d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb49a6",
   "metadata": {
    "id": "fcfb49a6"
   },
   "source": [
    "## **10. Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3VVe1M_Nw5oa",
   "metadata": {
    "id": "3VVe1M_Nw5oa"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Shape [C].\n",
    "            gamma (float): Focusing parameter. Higher value = more focus on hard examples.\n",
    "                           Default is 2.0 (standard from the paper).\n",
    "            reduction (str): 'mean', 'sum', or 'none'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: [Batch, C] (Logits)\n",
    "        # targets: [Batch] (Class Indices)\n",
    "        \n",
    "        # 1. Standard Cross Entropy Loss (element-wise, no reduction yet)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # 2. Get the probability of the true class (pt)\n",
    "        # pt = exp(-ce_loss) because ce_loss = -log(pt)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # 3. Calculate Focal Component: (1 - pt)^gamma\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        # 4. Apply Class Weights (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            # Gather the alpha value corresponding to the target class for each sample\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            \n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        # 5. Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c87c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b270c87c",
    "outputId": "1eef9d8d-dbe1-407b-d2b2-c5b62c2967bd"
   },
   "outputs": [],
   "source": [
    "# 1. Get Counts (from your snippet)\n",
    "class_counts = df_train['label_encoded'].value_counts().sort_index().values\n",
    "total_samples = sum(class_counts)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "# 2. Define Manual Tuning Factors (The \"weight\" knob)\n",
    "# 1.0 = Default (Pure Inverse Frequency)\n",
    "# > 1.0 = Force model to focus MORE on this class (e.g., critical error)\n",
    "# < 1.0 = Force model to focus LESS on this class (e.g., noisy label)\n",
    "# Ensure this list length matches n_classes (4 in your case)\n",
    "tuning_factors = torch.tensor([1.0, 1.0, 1.0, 0.6], dtype=torch.float32)\n",
    "\n",
    "# 3. Calculate Base Weights (Standard Inverse Frequency)\n",
    "# Formula: N / (C * freq)\n",
    "base_weights = torch.tensor(\n",
    "    [total_samples / (n_classes * c) for c in class_counts],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 4. Apply Tuning\n",
    "# Final Weight = Inverse_Freq_Weight * Manual_Tuning_Factor\n",
    "final_weights = base_weights * tuning_factors\n",
    "\n",
    "# 5. Move to device\n",
    "final_weights = final_weights.to(device)\n",
    "\n",
    "print(f\"Base Weights:  {base_weights}\")\n",
    "print(f\"Tuning Factors:{tuning_factors}\")\n",
    "print(f\"Final Weights: {final_weights}\")\n",
    "\n",
    "# --- DEFINING THE LOSS ---\n",
    "# Gamma=2.0 is the standard starting point.\n",
    "# If your model is still ignoring the minority class, try Gamma=3.0 or 4.0.\n",
    "criterion_cls = FocalLoss(alpha=None, gamma=2.3)\n",
    "\n",
    "# Optimizer \n",
    "#optimizer = torch.optim.RAdam(\n",
    "#    [p for p in model.parameters() if p.requires_grad],\n",
    "#    lr=LR_TL,\n",
    "#    betas=(0.9, 0.999),\n",
    "#    eps=1e-8,\n",
    "#    weight_decay=L2_REG\n",
    "#)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LR_TL,\n",
    "    weight_decay=L2_REG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aa519",
   "metadata": {
    "id": "9c7aa519"
   },
   "source": [
    "## **11. Function: Training & Validation Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(preds, targets):\n",
    "    # Move to CPU and convert to numpy\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    # Calculate Macro F1 (balanced for all classes)\n",
    "    return f1_score(targets, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abb343",
   "metadata": {
    "id": "37abb343"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion_cls, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, labels, masks in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # masks not used\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass only the image\n",
    "        logits = model(images)  # ✓ Fixed\n",
    "        \n",
    "        loss = criterion_cls(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate(model, loader, criterion_cls, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, masks in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            logits = model(images)  # ✓ Fixed\n",
    "            loss = criterion_cls(logits, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440fc05",
   "metadata": {
    "id": "e440fc05"
   },
   "source": [
    "## **12. Training Loop: Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6689e",
   "metadata": {
    "id": "41d6689e"
   },
   "source": [
    "### 12.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897de7e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "897de7e8",
    "outputId": "5c199b20-a6f4-4b16-c64a-0f5052937774"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Training Variables ---\n",
    "best_val_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_tl_epoch = 0\n",
    "model_saved = False\n",
    "\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "print(f\"Starting Training with PhikonViTB  (Patience: {PATIENCE})...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_TL):\n",
    "    # Train\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion_cls, optimizer, device, )\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion_cls, device)\n",
    "\n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    # --- Checkpointing (Save Best Model based on F1) ---\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0  # Reset counter\n",
    "        best_tl_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'models/best_model_phikonViTB_tl.pt')\n",
    "        model_saved = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Print matching your requested format\n",
    "    if model_saved:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS_TL} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE} Best:{best_val_f1:.4f} ✓\")\n",
    "    else:\n",
    "        print(f\"TL Epoch {epoch+1}/{NUM_EPOCHS_TL} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(\"   >>> Early Stopping Triggered! Training stopped.\")\n",
    "        break\n",
    "    \n",
    "SUB_MODEL = 'models/best_model_PhikonViTB_tl.pt'\n",
    "print(f\"Submodel saved to {SUB_MODEL} at epoch {best_tl_epoch} with Val F1: {best_val_f1:.4f} for now. Will update if better model found in fine tuning.\")\n",
    "model.load_state_dict(torch.load(SUB_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dbdd6",
   "metadata": {
    "id": "437dbdd6"
   },
   "source": [
    "### 12.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "a6263355",
    "outputId": "22fe2121-f760-4d1c-cbdd-ffc8853bd9de"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_tl_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2068748",
   "metadata": {
    "id": "c2068748"
   },
   "source": [
    "### 12.4 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88a0a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "cd88a0a7",
    "outputId": "f3abdc30-4f4e-46ab-edc0-14f54d06b179"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Calculate and Plot Confusion Matrix\n",
    "y_true_img, y_pred_img = get_image_predictions(model, val_loader, device, method=\"max_mean_confidence\")\n",
    "y_true_img_test, y_pred_img_test = get_image_predictions(model, test_loader, device, method=\"max_mean_confidence\")\n",
    "\n",
    "# Compute Matrix\n",
    "cm_val_tl = confusion_matrix(y_true_img, y_pred_img)\n",
    "cm_test_tl = confusion_matrix(y_true_img_test, y_pred_img_test)\n",
    "f1_val_tl = f1_score(y_true_img, y_pred_img, average='macro')\n",
    "f1_test_tl = f1_score(y_true_img_test, y_pred_img_test, average='macro')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(25, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm_val_tl, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Validation\\nF1 Score: {f1_val_tl:.2f}')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm_test_tl, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Test\\nF1 Score: {f1_test_tl:.2f}')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad76976",
   "metadata": {
    "id": "2ad76976"
   },
   "source": [
    "## **13. Training Loop: Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01cc5a",
   "metadata": {
    "id": "af01cc5a"
   },
   "source": [
    "### 13.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f10bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the NEW model instance\n",
    "#    (Make sure to use the same class definition you used for training)\n",
    "ft_model = PhikonClassifier(\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    freeze_backbone=False\n",
    ").to(device)\n",
    "\n",
    "# 2. Load the best weights from the first phase\n",
    "ft_model.load_state_dict(torch.load(\"models/best_model_phikonViTB_tl.pt\"), strict=True)\n",
    "\n",
    "# 3. Unfreeze parameters\n",
    "#    First, ensure everything is frozen\n",
    "for param in ft_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#    Unfreeze the Classifier (Head)\n",
    "for param in ft_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "#    Unfreeze the last 2 blocks of the Backbone (Features)\n",
    "#    For a Vision Transformer (ViT) like Phikon, the layers are in `backbone.encoder.layer`\n",
    "for block in ft_model.backbone.encoder.layer[-2:]:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Optimizer\n",
    "ft_optimizer = torch.optim.RAdam(\n",
    "    [p for p in ft_model.parameters() if p.requires_grad],\n",
    "    lr=1e-3,\n",
    "    weight_decay=L2_REG\n",
    ")\n",
    "\n",
    "# 5. New Scheduler for the new optimizer\n",
    "ft_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    ft_optimizer, mode='min', factor=0.1, patience=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ed2bdb",
    "outputId": "9ff703a9-4e5c-43e1-d1ac-77707d46e857"
   },
   "outputs": [],
   "source": [
    "print(\"Starting Fine-Tuning Phase...\")\n",
    "\n",
    "# Reset history\n",
    "ft_history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1_ft = 0.0\n",
    "patience_counter_ft = 0\n",
    "best_ft_epoch = 0\n",
    "model_saved = False\n",
    "ft_better_than_tl = False\n",
    "\n",
    "# --- 6. Fine-Tuning Loop ---\n",
    "for epoch in range(NUM_EPOCHS_FT):\n",
    "    # Train (Pass BOTH criteria)\n",
    "    train_loss, train_f1 = train_one_epoch(ft_model, train_loader, criterion_cls , ft_optimizer, device)\n",
    "\n",
    "    # Validate (Pass BOTH criteria)\n",
    "    val_loss, val_f1 = validate(ft_model, val_loader, criterion_cls, device)\n",
    "\n",
    "    # Update Scheduler based on Validation Loss\n",
    "    ft_scheduler.step(val_loss)\n",
    "\n",
    "    # Store history\n",
    "    ft_history['train_loss'].append(train_loss)\n",
    "    ft_history['train_f1'].append(train_f1)\n",
    "    ft_history['val_loss'].append(val_loss)\n",
    "    ft_history['val_f1'].append(val_f1)\n",
    "\n",
    "    # --- Checkpointing & Early Stopping ---\n",
    "    if val_f1 > best_val_f1_ft:\n",
    "        best_val_f1_ft = val_f1\n",
    "        best_ft_epoch = epoch + 1\n",
    "        patience_counter_ft = 0 \n",
    "        \n",
    "        # Save the fine-tuned model\n",
    "        torch.save(ft_model.state_dict(), 'models/best_model_PhikonViTB_ft.pt')\n",
    "        model_saved = True\n",
    "        \n",
    "        # Check if we beat the Transfer Learning phase\n",
    "        if best_val_f1_ft > best_val_f1:\n",
    "            ft_better_than_tl = True\n",
    "    else:\n",
    "        model_saved = False\n",
    "        patience_counter_ft += 1\n",
    "\n",
    "    # Print Status\n",
    "    status_mark = \"✓\" if model_saved else \"\"\n",
    "    print(f\"FT Epoch {epoch+1}/{NUM_EPOCHS_FT} | Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Patience: {patience_counter_ft}/{PATIENCE} {status_mark}\")\n",
    "\n",
    "    if patience_counter_ft >= PATIENCE:\n",
    "        print(f\"\\nEarly Stopping Triggered! Best FT Epoch: {best_ft_epoch} with Val F1: {best_val_f1_ft:.4f}\")\n",
    "        break\n",
    "\n",
    "# Final Summary\n",
    "SUB_MODEL = 'models/best_model_PhikonViTB_ft.pt' if ft_better_than_tl else 'models/best_model_PhikonViTB_tl.pt'\n",
    "print(f\"\\nFine-Tuning Complete.\")\n",
    "print(f\"Best TL F1: {best_val_f1:.4f}\")\n",
    "print(f\"Best FT F1: {best_val_f1_ft:.4f}\")\n",
    "print(f\"Best Model Saved to: {SUB_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb7a76",
   "metadata": {
    "id": "b2bb7a76"
   },
   "source": [
    "### 13.2 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc006f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "f8dc006f",
    "outputId": "fcd252d4-ebb2-411a-ae9b-f4d518d5161b"
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ft_history['train_loss'], label='Train Loss')\n",
    "plt.plot(ft_history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ft_history['train_f1'], label='Train F1 (Macro)')\n",
    "plt.plot(ft_history['val_f1'], label='Val F1 (Macro)')\n",
    "plt.legend()\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Fine-Tuned Validation F1 Score: {:.4f} at epoch {}\".format(best_val_f1, best_ft_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c2dac",
   "metadata": {
    "id": "808c2dac"
   },
   "source": [
    "### 13.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230087f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "230087f0",
    "outputId": "8b44a927-e646-4caa-b822-1c62b5c6ccce"
   },
   "outputs": [],
   "source": [
    "y_true_ft, y_pred_ft = get_image_predictions(ft_model, val_loader, device, method=\"max_confidence\")\n",
    "y_true_ft_test, y_pred_ft_test = get_image_predictions(ft_model, test_loader, device, method=\"max_confidence\")\n",
    "\n",
    "cm_val_ft = confusion_matrix(y_true_ft, y_pred_ft)\n",
    "cm_test_ft = confusion_matrix(y_true_ft_test, y_pred_ft_test)\n",
    "\n",
    "f1_val_ft = f1_score(y_true_ft, y_pred_ft, average='macro')\n",
    "f1_test_ft = f1_score(y_true_ft_test, y_pred_ft_test, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710db55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "a710db55",
    "outputId": "9ef8bde9-b596-4510-914d-667e984dcc09"
   },
   "outputs": [],
   "source": [
    "# Create a 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.flatten()  # Flatten to 1D array for easy iteration\n",
    "\n",
    "# Prepare data lists for iteration\n",
    "# Order: T1 Val, T1 Test, FT Val, FT Test\n",
    "cms = [cm_val_tl, cm_test_tl, cm_val_ft, cm_test_ft]\n",
    "f1_scores = [f1_val_tl, f1_test_tl, f1_val_ft, f1_test_ft]\n",
    "titles = [\n",
    "    \"Transfer Learning (Validation)\", \n",
    "    \"Transfer Learning (Test)\", \n",
    "    \"Fine-Tuning (Validation)\", \n",
    "    \"Fine-Tuning (Test)\"\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        cms[i], \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=label_encoder.classes_,\n",
    "        yticklabels=label_encoder.classes_,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Set title with F1 score included\n",
    "    ax.set_title(f\"{titles[i]}\\nF1 Score: {f1_scores[i]:.4f}\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06322077",
   "metadata": {
    "id": "06322077"
   },
   "source": [
    "## **14. Submission Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9edaa",
   "metadata": {
    "id": "05e9edaa"
   },
   "source": [
    "### 14.1 Create Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942dad1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0942dad1",
    "outputId": "118226f2-be59-4aa8-baf8-8a949e856904"
   },
   "outputs": [],
   "source": [
    "sub_model =PhikonClassifier(\n",
    "    num_classes=num_classes,\n",
    "    model_name=\"owkin/phikon-v2\",  # Using ViT-G model\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ").to(device)\n",
    "# 2. Load the best weights from the first phase\n",
    "sub_model.load_state_dict(torch.load('models/best_model_PhikonViTB_ft.pt'), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21a0e8",
   "metadata": {
    "id": "9c21a0e8"
   },
   "outputs": [],
   "source": [
    "def generate_submission(\n",
    "    model,\n",
    "    submission_folder,\n",
    "    label_encoder,\n",
    "    target_folder=None,\n",
    "    method='max_confidence',\n",
    "    output_csv=\"submission.csv\",\n",
    "    device='cuda',\n",
    "    two_stream=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a submission file for single-stream or two-stream models.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model. Can be:\n",
    "               - Single-stream (e.g., PhikonClassifier): forward(self, x)\n",
    "               - Two-stream (e.g., TwoStream/ResNetParallel): forward(self, x1, x2)\n",
    "        submission_folder: Folder containing test patch images (Input 1).\n",
    "        label_encoder: The LabelEncoder object used during training to decode predictions.\n",
    "        target_folder: (Optional) Folder containing corresponding second images (Input 2).\n",
    "                       If None and two_stream=True, uses black images as second stream.\n",
    "        method: 'max_confidence' or 'majority_voting' for aggregating patch predictions.\n",
    "        output_csv: Path to save the submission CSV.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        two_stream: If True, expects model(x1, x2). If False, expects model(x1).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "    # 1. Get list of test patches\n",
    "    patch_files = sorted([\n",
    "        f for f in os.listdir(submission_folder)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "\n",
    "    print(f\"Found {len(patch_files)} patches in {submission_folder}\")\n",
    "\n",
    "    if two_stream:\n",
    "        if target_folder:\n",
    "            print(f\"Two-stream mode: looking for second images in: {target_folder}\")\n",
    "        else:\n",
    "            print(\"Two-stream mode: no target folder provided. Using black images as second stream.\")\n",
    "    else:\n",
    "        if target_folder:\n",
    "            print(\"Single-stream mode: target_folder provided but will be ignored.\")\n",
    "        else:\n",
    "            print(\"Single-stream mode: using only primary images.\")\n",
    "\n",
    "    # 2. Define Transforms (Must match training/validation transforms)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Store predictions per image\n",
    "    image_predictions = {}\n",
    "    missing_targets_count = 0\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(patch_files):\n",
    "            filepath = os.path.join(submission_folder, filename)\n",
    "\n",
    "            try:\n",
    "                # --- ID Extraction ---\n",
    "                # Extracts \"img_123\" from \"img_123_patch_0.png\" or \"img_123_p1.png\"\n",
    "                if '_p' in filename:\n",
    "                    sample_id = filename.rsplit('_p', 1)[0]\n",
    "                elif '_patch' in filename:\n",
    "                    sample_id = filename.rsplit('_patch', 1)[0]\n",
    "                else:\n",
    "                    # Fallback for whole images\n",
    "                    sample_id = os.path.splitext(filename)[0]\n",
    "\n",
    "                if sample_id not in image_predictions:\n",
    "                    image_predictions[sample_id] = {'probs': []}\n",
    "\n",
    "                # --- A. Load Input Image ---\n",
    "                image = Image.open(filepath).convert('RGB')\n",
    "                input_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                # --- B. Load Second Image (Target) if two_stream ---\n",
    "                if two_stream:\n",
    "                    # Default: Black RGB image\n",
    "                    target_pil = Image.new('RGB', image.size, (0, 0, 0))\n",
    "                    \n",
    "                    if target_folder:\n",
    "                        # Look for the EXACT same filename in the target folder\n",
    "                        target_path = os.path.join(target_folder, filename)\n",
    "                        \n",
    "                        if os.path.exists(target_path):\n",
    "                            target_pil = Image.open(target_path).convert('RGB')\n",
    "                        else:\n",
    "                            missing_targets_count += 1\n",
    "                    \n",
    "                    # Apply normalization to target as well\n",
    "                    target_tensor = val_transform(target_pil).unsqueeze(0).to(device)\n",
    "\n",
    "                    # --- C. Model Inference (Two-Stream) ---\n",
    "                    logits = model(input_tensor, target_tensor)\n",
    "                else:\n",
    "                    # --- C. Model Inference (Single-Stream) ---\n",
    "                    logits = model(input_tensor)\n",
    "\n",
    "                # Softmax to get probabilities\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "                image_predictions[sample_id]['probs'].append(\n",
    "                    probs.cpu().numpy()[0]\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                pass\n",
    "    \n",
    "    if two_stream and target_folder and missing_targets_count > 0:\n",
    "        print(f\"WARNING: Could not find paired images for {missing_targets_count} patches.\")\n",
    "\n",
    "    # 3. Aggregate Results\n",
    "    final_results = []\n",
    "\n",
    "    print(f\"Aggregating results for {len(image_predictions)} unique samples...\")\n",
    "\n",
    "    for sample_id, data in image_predictions.items():\n",
    "        all_probs = np.array(data['probs']) \n",
    "\n",
    "        # Construct sample name (adjust extension if needed, e.g. .tif or .png)\n",
    "        # If your submission file requires IDs like \"10045\", remove the extension part below.\n",
    "        sample_index_name = f\"{sample_id}.png\"\n",
    "\n",
    "        if len(all_probs) == 0:\n",
    "            # Fallback (should not happen if loop ran)\n",
    "            final_results.append({\n",
    "                'sample_index': sample_index_name,\n",
    "                'label': \"Luminal A\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if method == 'majority_voting':\n",
    "            patch_preds = np.argmax(all_probs, axis=1)\n",
    "            counts = np.bincount(patch_preds)\n",
    "            final_class_idx = np.argmax(counts)\n",
    "\n",
    "        elif method == 'max_confidence':\n",
    "            # Average the probabilities across all patches for this slide\n",
    "            avg_probs = np.mean(all_probs, axis=0)\n",
    "            final_class_idx = np.argmax(avg_probs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation method: {method}\")\n",
    "\n",
    "        # Decode Label using the PASSED label_encoder\n",
    "        pred_label = label_encoder.inverse_transform([final_class_idx])[0]\n",
    "        final_results.append({\n",
    "            'sample_index': sample_index_name,\n",
    "            'label': pred_label\n",
    "        })\n",
    "\n",
    "    # 4. Save\n",
    "    df_submission = pd.DataFrame(final_results)\n",
    "    \n",
    "    if 'sample_index' in df_submission.columns:\n",
    "        df_submission.sort_values('sample_index', inplace=True)\n",
    "    \n",
    "    df_submission.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Submission saved to {output_csv}\")\n",
    "\n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_submission_distribution(submission_data, output_file=\"prediction_distribution.png\", ax=None):\n",
    "    \"\"\"\n",
    "    Plots the distribution of predicted labels.\n",
    "\n",
    "    Args:\n",
    "        submission_data: DataFrame or path to csv.\n",
    "        output_file: Path to save the plot (only used if ax is None).\n",
    "        ax: Matplotlib axes object. If None, a new figure is created.\n",
    "    \n",
    "    Returns:\n",
    "        ax: The matplotlib Axes object.\n",
    "    \"\"\"\n",
    "    # 1. Load Data\n",
    "    if isinstance(submission_data, str):\n",
    "        if not os.path.exists(submission_data):\n",
    "            print(f\"Error: File {submission_data} not found.\")\n",
    "            return None\n",
    "        df = pd.read_csv(submission_data)\n",
    "    elif isinstance(submission_data, pd.DataFrame):\n",
    "        df = submission_data\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas DataFrame or a file path string.\")\n",
    "\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"Error: DataFrame must contain a 'label' column.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Setup Plot Style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Check if external axes provided\n",
    "    is_standalone = False\n",
    "    if ax is None:\n",
    "        is_standalone = True\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # 3. Create Count Plot\n",
    "    order = df['label'].value_counts().index\n",
    "    sns.countplot(x='label', data=df, order=order, palette=\"viridis\", hue='label', legend=False, ax=ax)\n",
    "\n",
    "    # 4. Add Annotations\n",
    "    total = len(df)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0: # Check to avoid errors with empty bars\n",
    "            percentage = '{:.1f}%'.format(100 * height / total)\n",
    "            ax.annotate(f'{int(height)}\\n({percentage})',\n",
    "                        (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', \n",
    "                        fontsize=11, color='black', xytext=(0, 5),\n",
    "                        textcoords='offset points')\n",
    "\n",
    "    # 5. Formatting\n",
    "    ax.set_title(f'Prediction Distribution (N={total})', fontsize=15, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Class', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # 6. Save and Show (Only if standalone)\n",
    "    if is_standalone:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078d160",
   "metadata": {
    "id": "7078d160"
   },
   "source": [
    "### 14.3 Create the Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e05e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "187e05e0",
    "outputId": "c95167c2-2d55-4faa-931e-7ef249cbfbb9"
   },
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "# Replace 'path/to/SUBMISSION_PATCHES' with the actual path\n",
    "# If your folder is just named SUBMISSION_PATCHES in current dir:\n",
    "\n",
    "now = datetime.now()\n",
    "date_time_str = now.strftime(\"%d_%b-%H_%M\")\n",
    "\n",
    "sub_dir = os.path.join(os.path.pardir, \"submission_csvs\")\n",
    "OUTPUT_NAME = os.path.join(sub_dir, f\"submission_ft--{date_time_str}.csv\")\n",
    "\n",
    "\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "# Check if folder exists\n",
    "if os.path.exists(SUBMISSION_PATCHES_OUT):\n",
    "    # Method 1: Max Confidence / Average Probability (Recommended)\n",
    "    df_sub_max_conf = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='max_confidence', output_csv=OUTPUT_NAME, label_encoder=label_encoder)\n",
    "    print(f\"Submission CSV saved to: {OUTPUT_NAME}\")\n",
    "\n",
    "    # Method 2: Majority Voting (Optional, uncomment to run)\n",
    "    # df_sub_majority_voting = generate_submission(sub_model, SUBMISSION_PATCHES_OUT, method='majority_voting', output_csv=\"submission_voting.csv\")\n",
    "else:\n",
    "    print(f\"Directory '{SUBMISSION_PATCHES_OUT}' not found. Please create it or set the correct path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302bd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with 2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot on specific axes\n",
    "plot_submission_distribution(df_sub_max_conf, ax=axes[0])\n",
    "axes[0].set_title(\"Submission\") # Override title if desired\n",
    "\n",
    "plot_submission_distribution(CSV_PATH, ax=axes[1])\n",
    "axes[1].set_title(\"Train Data\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
