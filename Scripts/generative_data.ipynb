{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9ffad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7dff6",
   "metadata": {},
   "source": [
    "# **CONFIGURATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f8e28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Mask dir: d:\\POLIMI\\AN2DL\\AN2DL_CH_2\\Scripts\\..\\an2dl2526c2\\preprocessing_results\\train_patches\\masks\n",
      "Image dir: d:\\POLIMI\\AN2DL\\AN2DL_CH_2\\Scripts\\..\\an2dl2526c2\\preprocessing_results\\train_patches\n",
      "Synthetic output dir: d:\\POLIMI\\AN2DL\\AN2DL_CH_2\\Scripts\\temp\\synthetic_patches\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# \n",
    "# ==========================================               # Where to save new \"dreamt\" images\n",
    "# Paths \n",
    "MASK_DIR = os.path.join(os.getcwd(),os.pardir,\"an2dl2526c2\", \"preprocessing_results\", \"train_patches\", \"masks\")  # Where your binary masks are\n",
    "IMAGE_DIR = os.path.join(os.getcwd(),os.pardir,\"an2dl2526c2\", \"preprocessing_results\", \"train_patches\")        # Where your real tissue patches are\n",
    "#SYNTHETIC_OUT_DIR = os.path.join(os.getcwd(),os.pardir,\"an2dl2526c2\", \"synthetic_patches\")    \n",
    "#MASK_DIR = os.path.join(os.getcwd(),\"temp\",\"train_mask\")  # Where your binary masks are\n",
    "#IMAGE_DIR = os.path.join(os.getcwd(),\"temp\",\"train_patch\")  # Wher    # Where your real tissue patches are\n",
    "SYNTHETIC_OUT_DIR = os.path.join(os.getcwd(),\"temp\", \"synthetic_patches\")\n",
    "\n",
    "img_size = 256\n",
    "batch_size = 16\n",
    "epochs = 50          # 50-100 is usually enough for texture learning\n",
    "lr = 0.0002\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Mask dir: {MASK_DIR}\")\n",
    "print(f\"Image dir: {IMAGE_DIR}\")\n",
    "print(f\"Synthetic output dir: {SYNTHETIC_OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00981605",
   "metadata": {},
   "source": [
    "# 1. MODEL ARCHITECTURE (Pix2Pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d053d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "\n",
    "# ==========================================\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "        # Standard U-Net structure\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) \n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        return self.final(u7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e832b9",
   "metadata": {},
   "source": [
    "# 2. DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fede07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. DATASET (ROBUST VERSION)\n",
    "# ==========================================\n",
    "class PairedMaskDataset(Dataset):\n",
    "    def __init__(self, mask_dir, img_dir, transforms_=None):\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms_\n",
    "        \n",
    "        # 1. Check if directories exist\n",
    "        if not os.path.isdir(mask_dir):\n",
    "            raise ValueError(f\"Mask directory not found: {mask_dir}\")\n",
    "        if not os.path.isdir(img_dir):\n",
    "            raise ValueError(f\"Image directory not found: {img_dir}\")\n",
    "\n",
    "        # 2. Get file lists\n",
    "        mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "        img_files_set = set(os.listdir(img_dir)) # Set for fast lookup\n",
    "        \n",
    "        self.pairs = []\n",
    "        \n",
    "        # 3. Match logic\n",
    "        for m_file in mask_files:\n",
    "            # OPTION A: Exact match (e.g. \"img_01.png\" in masks vs \"img_01.png\" in images)\n",
    "            if m_file in img_files_set:\n",
    "                self.pairs.append((m_file, m_file))\n",
    "                continue\n",
    "            \n",
    "            # OPTION B: Prefix mismatch (e.g. \"mask_img_01.png\" vs \"img_01.png\")\n",
    "            # Try removing \"mask_\" or \"mask\"\n",
    "            clean_name = m_file.replace(\"mask_\", \"\").replace(\"mask\", \"\")\n",
    "            # Sometimes clean_name might need \"img\" added back if it was stripped too much? \n",
    "            # Let's try simple replacement first: \"mask\" -> \"img\"\n",
    "            swapped_name = m_file.replace(\"mask\", \"img\")\n",
    "            \n",
    "            if clean_name in img_files_set:\n",
    "                self.pairs.append((m_file, clean_name))\n",
    "            elif swapped_name in img_files_set:\n",
    "                self.pairs.append((m_file, swapped_name))\n",
    "        \n",
    "        # 4. Debugging output if empty\n",
    "        if len(self.pairs) == 0:\n",
    "            print(\"\\n!!! ERROR: No pairs found. !!!\")\n",
    "            print(f\"Mask Dir: {mask_dir}\")\n",
    "            print(f\"First 5 masks: {mask_files[:5]}\")\n",
    "            print(f\"First 5 images in target dir: {list(img_files_set)[:5]}\")\n",
    "            raise ValueError(\"Dataset is empty. Please check the filenames printed above.\")\n",
    "\n",
    "        print(f\"Found {len(self.pairs)} paired training examples.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mask_name, img_name = self.pairs[index]\n",
    "        \n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        mask = Image.open(mask_path).convert(\"RGB\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transforms:\n",
    "            # Apply seed for geometric consistency\n",
    "            seed = np.random.randint(2147483647) \n",
    "            \n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transforms(mask)\n",
    "            \n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        return {\"mask\": mask, \"image\": img, \"filename\": mask_name}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7832ba6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 3. TRAINING ROUTINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68f8947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_pix2pix():\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    \n",
    "    # Transforms\n",
    "    transforms_ = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    # Loaders\n",
    "    dataset = PairedMaskDataset(MASK_DIR, IMAGE_DIR, transforms_=transforms_)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Initialize Model\n",
    "    generator = GeneratorUNet().to(device)\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    criterion_pixel = torch.nn.L1Loss() # Pixel-wise loss is enough for simple structure learning\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    generator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(dataloader, leave=False)\n",
    "        for batch in loop:\n",
    "            real_mask = batch[\"mask\"].to(device)\n",
    "            real_img = batch[\"image\"].to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            fake_img = generator(real_mask)\n",
    "            \n",
    "            # Loss: L1 distance between generated tissue and real tissue\n",
    "            loss_G = criterion_pixel(fake_img, real_img)\n",
    "            \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss_G.item())\n",
    "            \n",
    "    # Save Model\n",
    "    torch.save(generator.state_dict(), \"saved_models/generator_final.pth\")\n",
    "    print(\"Training Complete. Model saved.\")\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e4bf5",
   "metadata": {},
   "source": [
    "# 4. SYNTHESIS ROUTINE (\"Dreaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b16b0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(generator, num_variations=1):\n",
    "    os.makedirs(SYNTHETIC_OUT_DIR, exist_ok=True)\n",
    "    generator.eval()\n",
    "    \n",
    "    # CRITICAL: Enable Dropout during inference to generate diversity\n",
    "    # This turns the deterministic UNet into a stochastic one\n",
    "    for m in generator.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train() \n",
    "            \n",
    "    transforms_ = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    # We iterate over ALL masks (training set)\n",
    "    mask_files = [f for f in os.listdir(MASK_DIR) if f.endswith('.png')]\n",
    "    print(f\"Generating {num_variations} variations for {len(mask_files)} masks...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(mask_files):\n",
    "            # Load Mask\n",
    "            mask_path = os.path.join(MASK_DIR, filename)\n",
    "            mask_img = Image.open(mask_path).convert(\"RGB\")\n",
    "            mask_tensor = transforms_(mask_img).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate N variations\n",
    "            for i in range(num_variations):\n",
    "                # Forward pass (Dropout creates random texture variations)\n",
    "                fake_img = generator(mask_tensor)\n",
    "                \n",
    "                # Denormalize\n",
    "                fake_img = fake_img * 0.5 + 0.5\n",
    "                \n",
    "                # Save: format \"filename_v1.png\"\n",
    "                save_name = f\"{os.path.splitext(filename)[0]}_syn_v{i+1}.png\"\n",
    "                save_path = os.path.join(SYNTHETIC_OUT_DIR, save_name)\n",
    "                save_image(fake_img, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd55db9",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5a852bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4071 paired training examples.\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     gen.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33msaved_models/generator_final.pth\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     gen = \u001b[43mtrain_pix2pix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Step 2: Generate\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Set num_variations=1 or 2 to double/triple your dataset\u001b[39;00m\n\u001b[32m     15\u001b[39m generate_synthetic_data(gen, num_variations=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_pix2pix\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Loss: L1 distance between generated tissue and real tissue\u001b[39;00m\n\u001b[32m     35\u001b[39m loss_G = criterion_pixel(fake_img, real_img)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss_G\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer_G.step()\n\u001b[32m     40\u001b[39m loop.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "\n",
    "# ==========================================\n",
    "\n",
    "# Step 1: Train\n",
    "if os.path.exists(\"saved_models/generator_final.pth\"):\n",
    "    print(\"Loading pre-trained generator...\")\n",
    "    gen = GeneratorUNet().to(device)\n",
    "    gen.load_state_dict(torch.load(\"saved_models/generator_final.pth\"))\n",
    "else:\n",
    "    gen = train_pix2pix()\n",
    "    \n",
    "# Step 2: Generate\n",
    "# Set num_variations=1 or 2 to double/triple your dataset\n",
    "generate_synthetic_data(gen, num_variations=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
